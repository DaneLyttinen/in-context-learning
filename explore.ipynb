{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: \"WANDB_NOTEBOOK_NAME\"=\"explore.ipynb\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x253fc8273b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "\n",
    "from utils.data import RandomLinearProjectionMNIST, DATA_PATH, get_mnist_data_loaders, get_emnist_data_loaders, select_from_classes\n",
    "from utils.visualization import show_imgs, get_model_dot, LivePlot\n",
    "from utils.others import measure_alloc_mem, count_parameters\n",
    "from utils.timing import func_timer\n",
    "from utils.metrics import get_accuracy\n",
    "\n",
    "#import wandb\n",
    "from IPython.display import clear_output\n",
    "import tqdm\n",
    "from livelossplot import PlotLosses\n",
    "import lovely_tensors as lt\n",
    "\n",
    "lt.monkey_patch()\n",
    "torch.set_printoptions(precision=3, linewidth=180)\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"explore.ipynb\"\n",
    "#wandb.login()\n",
    "DATA_PATH = os.path.join(os.getcwd(),\"data\")\n",
    "#print(DATA_PATH)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Running on cuda ...\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 128,\n",
    "    \"seq_len\": 100,\n",
    "    \"num_of_tasks\": 2**16,\n",
    "    \"permuted_images_frac\": 1.0,\n",
    "    \"permuted_labels_frac\": 0.1,\n",
    "    # \"whole_seq_prediction\": True,\n",
    "    \"whole_seq_prediction\": False,\n",
    "    \"lr\": 3e-4,\n",
    "    \"eps\": 1e-16,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"ckpt_dir\": \"artifacts/models\",\n",
    "    \"ckpt_freq\": 2,\n",
    "    \"in_context_learner\": {\n",
    "        \"dim\": 256,\n",
    "        \"depth\": 4,\n",
    "        \"heads\": 6,\n",
    "        \"dim_head\": 32,\n",
    "        \"inner_dim\": None, # fill in below\n",
    "        \"dropout\": 0.15,\n",
    "        \"whole_seq_prediction\": None, # fill in below\n",
    "    },\n",
    "}\n",
    "config[\"in_context_learner\"][\"inner_dim\"] = config[\"in_context_learner\"][\"dim\"] * 4\n",
    "config[\"in_context_learner\"][\"whole_seq_prediction\"] = config[\"whole_seq_prediction\"]\n",
    "\n",
    "print(f\"... Running on {config['device']} ...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mnist = datasets.MNIST(DATA_PATH, train=False, download=False, transform=RandomLinearProjectionMNIST.get_default_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_lin_proj_mnist_dataset_train = RandomLinearProjectionMNIST(\n",
    "    orig_mnist_dataset=datasets.MNIST(DATA_PATH, train=True, download=False, transform=RandomLinearProjectionMNIST.get_default_transform()),\n",
    "    num_tasks=config[\"num_of_tasks\"],\n",
    "    seq_len=config[\"seq_len\"],\n",
    "    permuted_images_frac=config[\"permuted_images_frac\"],\n",
    "    permuted_labels_frac=config[\"permuted_labels_frac\"],\n",
    "    labels_shifted_by_one=config[\"whole_seq_prediction\"]\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(rand_lin_proj_mnist_dataset_train, batch_size=config[\"batch_size\"], num_workers=8, shuffle=True)\n",
    "\n",
    "rand_lin_proj_mnist_dataset_test = RandomLinearProjectionMNIST(\n",
    "    orig_mnist_dataset=datasets.MNIST(DATA_PATH, train=False, download=False, transform=RandomLinearProjectionMNIST.get_default_transform()),\n",
    "    num_tasks=config[\"num_of_tasks\"],\n",
    "    seq_len=config[\"seq_len\"],\n",
    "    permuted_images_frac=0.,\n",
    "    permuted_labels_frac=0.,\n",
    "    labels_shifted_by_one=config[\"whole_seq_prediction\"]\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(rand_lin_proj_mnist_dataset_test, batch_size=config[\"batch_size\"],num_workers=8, shuffle=True)\n",
    "\n",
    "### save dataloaders locally\n",
    "torch.save(train_loader, \"artifacts/data/train_loader.pt\")\n",
    "torch.save(test_loader, \"artifacts/data/test_loader.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### load dataloaders\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43martifacts/data/train_loader.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martifacts/data/test_loader.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dane2\\miniconda3\\envs\\online-dt\\lib\\site-packages\\torch\\serialization.py:592\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    590\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[0;32m    591\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[1;32m--> 592\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32mc:\\Users\\dane2\\miniconda3\\envs\\online-dt\\lib\\site-packages\\torch\\serialization.py:851\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m    849\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mUnpickler(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m    850\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m--> 851\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\dane2\\miniconda3\\envs\\online-dt\\lib\\site-packages\\torch\\serialization.py:843\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    841\u001b[0m data_type, key, location, size \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[1;32m--> 843\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m storage \u001b[38;5;241m=\u001b[39m loaded_storages[key]\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m storage\n",
      "File \u001b[1;32mc:\\Users\\dane2\\miniconda3\\envs\\online-dt\\lib\\site-packages\\torch\\serialization.py:831\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(data_type, size, key, location)\u001b[0m\n\u001b[0;32m    828\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    829\u001b[0m dtype \u001b[38;5;241m=\u001b[39m data_type(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m--> 831\u001b[0m storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstorage()\n\u001b[0;32m    832\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m restore_location(storage, location)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### load dataloaders\n",
    "train_loader = torch.load(\"artifacts/data/train_loader.pt\")\n",
    "test_loader = torch.load(\"artifacts/data/test_loader.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m     test_labels_shifted_by_one(data_loader\u001b[38;5;241m=\u001b[39mtest_loader)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 24\u001b[0m     \u001b[43mtest_labels_not_shifted_by_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     test_labels_not_shifted_by_one(data_loader\u001b[38;5;241m=\u001b[39mtest_loader)\n",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m, in \u001b[0;36mtest_labels_not_shifted_by_one\u001b[1;34m(data_loader)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_labels_not_shifted_by_one\u001b[39m(data_loader):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# test that the labels are not shifted by one to the right\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m784\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m y\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],)\n",
      "File \u001b[1;32mc:\\Users\\dane2\\miniconda3\\envs\\online-dt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:355\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dane2\\miniconda3\\envs\\online-dt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:301\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dane2\\miniconda3\\envs\\online-dt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:914\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    907\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m--> 914\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\dane2\\miniconda3\\envs\\online-dt\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dane2\\miniconda3\\envs\\online-dt\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dane2\\miniconda3\\envs\\online-dt\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dane2\\miniconda3\\envs\\online-dt\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\dane2\\miniconda3\\envs\\online-dt\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dane2\\miniconda3\\envs\\online-dt\\lib\\site-packages\\torch\\multiprocessing\\reductions.py:313\u001b[0m, in \u001b[0;36mreduce_storage\u001b[1;34m(storage)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m get_sharing_strategy() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_system\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    312\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39m_share_filename_()\n\u001b[1;32m--> 313\u001b[0m     cache_key \u001b[38;5;241m=\u001b[39m \u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    314\u001b[0m     rebuild \u001b[38;5;241m=\u001b[39m rebuild_storage_filename\n\u001b[0;32m    315\u001b[0m     storage\u001b[38;5;241m.\u001b[39m_shared_incref()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### perform a few data tests\n",
    "def test_labels_shifted_by_one(data_loader):\n",
    "    # test that the labels are shifted by one to the right\n",
    "    for x, y in data_loader:\n",
    "        assert x.shape == (config[\"batch_size\"], config[\"seq_len\"], 784 + 10)\n",
    "        assert y.shape == (config[\"batch_size\"], config[\"seq_len\"])\n",
    "        assert torch.all(x[:,0,-10:] == 0)\n",
    "        assert torch.all(x[:,1:,-10:].argmax(-1) == y[:,:-1])\n",
    "        break\n",
    "\n",
    "def test_labels_not_shifted_by_one(data_loader):\n",
    "    # test that the labels are not shifted by one to the right\n",
    "    for x, y in data_loader:\n",
    "        assert x.shape == (config[\"batch_size\"], config[\"seq_len\"], 784 + 10)\n",
    "        assert y.shape == (config[\"batch_size\"],)\n",
    "        assert torch.all(x[:,-1,-10:] == 0)\n",
    "        assert torch.all(x[:,:-1,-10:].max(-1).values == 1.)\n",
    "        break\n",
    "\n",
    "if config[\"whole_seq_prediction\"]:\n",
    "    test_labels_shifted_by_one(data_loader=train_loader)\n",
    "    test_labels_shifted_by_one(data_loader=test_loader)\n",
    "else:\n",
    "    test_labels_not_shifted_by_one(data_loader=train_loader)\n",
    "    test_labels_not_shifted_by_one(data_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_dataset_projections(dataset):\n",
    "    def get_pseudo_inverse(A, eps=1e-16):\n",
    "        return torch.inverse(A.T @ A + eps * torch.eye(A.shape[1], device=A.device)) @ A.T\n",
    "    lin_projection = dataset.get_lin_transform(0)\n",
    "    lin_projection_inv = get_pseudo_inverse(lin_projection)\n",
    "\n",
    "    task = dataset.get_task(0)\n",
    "    orig_img = task[0][0].view(784)\n",
    "    projected_img_manual = lin_projection @ orig_img\n",
    "    # projected_img_manual = (projected_img_manual - projected_img_manual.mean()) / projected_img_manual.std()\n",
    "    projected_img_manual_inv = lin_projection_inv @ projected_img_manual\n",
    "\n",
    "    projected_img = dataset[0][0][0,:-10]\n",
    "    projected_img_inv = (lin_projection_inv @ projected_img)\n",
    "    projected_img_inv = (projected_img_inv - projected_img_inv.mean()) / projected_img_inv.std()\n",
    "\n",
    "    show_imgs(torch.cat((orig_img.view(784), projected_img_manual, projected_img_manual_inv, projected_img, projected_img_inv), dim=0).view(-1,1,28,28))\n",
    "\n",
    "show_dataset_projections(train_loader.dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InContextLearner(\n",
      "  (transformer): Transformer(\n",
      "    (embed_proj): Linear(in_features=794, out_features=256, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): Attention(\n",
      "          (to_qkv): Linear(in_features=256, out_features=576, bias=False)\n",
      "          (to_out): Linear(in_features=192, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.15, inplace=False)\n",
      "        )\n",
      "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (3): Dropout(p=0.15, inplace=False)\n",
      "        )\n",
      "        (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): Attention(\n",
      "          (to_qkv): Linear(in_features=256, out_features=576, bias=False)\n",
      "          (to_out): Linear(in_features=192, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.15, inplace=False)\n",
      "        )\n",
      "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (3): Dropout(p=0.15, inplace=False)\n",
      "        )\n",
      "        (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): Attention(\n",
      "          (to_qkv): Linear(in_features=256, out_features=576, bias=False)\n",
      "          (to_out): Linear(in_features=192, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.15, inplace=False)\n",
      "        )\n",
      "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (3): Dropout(p=0.15, inplace=False)\n",
      "        )\n",
      "        (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): ModuleList(\n",
      "        (0): Attention(\n",
      "          (to_qkv): Linear(in_features=256, out_features=576, bias=False)\n",
      "          (to_out): Linear(in_features=192, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.15, inplace=False)\n",
      "        )\n",
      "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (3): Dropout(p=0.15, inplace=False)\n",
      "        )\n",
      "        (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_classifier): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "3099914 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_dim,\n",
    "        heads=2,\n",
    "        dim_head=16,\n",
    "        dropout=0.,\n",
    "        causal=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.causal = causal\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(model_dim, inner_dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, model_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=self.heads), (q, k, v))\n",
    "        sim = einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n",
    "\n",
    "        if self.causal:\n",
    "            # apply causal mask\n",
    "            mask = torch.ones(size=sim.shape[-2:], device=sim.device).triu_(1).bool()\n",
    "            sim.masked_fill_(mask, float(\"-inf\"))\n",
    "\n",
    "        attn = sim.softmax(dim=-1) # (batch, heads, query, key)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\", h=self.heads) # merge heads\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head,\n",
    "        token_dim=784 + 10,\n",
    "        inner_dim=None,\n",
    "        dropout=0.,\n",
    "        causal=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_proj = nn.Linear(token_dim, dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        inner_dim = inner_dim or 4 * dim\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout, causal=causal),\n",
    "                nn.LayerNorm(dim),\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(dim, inner_dim),\n",
    "                    nn.GELU(),\n",
    "                    nn.Linear(inner_dim, dim),\n",
    "                    nn.Dropout(dropout)\n",
    "                ),\n",
    "                nn.LayerNorm(dim)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed_proj(x)\n",
    "        for attn, ln1, mlp, ln2 in self.layers:\n",
    "            x = x + attn(x)\n",
    "            x = x + mlp(ln1(x))\n",
    "        return x\n",
    "\n",
    "class InContextLearner(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth=2,\n",
    "        heads=4,\n",
    "        dim_head=16,\n",
    "        inner_dim=None,\n",
    "        dropout=0.1,\n",
    "        whole_seq_prediction=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = inner_dim or 4 * dim\n",
    "        self.whole_seq_prediction = whole_seq_prediction\n",
    "        self.transformer = Transformer(\n",
    "            dim=dim,\n",
    "            depth=depth,\n",
    "            heads=heads,\n",
    "            dim_head=dim_head,\n",
    "            inner_dim=inner_dim,\n",
    "            dropout=dropout,\n",
    "            causal=whole_seq_prediction,\n",
    "        )\n",
    "        self.final_classifier = nn.Linear(dim, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x) # (batch, seq_len, dim)\n",
    "        if self.whole_seq_prediction:\n",
    "            return self.final_classifier(x)\n",
    "        else:\n",
    "            return self.final_classifier(x[:,-1,:])\n",
    "\n",
    "model = InContextLearner(**config[\"in_context_learner\"]).to(config[\"device\"])\n",
    "\n",
    "print(model)\n",
    "print(f\"{count_parameters(model)} trainable parameters\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, test_loader):\n",
    "    model.eval()\n",
    "    loss, acc = 0.0, 0.0\n",
    "    acc_over_seq = np.zeros(config[\"seq_len\"])\n",
    "    acc_max_improvement_within_seq = 0.0\n",
    "    with torch.no_grad():\n",
    "        # loss, acc, acc_max_improvement_within_seq = 0, 0, 0\n",
    "        # acc_over_seq = np.array([0.] * config[\"seq_len\"])\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(config[\"device\"], non_blocking=True), y.to(config[\"device\"], non_blocking=True)\n",
    "\n",
    "            y_hat = model(x)\n",
    "            if config[\"whole_seq_prediction\"]:\n",
    "                loss += F.cross_entropy(y_hat.view(-1, 10), y.view(-1)).item()\n",
    "                accuracy_per_seq = (y_hat.argmax(dim=-1) == y).float().mean(dim=0)\n",
    "                acc_over_seq += accuracy_per_seq.cpu().numpy()\n",
    "                acc_improvement = (y_hat[:, 1:, :].argmax(dim=-1) == y[:, 1:]).float().max(dim=1).values\n",
    "                acc_degradation = (y_hat[:, 0, :].argmax(dim=-1) == y[:, 0]).float()\n",
    "                acc_max_improvement_within_seq += (acc_improvement - acc_degradation).mean().item()\n",
    "            else:\n",
    "                loss += F.cross_entropy(y_hat, y).item()\n",
    "            acc += (y_hat.argmax(dim=-1) == y).float().mean().item()\n",
    "        num_batches = len(test_loader)\n",
    "        loss /= num_batches\n",
    "        acc /= num_batches\n",
    "        acc_over_seq /= num_batches\n",
    "        acc_max_improvement_within_seq /= num_batches\n",
    "        print(f\"loss: {loss:.4f}, acc: {acc:.4f}\")\n",
    "    return loss, acc, list(acc_over_seq), acc_max_improvement_within_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer, loss_train, loss_eval, config):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\" : model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss_train\": loss_train,\n",
    "        \"loss_eval\": loss_eval,\n",
    "        \"config\": config,\n",
    "    }, os.path.join(config[\"ckpt_dir\"], f\"model_{epoch}_{config['in_context_learner']['dim']}_{config['num_of_tasks']}.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    loss_train = checkpoint[\"loss_train\"]\n",
    "    loss_eval = checkpoint[\"loss_eval\"]\n",
    "    config = checkpoint[\"config\"]\n",
    "    return model, optimizer, epoch, loss_train, loss_eval, config\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optim = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], eps=config[\"eps\"])\n",
    "\n",
    "### logging\n",
    "groups = [\"train_loss\", \"train_acc\", \"eval_loss\", \"eval_acc\"]\n",
    "if config[\"whole_seq_prediction\"]:\n",
    "    groups.extend([\"train_acc_over_seq\", \"train_acc_max_improvement_within_seq\", \"eval_acc_over_seq\", \"eval_acc_max_improvement_within_seq\"])\n",
    "live_plot = LivePlot(figsize=(26, 24) if config[\"whole_seq_prediction\"] else (26, 14), use_seaborn=True, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load model checkpoint\n",
    "model, model_optim, start_epoch, loss_train, loss_eval, config = load_checkpoint(model, model_optim, os.path.join(config[\"ckpt_dir\"], \"model_4.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.37589848041534424\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.4656608998775482\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.45757150650024414\n",
      "Training accuracy 0.875\n",
      "Training loss: 0.38646095991134644\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.47215068340301514\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.3631209433078766\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.43915611505508423\n",
      "Training accuracy 0.875\n",
      "Training loss: 0.40212947130203247\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.3711760640144348\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.4187106490135193\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.481671005487442\n",
      "Training accuracy 0.87109375\n",
      "Training loss: 0.4277920722961426\n",
      "Training accuracy 0.85546875\n",
      "Training loss: 0.45339855551719666\n",
      "Training accuracy 0.875\n",
      "Training loss: 0.4617387652397156\n",
      "Training accuracy 0.85546875\n",
      "Training loss: 0.34761834144592285\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.34107473492622375\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.4635290205478668\n",
      "Training accuracy 0.875\n",
      "Training loss: 0.404579222202301\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.3547264039516449\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.3764766454696655\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.3618020713329315\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.40282580256462097\n",
      "Training accuracy 0.87890625\n",
      "Training loss: 0.3442731499671936\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.38324859738349915\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.35050418972969055\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.4302027225494385\n",
      "Training accuracy 0.87109375\n",
      "Training loss: 0.4361714720726013\n",
      "Training accuracy 0.87109375\n",
      "Training loss: 0.357580304145813\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.3832210302352905\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.3523748219013214\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.3691742420196533\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.43608054518699646\n",
      "Training accuracy 0.87890625\n",
      "Training loss: 0.34862491488456726\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.4046033024787903\n",
      "Training accuracy 0.87109375\n",
      "Training loss: 0.3418417274951935\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.29220062494277954\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.3507311940193176\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.29820942878723145\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.28278419375419617\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.2632772922515869\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.3087371289730072\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.3211488425731659\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.3395179510116577\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.31033754348754883\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.5085768103599548\n",
      "Training accuracy 0.83984375\n",
      "Training loss: 0.33015456795692444\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.42412641644477844\n",
      "Training accuracy 0.87109375\n",
      "Training loss: 0.32886558771133423\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.3808751702308655\n",
      "Training accuracy 0.87890625\n",
      "Training loss: 0.36407262086868286\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.40785089135169983\n",
      "Training accuracy 0.875\n",
      "Training loss: 0.26969683170318604\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.3323744833469391\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.37911829352378845\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.3833833932876587\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.25426191091537476\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.36592382192611694\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.2501269578933716\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.35557040572166443\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.2725558876991272\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.2541494071483612\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.3570405840873718\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.3124622404575348\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.26363930106163025\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.3181741237640381\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.3399891257286072\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.24016965925693512\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.39145439863204956\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.4434547424316406\n",
      "Training accuracy 0.875\n",
      "Training loss: 0.2763993740081787\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.28138425946235657\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.3378128111362457\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.33139580488204956\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.3877008855342865\n",
      "Training accuracy 0.87109375\n",
      "Training loss: 0.23277884721755981\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.3021236062049866\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.27419063448905945\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.28185081481933594\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.25968077778816223\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.34336042404174805\n",
      "Training accuracy 0.86328125\n",
      "Training loss: 0.3229522705078125\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.27600210905075073\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.34553608298301697\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.31739458441734314\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.28378766775131226\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.341547429561615\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.2648286819458008\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.3092731833457947\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.20690125226974487\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.3165247142314911\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.3591485917568207\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.3488267958164215\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.33206871151924133\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.3639882504940033\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.4171730875968933\n",
      "Training accuracy 0.875\n",
      "Training loss: 0.24088388681411743\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.2972494065761566\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.28656601905822754\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.31030571460723877\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.33932745456695557\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.38919389247894287\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.3478901982307434\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.36564797163009644\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.28160011768341064\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.20229336619377136\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.322432279586792\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.29342445731163025\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.30986595153808594\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.33837825059890747\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.2772009074687958\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.3676527440547943\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.2970026433467865\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.3482498228549957\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.2609291970729828\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.33220693469047546\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.3272150456905365\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.4971507489681244\n",
      "Training accuracy 0.84765625\n",
      "Training loss: 0.29000288248062134\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.32778456807136536\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.2671688199043274\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.38436388969421387\n",
      "Training accuracy 0.875\n",
      "Training loss: 0.31239062547683716\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.3254106044769287\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.3056601881980896\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.30357128381729126\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.2864983081817627\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.3682188391685486\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.3347223699092865\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.2855011522769928\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.3017576038837433\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.350115031003952\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.3210858702659607\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.26926302909851074\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.31579670310020447\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.2367156744003296\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.3638806939125061\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.2732078433036804\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.32466450333595276\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.34794217348098755\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.3543834984302521\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.2860165536403656\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.2993236482143402\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.3242568373680115\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.3009887635707855\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.24864332377910614\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.3029789626598358\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.2700938582420349\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.28752022981643677\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.2920389175415039\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.2514388859272003\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.4247680902481079\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.3563471734523773\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.3053268492221832\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.29860788583755493\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.2530067563056946\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.3817761242389679\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.3145345449447632\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.24540278315544128\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.3200584352016449\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.29143857955932617\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.22120197117328644\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.28995680809020996\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.2572278082370758\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.3407913148403168\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.3530637323856354\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.2694019675254822\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.3157380223274231\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.31794995069503784\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.313728392124176\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.33075159788131714\n",
      "Training accuracy 0.87890625\n",
      "Training loss: 0.31794217228889465\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.19111798703670502\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.29725411534309387\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.2548706531524658\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.33459964394569397\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.3195383846759796\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.29829755425453186\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.3175893723964691\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.2949121296405792\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.23485644161701202\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.22936177253723145\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.2812027633190155\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.38272854685783386\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.35010039806365967\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.2304372489452362\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.264852911233902\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.3002273738384247\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.26595526933670044\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.2687562108039856\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.3304060399532318\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.32804664969444275\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.29651814699172974\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.3558610677719116\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.39902690052986145\n",
      "Training accuracy 0.875\n",
      "Training loss: 0.26431089639663696\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.3194771111011505\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.24352195858955383\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.3213041424751282\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.2497737854719162\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.27807852625846863\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.2539542615413666\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.3253841698169708\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.3230285942554474\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.33477115631103516\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.23513226211071014\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.21198342740535736\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.2543455958366394\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.235725536942482\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.32807570695877075\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.16565187275409698\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.2485634833574295\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.2984224259853363\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.20555613934993744\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.2634032368659973\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.3316999077796936\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.26067885756492615\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.26603299379348755\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.21999593079090118\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.20217549800872803\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.3338201642036438\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.2166246622800827\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.23097345232963562\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.32305869460105896\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.28128838539123535\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.2914743721485138\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.28695425391197205\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.25312650203704834\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.3134734630584717\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.3316194713115692\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.2789353132247925\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.37174785137176514\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.2277384102344513\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.2818296253681183\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.37299758195877075\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.27145659923553467\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.35410887002944946\n",
      "Training accuracy 0.875\n",
      "Training loss: 0.3101569414138794\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.27619895339012146\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.22563907504081726\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.24226076900959015\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.2863549590110779\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.22763529419898987\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.26010802388191223\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.24181152880191803\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.27984705567359924\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.25715866684913635\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.20238536596298218\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.31363949179649353\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.3202921152114868\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.302518755197525\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.23673616349697113\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.26668059825897217\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.31041771173477173\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.2916472256183624\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.26170095801353455\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.3067856431007385\n",
      "Training accuracy 0.92578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/36 [03:58<2:19:05, 238.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.9058, acc: 0.2581\n",
      "eval loss 2.9058297649025917\n",
      "Training loss: 0.07267428189516068\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07709158211946487\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06789796054363251\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.047605905681848526\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.040039774030447006\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0507027693092823\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05669717863202095\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.06742090731859207\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06014294549822807\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05040781572461128\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.046839673072099686\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.051531072705984116\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03784143552184105\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.05400324612855911\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.046822208911180496\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0578104704618454\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05241414159536362\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.049735456705093384\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04474489018321037\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06202111765742302\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05569007247686386\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04459270089864731\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.057359445840120316\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07234983891248703\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04244186729192734\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.05391193926334381\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0515136756002903\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05491555482149124\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.037702880799770355\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02950076200067997\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.043163929134607315\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0430389903485775\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.05768700689077377\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04398472234606743\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.036373455077409744\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03447052836418152\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.051276493817567825\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04676840454339981\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.037680670619010925\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.041879404336214066\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0319913811981678\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.028536304831504822\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03878140076994896\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06931617856025696\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06547471880912781\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03392341360449791\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06562189012765884\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.052895016968250275\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.044811222702264786\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03579488769173622\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03706623613834381\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.029007356613874435\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.047236159443855286\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.043027568608522415\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03891440108418465\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.046113498508930206\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03683032840490341\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05730384215712547\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.045663293451070786\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04104354977607727\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030877765268087387\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03328017145395279\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04335802048444748\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04000633955001831\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03600994870066643\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03318830952048302\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04344150424003601\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04496040940284729\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03873782232403755\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03170863166451454\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03335656598210335\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.043359413743019104\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04133514314889908\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04071708396077156\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.027051422744989395\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04263656586408615\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024805109947919846\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.030273349955677986\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.048377107828855515\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.032693374902009964\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03912185877561569\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04295745864510536\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029018638655543327\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04568649083375931\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.025213975459337234\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.040016110986471176\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.047948963940143585\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0523088239133358\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03638510778546333\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.037341997027397156\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03384522721171379\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04017350822687149\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03444619104266167\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02635486051440239\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03203471004962921\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03153768554329872\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04011835902929306\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03248164802789688\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.028533196076750755\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04017188400030136\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02691737376153469\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.032358866184949875\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02673066034913063\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.040909864008426666\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030707506462931633\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027689281851053238\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03269609808921814\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.035429440438747406\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04825340211391449\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03014451451599598\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02199685201048851\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.023529887199401855\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03506210818886757\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.039133310317993164\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0428389273583889\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03100327029824257\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03793326020240784\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021343210712075233\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.024138465523719788\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.043069027364254\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.036523934453725815\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03438383713364601\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.047508567571640015\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019015494734048843\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02406688779592514\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02681051567196846\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.031837522983551025\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.045434433966875076\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023732658475637436\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.030412063002586365\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029658256098628044\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03001561015844345\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028737477958202362\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028077835217118263\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.023347752168774605\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02859323099255562\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.035513173788785934\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.028924211859703064\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030705492943525314\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027968233451247215\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.027665860950946808\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03504931181669235\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022799978032708168\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02737467549741268\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03795316070318222\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.046146053820848465\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.038011159747838974\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.014038399793207645\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02265719138085842\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0306379571557045\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030249111354351044\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02476862445473671\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.037465762346982956\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.042119428515434265\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0257935281842947\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03791302070021629\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02983982302248478\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04309595748782158\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.043146561831235886\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03586725890636444\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04901124909520149\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03619310259819031\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.036420777440071106\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0318085215985775\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.022092029452323914\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04049798846244812\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.031483858823776245\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024304809048771858\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.030646424740552902\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02858864888548851\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04510215297341347\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05946456268429756\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028205880895256996\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03723724186420441\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03666432574391365\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04052756354212761\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06330162286758423\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02388884872198105\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.022925475612282753\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.041247449815273285\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028401585295796394\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03214314207434654\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.024985725060105324\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.026900330558419228\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.029587240889668465\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030739258974790573\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04443461075425148\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030704360455274582\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.025460150092840195\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04988027364015579\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026469390839338303\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030345918610692024\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0279315784573555\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.020582593977451324\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02788791060447693\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04047117009758949\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.040771130472421646\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.022674692794680595\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017743829637765884\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02752256952226162\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026571249589323997\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04239876940846443\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03591722622513771\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02234751172363758\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03267829865217209\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03343082591891289\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03232164308428764\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029224630445241928\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.020052839070558548\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.022848719730973244\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.025465117767453194\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02801721729338169\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.029552536085247993\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05515505373477936\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.042500339448451996\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02209548093378544\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0230577252805233\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03230733424425125\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.032456036657094955\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.040156759321689606\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04021685943007469\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03434925898909569\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026079930365085602\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.040576569736003876\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.035279471427202225\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01975845918059349\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.024675210937857628\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.029120761901140213\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02321702055633068\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.018554385751485825\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.019919628277420998\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.025312503799796104\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.027954543009400368\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0543263740837574\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.030306581407785416\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03155168890953064\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02297534979879856\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02402503788471222\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03357923775911331\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017639687284827232\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.024306174367666245\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.023840898647904396\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02307680808007717\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.017417198047041893\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02087455801665783\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03657210245728493\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.026949122548103333\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.031230933964252472\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.029191482812166214\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03439301252365112\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02668812684714794\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.031132373958826065\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03249062970280647\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.035303995013237\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01585831120610237\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02034204825758934\n",
      "Training accuracy 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2/36 [07:55<2:14:50, 237.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.7520, acc: 0.2944\n",
      "eval loss 3.751967783086002\n",
      "Training loss: 0.008423504419624805\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008159210905432701\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00871141068637371\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01327693834900856\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01283703651279211\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01834249123930931\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008191132918000221\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008014336228370667\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009918139316141605\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011385616846382618\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005740839522331953\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007838173769414425\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00870212446898222\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010750043205916882\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010210991837084293\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012866877019405365\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00999485608190298\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010620527900755405\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008115056902170181\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007785214576870203\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010364342480897903\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008310914970934391\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009394185617566109\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01208691019564867\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01569541171193123\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013820772059261799\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.010368619114160538\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010331744328141212\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.009072638116776943\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008949467912316322\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01109197735786438\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00851261056959629\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008181015960872173\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010362195782363415\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009396414272487164\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005557392258197069\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005925920791924\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008485955186188221\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011892175301909447\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.006152951158583164\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.014028026722371578\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.010386529378592968\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005474083125591278\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010631063021719456\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008812160231173038\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.014863679185509682\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006809466518461704\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008584978058934212\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.013005493208765984\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.009478643536567688\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0111752450466156\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009432119317352772\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011335820890963078\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.007392094004899263\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011154882609844208\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006286433432251215\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011083528399467468\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011503092013299465\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007780282758176327\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010919207707047462\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009979783557355404\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010199085809290409\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00840536504983902\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010477904230356216\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012425921857357025\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012151701375842094\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011203248053789139\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00794821698218584\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010450897738337517\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00955285970121622\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005875411909073591\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007657053880393505\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010490876622498035\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.013476813212037086\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00800197571516037\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010768728330731392\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0065616099163889885\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00968585442751646\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0163804329931736\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008578401990234852\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009028700180351734\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010699756443500519\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008934903889894485\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007248477544635534\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007817675359547138\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011727527715265751\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008179987780749798\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00791510846465826\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006562783848494291\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00879942998290062\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007387569639831781\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006281555164605379\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00812298059463501\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0100436732172966\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00560110155493021\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006072752643376589\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010138011537492275\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006230893544852734\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008066385984420776\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01064508780837059\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007754351012408733\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007543765474110842\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010028130374848843\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007877702824771404\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007155010011047125\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004797374829649925\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009198382496833801\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007678179070353508\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00838480331003666\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010447703301906586\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006606903858482838\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007494877092540264\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005524154286831617\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008330373093485832\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0087839150801301\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010328894481062889\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007528245914727449\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007563477847725153\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011016023345291615\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011195206083357334\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00895728264003992\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.021008063107728958\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.010417516343295574\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005510526709258556\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009474772028625011\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00477256253361702\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007177687715739012\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006466193590313196\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01150858961045742\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003892172360792756\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011310024186968803\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006424540653824806\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007383295334875584\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00621420331299305\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004809572361409664\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00538175692781806\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005889415740966797\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008586195297539234\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005712356884032488\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008089969865977764\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00851913820952177\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012885096482932568\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008064037188887596\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00602205703034997\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007199990097433329\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004809109028428793\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008006489835679531\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009143007919192314\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009173494763672352\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009877888485789299\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007676786743104458\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008161637000739574\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012780159711837769\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.009874100796878338\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004840508569031954\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007382591720670462\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007781368214637041\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008230558596551418\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006602533161640167\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0074685378931462765\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01057662907987833\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007731116376817226\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008245143108069897\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005301695317029953\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007899049669504166\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006480966694653034\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005158634856343269\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009382499381899834\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007520503830164671\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005884816870093346\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005535174626857042\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.013108845800161362\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0044399769976735115\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007126925513148308\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010784617625176907\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.005575668066740036\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006313320714980364\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010826369747519493\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008037600666284561\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006234197411686182\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004152596462517977\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006611321121454239\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00536768976598978\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011378283612430096\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.005259786732494831\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0049191624857485294\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006872447207570076\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005117603577673435\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012433835305273533\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008326150476932526\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013073950074613094\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004828962497413158\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005264141596853733\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010730855166912079\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.006375203374773264\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006913664750754833\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008360159583389759\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004763357806950808\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006936917081475258\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007134828716516495\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004852066282182932\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010551589541137218\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004227904137223959\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005739537533372641\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0045553105883300304\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0060244109481573105\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007815111428499222\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00823540985584259\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005456206854432821\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0076715401373803616\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005495219025760889\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00653994781896472\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0058470298536121845\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004525373689830303\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0037083716597408056\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0043781474232673645\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0102237518876791\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0075026038102805614\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009201213717460632\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0051802401430904865\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007687013130635023\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008091357536613941\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006516153924167156\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006200466305017471\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.013204064220190048\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.005747681017965078\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0057153888046741486\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008302188478410244\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0037541978526860476\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0073798163793981075\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008440225385129452\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005387783516198397\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0063574244268238544\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009243310429155827\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007163128349930048\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0037173626478761435\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009244512766599655\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006492216605693102\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007497386075556278\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008134022355079651\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006457068957388401\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011067094281315804\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0040183779783546925\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0071279071271419525\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00731729855760932\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007744274102151394\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008702577091753483\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008938533253967762\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004607240203768015\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006778398994356394\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012376159429550171\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.009249415248632431\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005921950563788414\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011508949100971222\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00681386748328805\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00828746147453785\n",
      "Training accuracy 0.99609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/36 [11:51<2:10:34, 237.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.3830, acc: 0.2836\n",
      "eval loss 4.3830058528110385\n",
      "Training loss: 0.0027861050330102444\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004669795278459787\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0032614925876259804\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004726762883365154\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004023087676614523\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0039594522677361965\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0034960582852363586\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002850412158295512\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004264356568455696\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0033135919366031885\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0040201423689723015\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004142142366617918\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0035387345124036074\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006315520964562893\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002595691941678524\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0041979518719017506\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0045229485258460045\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0034451845567673445\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027763640973716974\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0038816886954009533\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0038099319208413363\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004355952143669128\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008156140334904194\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004656767472624779\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00279485946521163\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004708684980869293\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0071823690086603165\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003912667278200388\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0029952307231724262\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005156889092177153\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004120726604014635\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027268873527646065\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020019621588289738\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005183829925954342\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00353725696913898\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002029548631981015\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004783009644597769\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006271442864090204\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00285721686668694\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0062382956966757774\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0036290246061980724\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007156075909733772\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0032706046476960182\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005891847424209118\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0036286741960793734\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003907470498234034\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005374016705900431\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002802408766001463\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005109819583594799\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004471829626709223\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0033178103622049093\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002558573614805937\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0035951738245785236\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00390578992664814\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003874552436172962\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017804057570174336\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002989942440763116\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019068993860855699\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002103386679664254\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0038798926398158073\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004898224025964737\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004215802997350693\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003271276829764247\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002577075967565179\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005459969397634268\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030244882218539715\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005698860622942448\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003310338594019413\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005999666638672352\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0035591470077633858\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0036403355188667774\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002199165988713503\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00446916650980711\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030838644597679377\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003333954606205225\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002381299389526248\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005791838746517897\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002797977067530155\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005633997730910778\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030297469347715378\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005484295543283224\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.004497064743191004\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0036899042315781116\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0035031987354159355\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004661859944462776\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0033823149278759956\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003481220453977585\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003969599958509207\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003005555598065257\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002110961591824889\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008501800708472729\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00411447836086154\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0035721338354051113\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005000269506126642\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025449050590395927\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024213166907429695\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003625052748247981\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004427788313478231\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0067823114804923534\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00462079793214798\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0033757202327251434\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0023341199848800898\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006176673341542482\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022588842548429966\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004921298939734697\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0028898282907903194\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0034072077833116055\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004752936773002148\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025019394233822823\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0033960684668272734\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004448177292943001\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003665042808279395\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002793432679027319\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003998599946498871\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020938573870807886\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004435710608959198\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003517144126817584\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00803083460777998\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.004273624625056982\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022270551417022943\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007353519089519978\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0028668425511568785\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0036032525822520256\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030306586995720863\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018046704353764653\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003006136044859886\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002587884897366166\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0035165545996278524\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0046126809902489185\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0033629199024289846\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0044601173140108585\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003250231733545661\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0023045234847813845\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021965703926980495\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003427680116146803\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0036407625302672386\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004567528609186411\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004782750736922026\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006806143093854189\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003038384485989809\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0034443114418536425\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003912489395588636\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006012958940118551\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003499650163576007\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0029310965910553932\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004765427205711603\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0029353629797697067\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0032576301600784063\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006396633572876453\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0038316603749990463\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002746861893683672\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004803840070962906\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003739836160093546\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0038928708527237177\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030782425310462713\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00742849288508296\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00831717997789383\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0030410722829401493\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0048276931047439575\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002972720190882683\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025108540430665016\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027952406089752913\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019372368697077036\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0036148875951766968\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008601254783570766\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0029215223621577024\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009030541405081749\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003193675773218274\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027169364038854837\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0036713869776576757\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005019995383918285\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002351704752072692\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003134640399366617\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005900685675442219\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00459259981289506\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005626133177429438\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0035093394108116627\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0036681308411061764\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0034570973366498947\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018082292517647147\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004602301865816116\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0028715557418763638\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002916191006079316\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004536067135632038\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004585717339068651\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0038663491141051054\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002702348632737994\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0034513140562921762\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003922184929251671\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003049317514523864\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004095315933227539\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004062763415277004\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002472551539540291\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004247389268130064\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004098792560398579\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0035942927934229374\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004038133192807436\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0036127036437392235\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0028497788589447737\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006043515633791685\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0029148596804589033\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025482496712356806\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0037797726690769196\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004132900387048721\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008114892989397049\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.004005666822195053\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005226591136306524\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003692350583150983\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004593115299940109\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0028305200394243\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004204530734568834\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0038649921771138906\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0023112031631171703\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030905003659427166\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004643845371901989\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0036115103866904974\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004368635360151529\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004151216708123684\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030579306185245514\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003450656309723854\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007327198050916195\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021833970677107573\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0033732152078300714\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0037376268301159143\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022356947883963585\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004234697669744492\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006342910695821047\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00315720122307539\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004512394312769175\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005873521789908409\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0034188854042440653\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005302728619426489\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003253157017752528\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006862619426101446\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0036124861799180508\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004485611338168383\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021067599300295115\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005334946326911449\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003919072914868593\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0029618802946060896\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005617826711386442\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004707040265202522\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019463303033262491\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003191601950675249\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002206007717177272\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030399751849472523\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004031682852655649\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0034603490494191647\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003112653037533164\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0038410827983170748\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005532179493457079\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002504271687939763\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005792098585516214\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0033582206815481186\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00223428918980062\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003834717907011509\n",
      "Training accuracy 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 4/36 [15:47<2:06:28, 237.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.7455, acc: 0.2822\n",
      "eval loss 4.7454591896384954\n",
      "Training loss: 0.002435992704704404\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0032386488746851683\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005082848481833935\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002948135370388627\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024663780350238085\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004374523647129536\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030679162591695786\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021805623546242714\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018066802294924855\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018681922229006886\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002192307962104678\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0033555184490978718\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011202527675777674\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002151294145733118\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024095315020531416\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002083385596051812\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025821712333709\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005359998904168606\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0076929498463869095\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.003484961111098528\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003615888999775052\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002406188054010272\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0031932187266647816\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002986228559166193\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021123250480741262\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0033482711296528578\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0023574840743094683\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002295329235494137\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012664746027439833\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0023984317667782307\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024788873270154\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003309734631329775\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003345975186675787\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0032350458204746246\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002336871111765504\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001608586055226624\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002932550385594368\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019209275487810373\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017671679379418492\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020345603115856647\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024265216197818518\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006157719064503908\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018485856708139181\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017795647727325559\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022655182983726263\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003276276169344783\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003396133426576853\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0023601362481713295\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005375273525714874\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.001963359070941806\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024232480209320784\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019768511410802603\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001733144512400031\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025316551327705383\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003010372631251812\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012810982298105955\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017213573446497321\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022217927034944296\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001288709114305675\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002047630026936531\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020756360609084368\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019026185618713498\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002318150829523802\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004334684927016497\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015796415973454714\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027980227023363113\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005123187322169542\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0031235849019140005\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002468955935910344\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0035926334094256163\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017434383044019341\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001854929607361555\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0031355402898043394\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002976984716951847\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0029919312801212072\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002721090568229556\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027991305105388165\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013126583071425557\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002172649372369051\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019193224143236876\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021904350724071264\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003369302721694112\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0032271151430904865\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001643860130570829\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0023127361200749874\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002896797377616167\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004602543078362942\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002065430860966444\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001200718805193901\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004474034998565912\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016952817095443606\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0031659596133977175\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017327615059912205\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0028168915305286646\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019250536570325494\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011966864112764597\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002519553294405341\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001418059109710157\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0032797500025480986\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015298848738893867\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007514551747590303\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0022671076003462076\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017550481716170907\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003068700898438692\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016710753552615643\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024333696346729994\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002390326000750065\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0031530549749732018\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001986641902476549\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002247980097308755\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002313213888555765\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022035068832337856\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001900607137940824\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016276731621474028\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017087351297959685\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022187805734574795\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002315562916919589\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016332882223650813\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025645883288234472\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005077864974737167\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0023715589195489883\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0033860011026263237\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002791396575048566\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001578821800649166\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019092281581833959\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018829682376235723\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00213414104655385\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002326016314327717\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003782495856285095\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019232043996453285\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003106961725279689\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018253088928759098\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014522902201861143\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001908276230096817\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00368426158092916\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001930546248331666\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00182593974750489\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001824390608817339\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002328515751287341\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017017279751598835\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0026994741056114435\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030243820510804653\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0028999720234423876\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020262557081878185\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002685955259948969\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014183135936036706\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018346054712310433\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002645071828737855\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016886716475710273\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0023518423549830914\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0032508843578398228\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001644638366997242\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0028766870964318514\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004879137501120567\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0016934257000684738\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022429300006479025\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020620229188352823\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016822675243020058\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012978077866137028\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002131934743374586\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024438020773231983\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018558045849204063\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00263487477786839\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013704715529456735\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002826591022312641\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024653724394738674\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001922407653182745\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011717373272404075\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030319264624267817\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018574604764580727\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0028790337964892387\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001292147091589868\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020816402975469828\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002257842570543289\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018972253892570734\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00428577559068799\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003152627730742097\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022857787553220987\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018241474172100425\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030748408753424883\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015017436817288399\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003344652010127902\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018380278488621116\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001153155812062323\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013327817432582378\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001767258974723518\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001657118322327733\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000896434998139739\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009542896877974272\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0032354199793189764\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001551074325107038\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018532179528847337\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002489292761310935\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019030797993764281\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021281756926327944\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002756428439170122\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014499721582978964\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016322064911946654\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021162668708711863\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005299802869558334\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0016894000582396984\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003522943239659071\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001448792521841824\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002592272823676467\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002802916569635272\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001668959273956716\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001653940649703145\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020004380494356155\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013838463928550482\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001508028944954276\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018959398148581386\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0029994389042258263\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004190357401967049\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0026925248093903065\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0038092255126684904\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0031262501142919064\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003094624960795045\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015286862617358565\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019645693246275187\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0035194200463593006\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002826124429702759\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001413246733136475\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022848956286907196\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021679631900042295\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002252546837553382\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019202381372451782\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017524870345368981\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019036089070141315\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001771685783751309\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001364794559776783\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004345326218754053\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024604061618447304\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014653895050287247\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027954040560871363\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001694846898317337\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001573474146425724\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001516641234047711\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002668053610250354\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027026457246392965\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012354551581665874\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002520136535167694\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016205917345359921\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0031960175838321447\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002111941808834672\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017746498342603445\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002825444098562002\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012403641594573855\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013555421028286219\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0023410837166011333\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0046944888308644295\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014041500398889184\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001487024244852364\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016486637759953737\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002667607506737113\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012896700063720345\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005793040152639151\n",
      "Training accuracy 0.99609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 5/36 [19:45<2:02:34, 237.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.0463, acc: 0.2775\n",
      "eval loss 5.0463474821299314\n",
      "Training loss: 0.001266240025870502\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017147083999589086\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009754677885212004\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012262363452464342\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010749243665486574\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001043673837557435\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018553470727056265\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001344455173239112\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001704173511825502\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016637368826195598\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014351605204865336\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001444367691874504\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002649101661518216\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016327659832313657\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013135838089510798\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027409952599555254\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014090087497606874\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001935648499056697\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001367395045235753\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001404362148605287\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002127461601048708\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000944420404266566\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017315110890194774\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011394358007237315\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001430834410712123\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025588024873286486\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017355200834572315\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013601715909317136\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002037232741713524\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017393050948157907\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013583096442744136\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021728218998759985\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011542192660272121\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001066518248990178\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013872304698452353\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021727082785218954\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012253813911229372\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0045728497207164764\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.001358045032247901\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00298609328456223\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002178760012611747\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015630400739610195\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013475328451022506\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014550703344866633\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007424151408486068\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025305841118097305\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025771434884518385\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015908358618617058\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020031812600791454\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015953581314533949\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016153502510860562\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020930953323841095\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001847107196226716\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015430599451065063\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022080542985349894\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013169724261388183\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019318280974403024\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013600328238680959\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012394897639751434\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003371355589479208\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004294235724955797\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00196602800861001\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001434070523828268\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009030054206959903\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013398342998698354\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001311353174969554\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010272579966112971\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002874750876799226\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00252171722240746\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016986478585749865\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019494659500196576\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017055690987035632\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013116688933223486\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014608345227316022\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016941033536568284\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009885745821520686\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017194185638800263\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00217573344707489\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014236835995689034\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017374728340655565\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013769008219242096\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025065916124731302\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00172973214648664\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008984605665318668\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001606357516720891\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014687128132209182\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013587001012638211\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013744211755692959\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001068669487722218\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016938302433118224\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011939360992982984\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009634764865040779\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001220582751557231\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018452235963195562\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009834947995841503\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001389743061736226\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001574064022861421\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015284349210560322\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010287476470693946\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021552008111029863\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016833213157951832\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022614162880927324\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001330120605416596\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002814890118315816\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014831912703812122\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011157025583088398\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012847622856497765\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002725835656747222\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001421396853402257\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014821039512753487\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0036973212845623493\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003315553767606616\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011166478507220745\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0028752675279974937\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00108198297675699\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018294831970706582\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017800182104110718\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001037961570546031\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016381642781198025\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0039125289767980576\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009953739354386926\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018056008266285062\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011806851252913475\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025921855121850967\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002080816775560379\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017172138905152678\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010345452465116978\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0037827841006219387\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013993713073432446\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001869158004410565\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001141996937803924\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012527136132121086\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014878178481012583\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0023718189913779497\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010279788402840495\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002599405823275447\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019399457378312945\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001426971866749227\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002009622985497117\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013181561371311545\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017226770287379622\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010863224742934108\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021184911020100117\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00204521045088768\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015788207529112697\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001218027318827808\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020124725997447968\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008317970787175\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015670597786083817\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009085351484827697\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013077997136861086\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002312230644747615\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008654569974169135\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008686552755534649\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013642814010381699\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009453952661715448\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00163525331299752\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018202640349045396\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010579770896583796\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002293602330610156\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007956357439979911\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004081142134964466\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010987159330397844\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016253850189968944\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000891026807948947\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018225782550871372\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017235113773494959\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011663571931421757\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0029943182598799467\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021838126704096794\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002184634329751134\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012642934452742338\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020581288263201714\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010706051252782345\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002156081609427929\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011570428032428026\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025901731569319963\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017807030817493796\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009117780136875808\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009353778441436589\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001153977820649743\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016035231528803706\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001975690247491002\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015158129390329123\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00218377192504704\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020274228882044554\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015614518197253346\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012405791785567999\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016364851035177708\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015476533444598317\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008303219801746309\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001402313937433064\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030621106270700693\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001517796074040234\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009401578572578728\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001507488777860999\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005214350763708353\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001474191783927381\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013206499861553311\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011053758207708597\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015276476042345166\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013496646424755454\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011022916296496987\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011768683325499296\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019530613208189607\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006926078931428492\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015794823411852121\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001706790179014206\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011210758239030838\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008684094063937664\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012252131709828973\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008230163948610425\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019066807581111789\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004376464057713747\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015179179608821869\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0028010522946715355\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015446931356564164\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017322360072284937\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022256833035498857\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024578063748776913\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009132802370004356\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008711314294487238\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013249792391434312\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021938937716186047\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009781173430383205\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012199529446661472\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002539014210924506\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011773223523050547\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001253599300980568\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009316187934018672\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018003376899287105\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009970373939722776\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015960487071424723\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016047073295339942\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007482446380890906\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002576989820227027\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0036555349361151457\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0036300348583608866\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024739368818700314\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014668232761323452\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021910546347498894\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011676179710775614\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011434104526415467\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00124273425899446\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001732277451083064\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030228178948163986\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002409884938970208\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002001897431910038\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015531782992184162\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013939583441242576\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018322024261578918\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008536344394087791\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020572436042129993\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009390120976604521\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010165898129343987\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016498083714395761\n",
      "Training accuracy 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 6/36 [23:41<1:58:30, 237.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.3091, acc: 0.2811\n",
      "eval loss 5.309052953496575\n",
      "Training loss: 0.002198304980993271\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006317576626315713\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015263998648151755\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001458420418202877\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017511086771264672\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017268212977796793\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0032274972181767225\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011650394881144166\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014416500926017761\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016890248516574502\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012298310175538063\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009534407872706652\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015866728499531746\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007914481102488935\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012932791141793132\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014441233361139894\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024170803371816874\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019413121044635773\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001177313388325274\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011197830317541957\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011251074029132724\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012482966994866729\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000806960859335959\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009209025884047151\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025066665839403868\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014572790823876858\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012245296966284513\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008126756874844432\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017888620495796204\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017144212033599615\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006671259179711342\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00109479995444417\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011704321950674057\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006619745981879532\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00152639823500067\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001173372264020145\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011127034667879343\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005945106386207044\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010444018989801407\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012773299822583795\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003321055555716157\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015761465765535831\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006943480111658573\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007934545865282416\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021207239478826523\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022005769424140453\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007511803414672613\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012578763999044895\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012127108639106154\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000850129290483892\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008239923627115786\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008800710784271359\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013680104166269302\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006375621305778623\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014138208935037255\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016671759076416492\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0029286693315953016\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009756485815159976\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001198896556161344\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012219815980643034\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001751254196278751\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006454600370489061\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009324506390839815\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002910721581429243\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016897183377295732\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007874284638091922\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011469812598079443\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010225092992186546\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017001847736537457\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007642164709977806\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008922647684812546\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007765956106595695\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007494799792766571\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001177338301204145\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007839144091121852\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0036518836859613657\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008315631421282887\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016208584420382977\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009303304832428694\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006556766456924379\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008860802045091987\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015827948227524757\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0032367585226893425\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001124559435993433\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010073900921270251\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009452216327190399\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005835613701492548\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001687520183622837\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003409016178920865\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002294483594596386\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016775639960542321\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014986110618337989\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000923052488360554\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002380755264312029\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009037544950842857\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001772508374415338\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006566755473613739\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011899670353159308\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001388315693475306\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010074219899252057\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009289732552133501\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006003006710670888\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011010211892426014\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009955480927601457\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006225075339898467\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018109892262145877\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019676070660352707\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014244646299630404\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014499847311526537\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007858271710574627\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013045936357229948\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002223307266831398\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007280419231392443\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004656380973756313\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0020070963073521852\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018060996662825346\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00099729944486171\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015045885229483247\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009194493177346885\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009475370170548558\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010950982104986906\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008597296546213329\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001836111070588231\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008614821708761156\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010283758165314794\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007276292890310287\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011153009254485369\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000832352670840919\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0026398063637316227\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0032695247791707516\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013163734693080187\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000738834380172193\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009004806634038687\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009185827220790088\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002518251771107316\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005869555752724409\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006889132200740278\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010821957839652896\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008771137217991054\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011056417133659124\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016524478560313582\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009318259544670582\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001933519379235804\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001095996587537229\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0035122272092849016\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001742695807479322\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018772565526887774\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006630697753280401\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008586284820921719\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004442882724106312\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0006607730756513774\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008575946558266878\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001265103230252862\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002049513626843691\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025867235381156206\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011149250203743577\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007997769862413406\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017755823209881783\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006998531171120703\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0033067651093006134\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017402495723217726\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000928922847378999\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015270764706656337\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013542555971071124\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008977804682217538\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00347784417681396\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00119302689563483\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000770945567637682\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009847921319305897\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008031976176425815\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006069017108529806\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010114653268828988\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012976026628166437\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007478203624486923\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014689681120216846\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009684909018687904\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007762555032968521\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012167547829449177\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011466614669188857\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00146760034840554\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009843481238931417\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000904453219845891\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009928493527695537\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014500223333016038\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00105151382740587\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008500908152200282\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011496706865727901\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006975553696975112\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005394609761424363\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008184328326024115\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010136269265785813\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006333943456411362\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010478799231350422\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016287544276565313\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001220134785398841\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0037887319922447205\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00102770805824548\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005203471053391695\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0008930301992222667\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010500869248062372\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0034821166191250086\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011150981299579144\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009084258927032351\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008744907681830227\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014073110651224852\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010109436698257923\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018799788085743785\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015154858119785786\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014202222228050232\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007911738357506692\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010172349866479635\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021278115455061197\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002542305737733841\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011666230857372284\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006016991101205349\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0004467307298909873\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005998069536872208\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017761647468432784\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009097912115976214\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000721825985237956\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012851239880546927\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010665099835023284\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008430416928604245\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008232487598434091\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007819071179255843\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011881660902872682\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004205741919577122\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014200046425685287\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015627237735316157\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021869977936148643\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010173529153689742\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016866823425516486\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015506482450291514\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000874944613315165\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010367664508521557\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001639660564251244\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001018310314975679\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001139925210736692\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000877514248713851\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00243597780354321\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006675560725852847\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007949882419779897\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002518236171454191\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005892858025617898\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015844361623749137\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005595319089479744\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008517298265360296\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000836613995488733\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001999645261093974\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00110870401840657\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015152143314480782\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012901586014777422\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012449788628146052\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017634574323892593\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001675354316830635\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011086852755397558\n",
      "Training accuracy 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 7/36 [27:39<1:54:39, 237.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.5956, acc: 0.2704\n",
      "eval loss 5.595557529479265\n",
      "Training loss: 0.0010857230518013239\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027495550457388163\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027296263724565506\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010397504083812237\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001393504673615098\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006463528261519969\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009984446223825216\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014887037687003613\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013936389004811645\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00086676434148103\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007309689535759389\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000972972484305501\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010360776213929057\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011386184487491846\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008443809347227216\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015323227271437645\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010420915205031633\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007478086045011878\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008815439650788903\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003075280925258994\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010695718228816986\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018054706742987037\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001111492863856256\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006583395879715681\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009652951848693192\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014076917432248592\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006756719085387886\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007743708556517959\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007267355103977025\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001565359765663743\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001008858671411872\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007000374025665224\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007226194138638675\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000749930099118501\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008491894695907831\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008737250464037061\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005306918174028397\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009000241989269853\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001108929398469627\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008413617615588009\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00045602209866046906\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010598499793559313\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007425288786180317\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011674197157844901\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006906100898049772\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012955897254869342\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012593514984473586\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000875296478625387\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007983113173395395\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008293855935335159\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007661344716325402\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005730276927351952\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0008832112071104348\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007004897925071418\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009266062406823039\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008035619393922389\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009956540307030082\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0029214590322226286\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011334166629239917\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008958637481555343\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005577851552516222\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010661387350410223\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006122981430962682\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009013974922709167\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007862101192586124\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011632345849648118\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006643605302087963\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000990503584034741\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008492403430864215\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005731210112571716\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011764256050810218\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007472143624909222\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001311275758780539\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0004906771355308592\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011999716516584158\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008774754824116826\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006912907119840384\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00042157495045103133\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009376125526614487\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000681200297549367\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008391401497647166\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00052527931984514\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001067620818503201\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018954586703330278\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00469233701005578\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.001361893955618143\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00144546153023839\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012421989813446999\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008406089036725461\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005757011007517576\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0005228417576290667\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002415388124063611\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007797631551511586\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009825292509049177\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001728971372358501\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014836117625236511\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007073376327753067\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008504217257723212\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013064094819128513\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004777878522872925\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0010662494460120797\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001834722119383514\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012350588804110885\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018772155744954944\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019508397672325373\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0003901903401128948\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002322473796084523\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016375805716961622\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010626602452248335\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001199746853671968\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009933537803590298\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006700821104459465\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005380089860409498\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010982048697769642\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005033983616158366\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013233140343800187\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007601072546094656\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0009066214552149177\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012712489115074277\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013456116430461407\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0035595567896962166\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010019390610978007\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018393619684502482\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007501773070544004\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010858183959499002\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008845474221743643\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013710028724744916\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0028072544373571873\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002730251755565405\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002169074257835746\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014336910098791122\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009119821479544044\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006813879008404911\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010754511458799243\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011045202845707536\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001348545658402145\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00118371588177979\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002556054387241602\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001051509054377675\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001600997638888657\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0028069177642464638\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007899267366155982\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006974416901357472\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012506068451330066\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0026250341907143593\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001470208284445107\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006400011479854584\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011180737055838108\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007179329986684024\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0031333216466009617\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008754144073463976\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00126998545601964\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011881861137226224\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014033050974830985\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012695450568571687\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007244051666930318\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027232158463448286\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015304874395951629\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001054515945725143\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00244700163602829\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013374873669818044\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010719143319875002\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001306031714193523\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016462899511680007\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0032005554530769587\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006660015787929296\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013034779112786055\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0004686736792791635\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007075494504533708\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017127185128629208\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010828004451468587\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005379645619541407\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.000857880397234112\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000980369746685028\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014434577897191048\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006140755722299218\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002013019984588027\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018213597359135747\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001379353809170425\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014738867757841945\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010940423235297203\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001972339814528823\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018550055101513863\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010060224449262023\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008531367057003081\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00074973568553105\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001996145350858569\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017203025054186583\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015706438571214676\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007671458297409117\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008000603993423283\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015722860116511583\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013552133459597826\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000679260934703052\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001309296116232872\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013727627228945494\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001379990833811462\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001715415040962398\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00223102536983788\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014489785535261035\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008707794477231801\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004211549647152424\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012746743159368634\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030241517815738916\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002118332078680396\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000721708289347589\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003309080610051751\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009869809728115797\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005357663612812757\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00047861909843049943\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0031717047095298767\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009895258117467165\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004504386801272631\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010888988617807627\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014949493343010545\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001018973533064127\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012694590259343386\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008443614933639765\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001137621351517737\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025714102666825056\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007908514235168695\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00497277919203043\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010003468487411737\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006340919062495232\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007508425624109805\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001339371781796217\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001192020601592958\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017541274428367615\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018665972165763378\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010952756274491549\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020173152443021536\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0023321984335780144\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004904595203697681\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0012898939894512296\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014438193757086992\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005544821033254266\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013054109876975417\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007890355191193521\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015831622295081615\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017649539513513446\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001425566733814776\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013109203428030014\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000845578673761338\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00396434310823679\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0011051460169255733\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008203824982047081\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012600002810359001\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013329973444342613\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002375195501372218\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009063106845133007\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001023395569063723\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011209710501134396\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013727897312492132\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00066246377537027\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009987885132431984\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009239535429514945\n",
      "Training accuracy 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 8/36 [31:35<1:50:34, 236.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.8781, acc: 0.2754\n",
      "eval loss 5.878131067380309\n",
      "Training loss: 0.0008573880768381059\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012660023057833314\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007724820170551538\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006935453275218606\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014395711477845907\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008264041389338672\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007904945523478091\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008405305561609566\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005059384275227785\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0008022363181225955\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000855887308716774\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012202062644064426\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007820020546205342\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007770571974106133\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010859809117391706\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010082786902785301\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007065579993650317\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001028392231091857\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008156870608218014\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006625727401115\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008743747603148222\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005599561845883727\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0031443540938198566\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011515072546899319\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013353647664189339\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007996438653208315\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015459249261766672\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008410835289396346\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0004464255180209875\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007727972697466612\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017811901634559035\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00602110056206584\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0009694551699794829\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020246547646820545\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001213150448165834\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012445608153939247\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007959154900163412\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000574435165617615\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005468414747156203\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001480643404647708\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005904122372157872\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030278221238404512\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007852220442146063\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008197968709282577\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008004826377145946\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005686568329110742\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009783129207789898\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001642292714677751\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011793276062235236\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011845154222100973\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007605613209307194\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022982058580964804\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007151954341679811\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006508632795885205\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008693088893778622\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000792686827480793\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019130198052152991\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0032791343983262777\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017462370451539755\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001072542741894722\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008492273045703769\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015542451292276382\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006789036560803652\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000861917098518461\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008971868082880974\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010640627006068826\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009237704798579216\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007977144559845328\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021218005567789078\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006258226349018514\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0004760430019814521\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016995357582345605\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007813069387339056\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001629031146876514\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006876547704450786\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006720151868648827\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009816563688218594\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007674889056943357\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001639113761484623\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019765777979046106\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012264703400433064\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006572529091499746\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010227072052657604\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014895169297233224\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010313810780644417\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008105430752038956\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00829338002949953\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0010749042266979814\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008545128512196243\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008693297859281301\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009523240150883794\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015806962037459016\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001359811401925981\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009353357600048184\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0023298379965126514\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010399546008557081\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008573308004997671\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006497499998658895\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019761479925364256\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009592953720130026\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00048753677401691675\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017559127882122993\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010883808135986328\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018537442665547132\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011292464332655072\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012460885336622596\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001881929812952876\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001713531673885882\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001493251882493496\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011833125725388527\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0015330497408285737\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008702455088496208\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011969052720814943\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027893949300050735\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010598761728033423\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020693824626505375\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009489594958722591\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0049626524560153484\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.002615337958559394\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027648562099784613\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013194390339776874\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013005782384425402\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009421830181963742\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002460074843838811\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018495072145015001\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014484917046502233\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011585122905671597\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0021037724800407887\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004110594745725393\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0023174320813268423\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001797276665456593\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00159476010594517\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006169811240397394\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010660028783604503\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008621543529443443\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001445567817427218\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001088285818696022\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020183727610856295\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001945635536685586\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017049864400178194\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013458108296617866\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012894695391878486\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002276740036904812\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011194408871233463\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001493112533353269\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012352443300187588\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001705413800664246\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00094973313389346\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014257547445595264\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007193342898972332\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010198317468166351\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005146870855242014\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014636990381404757\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007428432581946254\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013376103015616536\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008697158191353083\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010617224033921957\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0029431029688566923\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014008291764184833\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00262991851195693\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0029219260904937983\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001232610666193068\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010206892620772123\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00099524250254035\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001675650360994041\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003522065468132496\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012299481313675642\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010785588528960943\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014014524640515447\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008741217316128314\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012722968822345138\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001214845571666956\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013106074184179306\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003257415723055601\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008274138672277331\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002282407134771347\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014889677986502647\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0031484083738178015\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016319144051522017\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008886042633093894\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007040743948891759\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002344140550121665\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019689390901476145\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011364939855411649\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008733606664463878\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011558119440451264\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011841438245028257\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008730656118132174\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013133528409525752\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009430770296603441\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002101056044921279\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002044315217062831\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0037724815774708986\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.001407560775987804\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010335486149415374\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005217514932155609\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.006676856428384781\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001016903668642044\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007383733056485653\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.000621815852355212\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011670436942949891\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011716157896444201\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011018147924914956\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015703806420788169\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011788993142545223\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009944295743480325\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014944957802072167\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002074332907795906\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001261200406588614\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021038509439677\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020538687240332365\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014026816934347153\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0023946615401655436\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0017353016883134842\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013815369457006454\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002467984100803733\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004077563993632793\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008448428707197309\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010733997914940119\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002177420537918806\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024371540639549494\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014656169805675745\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001958089880645275\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002188671613112092\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013981627998873591\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002086559310555458\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012195921735838056\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013776276027783751\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002200763439759612\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002004054142162204\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00451668119058013\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009805293520912528\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018236050382256508\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00061957718571648\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0033853519707918167\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013874806463718414\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016330976504832506\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001104018185287714\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007328966166824102\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007870853878557682\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002687441883608699\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0038903949316591024\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024529467336833477\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0028987766709178686\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003580627730116248\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006670018774457276\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006243397481739521\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.001457627397030592\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019208048470318317\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005311965011060238\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006894852849654853\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022269743494689465\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003703064052388072\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0019317048136144876\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003918771166354418\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0028747173491865396\n",
      "Training accuracy 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 9/36 [35:30<1:46:17, 236.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.0605, acc: 0.2850\n",
      "eval loss 6.060463864356279\n",
      "Training loss: 0.0009165211231447756\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011596244294196367\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0022459381725639105\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012195288436487317\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0026919872034341097\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0031959782354533672\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010341376764699817\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014088209718465805\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018918475834652781\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015153593849390745\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018267843406647444\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010365535272285342\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006068652728572488\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007931339787319303\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001829069689847529\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012517367722466588\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014201586600393057\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001505983411334455\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015510712983086705\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018297211499884725\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019811626989394426\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015874076634645462\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006708084256388247\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011640285374596715\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016594064654782414\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003322926117107272\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003414946375414729\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021820301190018654\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001418314059264958\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00039081776048988104\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011792813893407583\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0029320945031940937\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003092158818617463\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009875930845737457\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011350249405950308\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002974048489704728\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007251244969666004\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0052605802193284035\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0028308669570833445\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008367973496206105\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001220022444613278\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010070931166410446\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009215768659487367\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011059633689001203\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013993713073432446\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0006233861786313355\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003848872845992446\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00211081700399518\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016417654696851969\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012246009428054094\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002329579321667552\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020403226371854544\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015453359810635448\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015882408479228616\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003980003762990236\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024989524390548468\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0029581442940980196\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0007050998392514884\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011888339649885893\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004493008367717266\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00314700766466558\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009139953181147575\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0008705515065230429\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009404852171428502\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004916854668408632\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0011541745625436306\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019640333484858274\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014773375587537885\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002646647859364748\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001997824991121888\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006006381008774042\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0008844754775054753\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012348328018561006\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002364914398640394\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006093870848417282\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0009582793572917581\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00668918713927269\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0031297567766159773\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0031671863980591297\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004484349861741066\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0020969482138752937\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011005373671650887\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0038685358595103025\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010190147440880537\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019465184304863214\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030696834437549114\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021907563786953688\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.000952643109485507\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024326315615326166\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001660399604588747\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01268032193183899\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.002340006874874234\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0033148801885545254\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0021130768582224846\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014689606614410877\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016594268381595612\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003460494801402092\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014176751719787717\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0019186820136383176\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002414570888504386\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011802345979958773\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002216210588812828\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002086219610646367\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00395556353032589\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001383490744046867\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002646966138854623\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0012538841692730784\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0037972822319716215\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0040575419552624226\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002471062820404768\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0014875170309096575\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0010874421568587422\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003259615506976843\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005326395854353905\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0029124519787728786\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0018191764829680324\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009698532521724701\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005458293482661247\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0025225880090147257\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013419537572190166\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0024851623456925154\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015764017589390278\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004533318802714348\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009633752051740885\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002484058029949665\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0013384269550442696\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005087342578917742\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0005514336517080665\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002605030545964837\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012633650563657284\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.005281348247081041\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001549839973449707\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0011125413002446294\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0026807747781276703\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015012439107522368\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027422932907938957\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.016322916373610497\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.003835351439192891\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0015261584194377065\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00301052862778306\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001048011821694672\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0009601374622434378\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0029140107799321413\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009256051853299141\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0026953215710818768\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008184920065104961\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.007149520795792341\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027953018434345722\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004325959365814924\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0020281521137803793\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003926415462046862\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002737293252721429\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0075952946208417416\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.005785453133285046\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003852283116430044\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004629193805158138\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005300832912325859\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0016072725411504507\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0035122709814459085\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005767304915934801\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007362877018749714\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0034140702337026596\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005261173937469721\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004322553053498268\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0023853552993386984\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005444852635264397\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0057967728935182095\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005771855358034372\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.006850960198789835\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027037044055759907\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027578622102737427\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00403670035302639\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007806490641087294\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.009534211829304695\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.003668607445433736\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002483582589775324\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030765824485570192\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027898962143808603\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.002352004637941718\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004179161973297596\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009716187603771687\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.003694867482408881\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00269953440874815\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006193001754581928\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00678612245246768\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.020758628845214844\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00330917164683342\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004266681149601936\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0025070898700505495\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012154844589531422\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0031410688534379005\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0030469370540231466\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008342254906892776\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0034288051538169384\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00532324006780982\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.001956875203177333\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004653191659599543\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.003112568985670805\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0037406100891530514\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0033756447955965996\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004277476109564304\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0032655331306159496\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.013702591881155968\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.007038237992674112\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005092755891382694\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006451417692005634\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0041803522035479546\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01866334304213524\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00368697801604867\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00927277933806181\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01572667807340622\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.010842079296708107\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.002010909840464592\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0027456539683043957\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0064346385188400745\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.004550644662231207\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.016953272745013237\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015910841524600983\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.007820935919880867\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.004662651568651199\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0068090311251580715\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.014937440864741802\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02375956065952778\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02518220618367195\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022386154159903526\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0034816102124750614\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01054586935788393\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008824260905385017\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.010594595223665237\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.010318211279809475\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.05077488347887993\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.005115432199090719\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0054206266067922115\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02416282333433628\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0051690456457436085\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.017445072531700134\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013492795638740063\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.012749938294291496\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02598203904926777\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.008434555493295193\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01663089171051979\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.022290216758847237\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.012650322169065475\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05254855006933212\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01600361056625843\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.034188445657491684\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.024518338963389397\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028207723051309586\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02276645600795746\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.037442777305841446\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011090710759162903\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01672983355820179\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02003389596939087\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027829227969050407\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02102312073111534\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026181576773524284\n",
      "Training accuracy 0.9921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 10/36 [39:24<1:42:08, 235.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.6022, acc: 0.2571\n",
      "eval loss 5.602212723344564\n",
      "Training loss: 0.0316559337079525\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026791241019964218\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.014114705845713615\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012579444795846939\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03841377794742584\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.014321321621537209\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06310576945543289\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02450406365096569\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.027743065729737282\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02633165940642357\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03543064370751381\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04615870118141174\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.038933951407670975\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03435676917433739\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.009767364710569382\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01593696139752865\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03457079082727432\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.055715203285217285\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.043394461274147034\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.043700896203517914\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08160893619060516\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06418176740407944\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.021361999213695526\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.039969027042388916\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.049516040831804276\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.032343581318855286\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.050514161586761475\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.09166987985372543\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02768382802605629\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04868101701140404\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.028227047994732857\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.061055269092321396\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06785425543785095\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0518721342086792\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0484871082007885\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06127540394663811\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07425776869058609\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.0661655142903328\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.03995861858129501\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06075499206781387\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.112000472843647\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.05371598154306412\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04039158299565315\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04318460822105408\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.09971319139003754\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.05416886880993843\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06928238272666931\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0407266803085804\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07582666724920273\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06848996132612228\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07641054689884186\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.08819133043289185\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.08368861675262451\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.07922488451004028\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.11942978203296661\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.07618934661149979\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.10415920615196228\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.07848179340362549\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0870625451207161\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.0744238868355751\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07486485689878464\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.07028313726186752\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.1266271024942398\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.10176092386245728\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.09410257637500763\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.13488392531871796\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.09593795984983444\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.14398464560508728\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.1268889158964157\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.09284920990467072\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.11978716403245926\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.15936768054962158\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.15272533893585205\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.13678045570850372\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.1282341331243515\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.11794883757829666\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.14551189541816711\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.10072872042655945\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.10916915535926819\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.08989724516868591\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.11982917040586472\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.14660333096981049\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.1416490375995636\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.16139233112335205\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.12693119049072266\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.14200906455516815\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.08965450525283813\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.12443822622299194\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.17826154828071594\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.19635891914367676\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.15934637188911438\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.1925235092639923\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.18725228309631348\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.09094275534152985\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.19299760460853577\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.17975690960884094\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.16494868695735931\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.21749550104141235\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.31410256028175354\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.19473214447498322\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.20857234299182892\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.1474577784538269\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.20736487209796906\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.1883137822151184\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.20836620032787323\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.23094525933265686\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.17721019685268402\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.21437813341617584\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.12352395057678223\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.19909976422786713\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.23701104521751404\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.14178122580051422\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.22060464322566986\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.13962174952030182\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.23360945284366608\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.17917877435684204\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.1561378389596939\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.14981727302074432\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.23074859380722046\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.21442165970802307\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.15653309226036072\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.18385049700737\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.23767869174480438\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.2381504476070404\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.29301556944847107\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.242752343416214\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.21190431714057922\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.23042826354503632\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.22834157943725586\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.20955121517181396\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.1407543569803238\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.251836359500885\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.2449824959039688\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.17125453054904938\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.36385059356689453\n",
      "Training accuracy 0.875\n",
      "Training loss: 0.2343171089887619\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.23831214010715485\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.23998552560806274\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.24286819994449615\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.21115434169769287\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.31955283880233765\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.25624996423721313\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.3189341723918915\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.2895057797431946\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.20461970567703247\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.24873597919940948\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.3118799924850464\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.27248454093933105\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.29559141397476196\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.30788537859916687\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.29655027389526367\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.33280637860298157\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.24607199430465698\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.358474999666214\n",
      "Training accuracy 0.8671875\n",
      "Training loss: 0.292881041765213\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.27590253949165344\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.19733397662639618\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.32057830691337585\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.29370245337486267\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.31364595890045166\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.26687565445899963\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.24483714997768402\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.2590108811855316\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.2888554036617279\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.3085341155529022\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.2104615420103073\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.2922775149345398\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.309426486492157\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.29203933477401733\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.32206740975379944\n",
      "Training accuracy 0.875\n",
      "Training loss: 0.3313148319721222\n",
      "Training accuracy 0.859375\n",
      "Training loss: 0.20290175080299377\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.2906987965106964\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.29858335852622986\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.2886672914028168\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.3170166313648224\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.28591111302375793\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.30518943071365356\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.26103800535202026\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.25977060198783875\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.3117227852344513\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.19928957521915436\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.2391493022441864\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.2773868441581726\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.28972265124320984\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.4140152633190155\n",
      "Training accuracy 0.83984375\n",
      "Training loss: 0.26316946744918823\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.308675080537796\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.2607254385948181\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.32873106002807617\n",
      "Training accuracy 0.875\n",
      "Training loss: 0.2658168375492096\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.2846987545490265\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.29351186752319336\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.27022838592529297\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.25261926651000977\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.24466846883296967\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.3012343943119049\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.3856164216995239\n",
      "Training accuracy 0.87890625\n",
      "Training loss: 0.29414865374565125\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.28841444849967957\n",
      "Training accuracy 0.875\n",
      "Training loss: 0.3343392610549927\n",
      "Training accuracy 0.87890625\n",
      "Training loss: 0.33681774139404297\n",
      "Training accuracy 0.87890625\n",
      "Training loss: 0.35775822401046753\n",
      "Training accuracy 0.87109375\n",
      "Training loss: 0.35392364859580994\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.38171935081481934\n",
      "Training accuracy 0.8671875\n",
      "Training loss: 0.36899349093437195\n",
      "Training accuracy 0.859375\n",
      "Training loss: 0.3847738802433014\n",
      "Training accuracy 0.859375\n",
      "Training loss: 0.2829253673553467\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.37480518221855164\n",
      "Training accuracy 0.87109375\n",
      "Training loss: 0.27159181237220764\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.28019845485687256\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.3036348521709442\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.2335558384656906\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.3524058759212494\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.31470251083374023\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.28917744755744934\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.3305668234825134\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.3428269326686859\n",
      "Training accuracy 0.859375\n",
      "Training loss: 0.22292892634868622\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.3112535774707794\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.3517320454120636\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.2844233512878418\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.3078548014163971\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.2995973229408264\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.31192415952682495\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.2998635172843933\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.30956968665122986\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.31638723611831665\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.36372193694114685\n",
      "Training accuracy 0.87890625\n",
      "Training loss: 0.2791522145271301\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.43148794770240784\n",
      "Training accuracy 0.8671875\n",
      "Training loss: 0.3138667345046997\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.31917035579681396\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.3015868365764618\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.36610910296440125\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.37016376852989197\n",
      "Training accuracy 0.87109375\n",
      "Training loss: 0.2892582416534424\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.3321990370750427\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.3016389310359955\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.3251495063304901\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.2942151427268982\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.2520717978477478\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.3367043137550354\n",
      "Training accuracy 0.89453125\n",
      "Training loss: 0.2775770425796509\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.34615689516067505\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.25507745146751404\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.4097648859024048\n",
      "Training accuracy 0.8828125\n",
      "Training loss: 0.3522354066371918\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.32967108488082886\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.20838212966918945\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.2925167381763458\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.31565409898757935\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.3234776258468628\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.37268224358558655\n",
      "Training accuracy 0.890625\n",
      "Training loss: 0.28694936633110046\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.3175272047519684\n",
      "Training accuracy 0.8984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 11/36 [43:19<1:38:02, 235.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.1895, acc: 0.3000\n",
      "eval loss 3.1894941767677665\n",
      "Training loss: 0.1304357498884201\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.15480121970176697\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.17149251699447632\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.1595064103603363\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.16871154308319092\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.18390201032161713\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.16670790314674377\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.19150763750076294\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.11309415847063065\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.1842956691980362\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.18221083283424377\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.19685232639312744\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.1413879543542862\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.1663891226053238\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.16730870306491852\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.17666928470134735\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.12543775141239166\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.13801555335521698\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.15159685909748077\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.11742734909057617\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.15868647396564484\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.17245185375213623\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.16296249628067017\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.18138138949871063\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.14038719236850739\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.12468212097883224\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.22861644625663757\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.15707553923130035\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.15099163353443146\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.1675163209438324\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.17767329514026642\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.1275438666343689\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.13507795333862305\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.1494850069284439\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.18042777478694916\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.17672878503799438\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.17051546275615692\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.15174657106399536\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.16965638101100922\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.16386601328849792\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.14030686020851135\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.15056182444095612\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.13908204436302185\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.18867577612400055\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.1966262012720108\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.17794060707092285\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.12108466029167175\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.14781951904296875\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.2270819991827011\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.13381297886371613\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.1291801929473877\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.13203567266464233\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.13456013798713684\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.16240492463111877\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.15846934914588928\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.1636393815279007\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.1466294676065445\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.1896880865097046\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.20265838503837585\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.19341236352920532\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.18640699982643127\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.19190505146980286\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.1426847130060196\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.23658685386180878\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.10473202913999557\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.14588294923305511\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.22475674748420715\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.14977027475833893\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.18309445679187775\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.21042339503765106\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.18378356099128723\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.15923292934894562\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.1912631392478943\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.1838659644126892\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.12883765995502472\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.19007253646850586\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.17315764725208282\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.1874891221523285\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.17799603939056396\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.1977834701538086\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.17647674679756165\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.18291190266609192\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.12991701066493988\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.13854292035102844\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.1861030012369156\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.13185052573680878\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.1330365389585495\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.213032066822052\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.16917842626571655\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.12246675044298172\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.12265276163816452\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.16356919705867767\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.19382835924625397\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.16365382075309753\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.13595570623874664\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.163337841629982\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.1883573979139328\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.2532951235771179\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.1493385285139084\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.19148963689804077\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.13534551858901978\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.16241750121116638\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.20650935173034668\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.12914657592773438\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.2415633648633957\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.1794108748435974\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.16938276588916779\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.19124776124954224\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.15063489973545074\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.16147412359714508\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.13532105088233948\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.21582303941249847\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.1429942548274994\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.17691540718078613\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.2045516073703766\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.15745851397514343\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.15369278192520142\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.20991283655166626\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.15275314450263977\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.19139252603054047\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.19150318205356598\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.19913682341575623\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.2254687398672104\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.17629674077033997\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.19595745205879211\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.19835972785949707\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.16578900814056396\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.2082253098487854\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.24727500975131989\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.1397094577550888\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.1647282838821411\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.20106667280197144\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.20843693614006042\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.16899557411670685\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.17995722591876984\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.17338119447231293\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.2465803623199463\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.23582307994365692\n",
      "Training accuracy 0.90625\n",
      "Training loss: 0.2326190322637558\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.1927453875541687\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.19523516297340393\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.1520843505859375\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.20485955476760864\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.15215839445590973\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.2442282885313034\n",
      "Training accuracy 0.90234375\n",
      "Training loss: 0.22969147562980652\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.2267601042985916\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.1833949238061905\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.18131330609321594\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.12952947616577148\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.23084157705307007\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.14468441903591156\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.11230452358722687\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.21763454377651215\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.1735970675945282\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.25828084349632263\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.21152210235595703\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.1549905389547348\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.14837339520454407\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.15024758875370026\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.20430676639080048\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.17631565034389496\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.21065528690814972\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.21413268148899078\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.18890590965747833\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.17052067816257477\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.20698508620262146\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.23190712928771973\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.14446274936199188\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.28576895594596863\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.14854426681995392\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.2369474470615387\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.17624157667160034\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.2283388078212738\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.20390482246875763\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.20990781486034393\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.15348771214485168\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.23379962146282196\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.15730535984039307\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.17690114676952362\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.18958112597465515\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.13399742543697357\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.21396838128566742\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.2006998360157013\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.22096572816371918\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.19693510234355927\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.19477441906929016\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.1555214524269104\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.20256219804286957\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.18146657943725586\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.22376714646816254\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.21377062797546387\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.23733943700790405\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.1791999191045761\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.14889077842235565\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.1762305647134781\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.210531085729599\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.17730095982551575\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.15265454351902008\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.1808317005634308\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.1563986986875534\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.27893146872520447\n",
      "Training accuracy 0.88671875\n",
      "Training loss: 0.21818026900291443\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.17114761471748352\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.23398004472255707\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.22649045288562775\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.13220779597759247\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.20119261741638184\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.1431341916322708\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.15864776074886322\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.2635204792022705\n",
      "Training accuracy 0.9140625\n",
      "Training loss: 0.1751643419265747\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.22325891256332397\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.19513966143131256\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.2040405124425888\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.2918860912322998\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.14834408462047577\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.12633024156093597\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.13882170617580414\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.1725972294807434\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.17981208860874176\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.19857104122638702\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.1443445086479187\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.1808672398328781\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.19415953755378723\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.20548361539840698\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.18824172019958496\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.19570600986480713\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.16982556879520416\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.18348106741905212\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.22995959222316742\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.14528782665729523\n",
      "Training accuracy 0.9375\n",
      "Training loss: 0.16074441373348236\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.2137463241815567\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.23821574449539185\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.16416719555854797\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.23927956819534302\n",
      "Training accuracy 0.91015625\n",
      "Training loss: 0.20777162909507751\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.24876995384693146\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.22932448983192444\n",
      "Training accuracy 0.91796875\n",
      "Training loss: 0.254813551902771\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.2189021110534668\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.22163796424865723\n",
      "Training accuracy 0.921875\n",
      "Training loss: 0.17720307409763336\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.12025701999664307\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.2062351107597351\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.1675473302602768\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.22087325155735016\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.2402561604976654\n",
      "Training accuracy 0.8984375\n",
      "Training loss: 0.16522806882858276\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.22564943134784698\n",
      "Training accuracy 0.93359375\n",
      "Training loss: 0.24746884405612946\n",
      "Training accuracy 0.92578125\n",
      "Training loss: 0.15323860943317413\n",
      "Training accuracy 0.9453125\n",
      "Training loss: 0.1408107578754425\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.23902080953121185\n",
      "Training accuracy 0.9296875\n",
      "Training loss: 0.20803605020046234\n",
      "Training accuracy 0.921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 12/36 [47:18<1:34:38, 236.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.5625, acc: 0.3104\n",
      "eval loss 3.562492902390659\n",
      "Training loss: 0.04083238169550896\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0671699196100235\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03916657716035843\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08475658297538757\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.057890042662620544\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07676852494478226\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.05268292874097824\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0493815541267395\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06251924484968185\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07781022787094116\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04713744297623634\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06631139665842056\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03537100553512573\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03612026944756508\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06526023149490356\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.05105290561914444\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.052954450249671936\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06946148723363876\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03931321203708649\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0544133335351944\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04927355796098709\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07119807600975037\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04855524003505707\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.058439016342163086\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06938987970352173\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07569937407970428\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.07759910076856613\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.044254086911678314\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07878590375185013\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.028982272371649742\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07767876237630844\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06915368139743805\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.051051121205091476\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04276183992624283\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04257389158010483\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.061677757650613785\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05322141572833061\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03176727145910263\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.09077370911836624\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.042113710194826126\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0517863854765892\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08189736306667328\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.057816289365291595\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03096598945558071\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06015338376164436\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.060787051916122437\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.10198166221380234\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.05314367264509201\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07460540533065796\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.039560772478580475\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03420238196849823\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05673852190375328\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.056704189628362656\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.09287703037261963\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05013187602162361\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05711549520492554\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04693473502993584\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08311687409877777\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0573870986700058\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.10555935651063919\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0382387712597847\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03261638060212135\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05905882269144058\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.033937592059373856\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.059875015169382095\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.061008140444755554\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0673026293516159\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05321824178099632\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.049036912620067596\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05125802010297775\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06760013848543167\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08515816926956177\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.07646200060844421\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0778442770242691\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.08077924698591232\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.07256556302309036\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.061275407671928406\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04749005287885666\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0424363911151886\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.029971549287438393\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.08122347295284271\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05188726633787155\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04384273290634155\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04336355999112129\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03703939542174339\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05746264383196831\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.048581674695014954\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04239102452993393\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03942449018359184\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0764944925904274\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04521552845835686\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07889365404844284\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04314101114869118\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07211492210626602\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.07788354158401489\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.047411974519491196\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07780177891254425\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03877005726099014\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07902615517377853\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04420383274555206\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.082884781062603\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05028554052114487\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07262851297855377\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.12102881819009781\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.04229976236820221\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.08408748358488083\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.07167699933052063\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.05450867488980293\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08235891908407211\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.10333763808012009\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.08814370632171631\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0502246730029583\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05243800953030586\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07643433660268784\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.05488793924450874\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05364173650741577\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.058571621775627136\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03772257640957832\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04288220405578613\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07883373647928238\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.08715784549713135\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.07089458405971527\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04255933314561844\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.045895129442214966\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.069242924451828\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04378007724881172\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04753356799483299\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05057466775178909\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08939965814352036\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04409835487604141\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07721229642629623\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07247881591320038\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06529855728149414\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.059119753539562225\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07736678421497345\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.05904871225357056\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06028217077255249\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.1047590896487236\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.06826888769865036\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.061829824000597\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.057760126888751984\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05294390395283699\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.09730705618858337\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.06743389368057251\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.04662322625517845\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04724093899130821\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.059685833752155304\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.059973765164613724\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07504487037658691\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.10572955012321472\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.08732213824987411\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.09433755278587341\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.058085571974515915\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06172700598835945\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0574069544672966\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.11402717977762222\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.09342757612466812\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.09374246746301651\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05902811884880066\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04263266548514366\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06987326592206955\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0534658320248127\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.1048344150185585\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.061084527522325516\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03585762158036232\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06030068174004555\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04034944623708725\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07091973721981049\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0416681170463562\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.09701564908027649\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.07207849621772766\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07339728623628616\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04585624858736992\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.08022086322307587\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.07955004274845123\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.08915270864963531\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.056506309658288956\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06772611290216446\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07018493860960007\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05612055957317352\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.09379403293132782\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.06273458898067474\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08487561345100403\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.046347107738256454\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.08880089223384857\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.06686950474977493\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05264681205153465\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.09126438200473785\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.07281748205423355\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.04534883424639702\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.055102284997701645\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.051277413964271545\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.09554535150527954\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.0859675407409668\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06457170844078064\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05344099551439285\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06627430766820908\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04664809629321098\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05008259788155556\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0814288780093193\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.06932741403579712\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04737698286771774\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.10612863302230835\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.05314188450574875\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07845288515090942\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.12778417766094208\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.049179770052433014\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05631377547979355\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06663002073764801\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.07053955644369125\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05998798459768295\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07710259407758713\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.055456891655921936\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.1250736266374588\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.05780138447880745\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.055586785078048706\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02974138967692852\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05022209882736206\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06750833988189697\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03789016604423523\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0793023407459259\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06284786760807037\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.05475647374987602\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.09710883349180222\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.08758915215730667\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.057102229446172714\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0579814575612545\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.14486874639987946\n",
      "Training accuracy 0.94140625\n",
      "Training loss: 0.07325994968414307\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06510449945926666\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06884773075580597\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0499454140663147\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.049300532788038254\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.09491420537233353\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06464830040931702\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.12623459100723267\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.07033216953277588\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07779078185558319\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.13498854637145996\n",
      "Training accuracy 0.953125\n",
      "Training loss: 0.04160301387310028\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07626920938491821\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.06600119918584824\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.053545624017715454\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07956129312515259\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.08459872007369995\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.06106457859277725\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.040061790496110916\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06179427728056908\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07496334612369537\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.12245459109544754\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.054872285574674606\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07867724448442459\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.10055641084909439\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.08147259056568146\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.08392154425382614\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.058323752135038376\n",
      "Training accuracy 0.984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 13/36 [51:20<1:31:14, 238.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.1927, acc: 0.2692\n",
      "eval loss 4.1926792385056615\n",
      "Training loss: 0.03826669231057167\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04281312972307205\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02858506329357624\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.042931582778692245\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01940191723406315\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02495461516082287\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03440951183438301\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03612170368432999\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02779502607882023\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0331118181347847\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01678944192826748\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.043265122920274734\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03379516303539276\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027522115036845207\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01672903448343277\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.030824093148112297\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02553684078156948\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04125059023499489\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.019806159660220146\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05305113270878792\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.046995919197797775\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.029979608952999115\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03823070600628853\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01663622446358204\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03474513813853264\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02341543510556221\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04057113081216812\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.051742956042289734\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02809814177453518\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03472217917442322\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.043518003076314926\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02247266098856926\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017901098355650902\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.021458230912685394\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.034302160143852234\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02734442427754402\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.020999623462557793\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03045186772942543\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.035530444234609604\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022646071389317513\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025013282895088196\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027528977021574974\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02116585150361061\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01930711604654789\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015839604660868645\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.013114532455801964\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.018379341810941696\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.037133749574422836\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025026338174939156\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025327734649181366\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.041320640593767166\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.027150852605700493\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030729498714208603\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028409266844391823\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04585730656981468\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025089822709560394\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021640749648213387\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02093876153230667\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03890843689441681\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.025542864575982094\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03100312128663063\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.033245958387851715\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03063385933637619\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.027271492406725883\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03428225219249725\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030014656484127045\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05622290074825287\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03249119594693184\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03530482202768326\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021516287699341774\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022356197237968445\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.010802243836224079\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03812674432992935\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01704709604382515\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02520064264535904\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01743571273982525\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02892110124230385\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029381804168224335\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.035151369869709015\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012694294564425945\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03583298623561859\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.026630893349647522\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02402580715715885\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013802483677864075\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.027527157217264175\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.035763341933488846\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.032200977206230164\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.021875232458114624\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01503534335643053\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04460464417934418\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02981502376496792\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.027765333652496338\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011683622375130653\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.023671578615903854\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017644505947828293\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008384302258491516\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02879878506064415\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02098490297794342\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03343992680311203\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009740238077938557\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.051781270653009415\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.036008842289447784\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.035303473472595215\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028056304901838303\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.053879790008068085\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.051822807639837265\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.024588700383901596\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03016245737671852\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030208434909582138\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022825399413704872\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0288059301674366\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022554593160748482\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.011625921353697777\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0512680858373642\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023434802889823914\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04544106498360634\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03488721698522568\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03082355484366417\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03916778787970543\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019021598622202873\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03980197012424469\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.031017638742923737\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016116652637720108\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02688187174499035\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014906530268490314\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.06241631507873535\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.024613190442323685\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04779134318232536\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.038620028644800186\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.031042516231536865\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024552013725042343\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03252030164003372\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.028430428355932236\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05267378315329552\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02815268002450466\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03722686693072319\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04166944697499275\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.040750645101070404\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.026901422068476677\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04378102719783783\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.024443648755550385\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016285736113786697\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01595059409737587\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03166079893708229\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.020926089957356453\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.029442619532346725\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03631970286369324\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.031189745292067528\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03957938402891159\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.038658056408166885\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.029513230547308922\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04557293280959129\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.018330318853259087\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03986813500523567\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.012553445994853973\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.024555813521146774\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06428420543670654\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.043876465409994125\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.033165182918310165\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026103822514414787\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.046461321413517\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.012203103862702847\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.027681250125169754\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014134230092167854\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012296268716454506\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.020983563736081123\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.031752750277519226\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04345666617155075\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03591788560152054\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.08802551031112671\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.01410645805299282\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05695374682545662\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.025625362992286682\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03189360722899437\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03072264790534973\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015795614570379257\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.06708994507789612\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.027348386123776436\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024777550250291824\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.058853551745414734\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03012174367904663\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.041227661073207855\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0367114394903183\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.037575241178274155\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03185361251235008\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.017438888549804688\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019255073741078377\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04495088383555412\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.020895864814519882\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025085309520363808\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.013514650985598564\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.036308757960796356\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.014199325814843178\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05359313637018204\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07665088027715683\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04949909821152687\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.032113753259181976\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05666195973753929\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05331502854824066\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.018247084692120552\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.036435674875974655\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02885626256465912\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03815247491002083\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04971148446202278\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.01835988461971283\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.054770614951848984\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03830082342028618\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.043037012219429016\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04014771059155464\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025918493047356606\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0529182143509388\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02935783565044403\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06618980318307877\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0745018720626831\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.03892431780695915\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0430750846862793\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021821599453687668\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.046639956533908844\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.035205841064453125\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0514618344604969\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02705676667392254\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.032924775034189224\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.041587475687265396\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04413401708006859\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03258047252893448\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05260816588997841\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01490401104092598\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02327656000852585\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02179521508514881\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.034240514039993286\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.033442843705415726\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0366913303732872\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04478440806269646\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.027224287390708923\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04241517186164856\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03309418633580208\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.045206218957901\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.046726617962121964\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.015935156494379044\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04811267554759979\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.039127349853515625\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.041244249790906906\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.015973463654518127\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.032344136387109756\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07359681278467178\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.049240343272686005\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03071977011859417\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.056961290538311005\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04455994442105293\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02810746245086193\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03916552662849426\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05076400935649872\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07438519597053528\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08307665586471558\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.027655042707920074\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05345248058438301\n",
      "Training accuracy 0.98046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 14/36 [55:20<1:27:28, 238.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.8048, acc: 0.2821\n",
      "eval loss 4.804752225987613\n",
      "Training loss: 0.036699987947940826\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01869288831949234\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04543713852763176\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.008920528925955296\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022005299106240273\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.00961622316390276\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.016956429928541183\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.021088657900691032\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015994170680642128\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022844232618808746\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02426445297896862\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011734743602573872\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.015752658247947693\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018529796972870827\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04660838469862938\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020033974200487137\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015048432163894176\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0073109641671180725\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03629111498594284\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012165570631623268\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01647220365703106\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013005373068153858\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030274536460638046\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.007724349852651358\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.043176256120204926\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.005922453012317419\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.023067187517881393\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013064309023320675\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.033489685505628586\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.022221798077225685\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03785045072436333\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014822190627455711\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03619598597288132\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04861133545637131\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.009054314345121384\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.022858386859297752\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0044858334586024284\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.013194095343351364\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01827705278992653\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00923816580325365\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012403800152242184\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.024526584893465042\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.012308311648666859\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0107843903824687\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.018543856218457222\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017610428854823112\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05226666480302811\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009931405074894428\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.019047260284423828\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.019381342455744743\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.012359023094177246\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017285771667957306\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.005613493733108044\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03394323214888573\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.013238080777227879\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008497966453433037\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.028134552761912346\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01223678421229124\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.020818300545215607\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014758587814867496\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.016864217817783356\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03037387877702713\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01720789447426796\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.020717725157737732\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.012157083489000797\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.040987856686115265\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.010010957717895508\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03531415015459061\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022190771996974945\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01627267338335514\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0059172105975449085\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.013004744425415993\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03598694130778313\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023242218419909477\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03023097850382328\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020616229623556137\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016153080388903618\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015113621950149536\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.015100964345037937\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04599953070282936\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02039276622235775\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013486526906490326\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.007963542826473713\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.021827159449458122\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.007163249887526035\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.023904860019683838\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.018987614661455154\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018438953906297684\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01128457486629486\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015037325210869312\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.011666318401694298\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02258443832397461\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012378260493278503\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015874095261096954\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01824156567454338\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02601313218474388\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018638037145137787\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02140575647354126\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009956269524991512\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.048156436532735825\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.052926111966371536\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.008720233105123043\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017736533656716347\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01967000402510166\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01527850329875946\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022386103868484497\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.016902312636375427\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00874822586774826\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009342359378933907\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012335422448813915\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.021903283894062042\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017320483922958374\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03471039608120918\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023530924692749977\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03346462920308113\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.014785682782530785\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03635792061686516\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06345909833908081\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01551846694201231\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.014403044246137142\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.011214851401746273\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02106892690062523\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0686686560511589\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.02857360988855362\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02557678148150444\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015629546716809273\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015633821487426758\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.029256556183099747\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016875823959708214\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026109853759407997\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015053174458444118\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.025688013061881065\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024302082136273384\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029397038742899895\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030602917075157166\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03060118854045868\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01900256611406803\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01342499814927578\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04368976876139641\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0511416532099247\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.017533540725708008\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.044311121106147766\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04036257043480873\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02072203904390335\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02182624861598015\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03146258741617203\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020781992003321648\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.033976394683122635\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.049577903002500534\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02706107124686241\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04762360081076622\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.057663097977638245\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.029085300862789154\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03533874824643135\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.011596664786338806\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.011524414643645287\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.019475366920232773\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011493396945297718\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03484422713518143\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019272424280643463\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022443870082497597\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03891207277774811\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03672138974070549\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011947404593229294\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017358265817165375\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.026375414803624153\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04614729434251785\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03030342049896717\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03130219504237175\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02379339188337326\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01767735555768013\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024405445903539658\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013182307593524456\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02664133906364441\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03133517503738403\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05111772194504738\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.013323575258255005\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.027036631479859352\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04426945373415947\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02670283429324627\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03667549043893814\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.017243506386876106\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.042245347052812576\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018712490797042847\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.020008040592074394\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03698296472430229\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.035558536648750305\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08923792839050293\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.026907872408628464\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03550831228494644\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04040342569351196\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013652942143380642\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015832465142011642\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.050171781331300735\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.029069026932120323\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008719113655388355\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04970265179872513\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.036858003586530685\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022642606869339943\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04215967282652855\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01621699519455433\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.019821371883153915\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0576864555478096\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03192640468478203\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016980068758130074\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030393164604902267\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03996466100215912\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02730671502649784\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023295117542147636\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02354675717651844\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.039129409939050674\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.017541537061333656\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04713215306401253\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03411844000220299\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02349008247256279\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.016298983246088028\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02468150295317173\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.032138846814632416\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020540885627269745\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02747921459376812\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.050012681633234024\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03961821272969246\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04164450615644455\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03658556938171387\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03744830936193466\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02336851879954338\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04755670577287674\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02806827239692211\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0529266893863678\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03405951336026192\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.037809818983078\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03517818823456764\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06961004436016083\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.045752447098493576\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04081118106842041\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.021398182958364487\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03813885152339935\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.052057623863220215\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02820812724530697\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05265286937355995\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.033177394419908524\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029603634029626846\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017603551968932152\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06864426285028458\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05963227152824402\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03644275665283203\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05029015988111496\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.055534493178129196\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.048465583473443985\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028153708204627037\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02507530339062214\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.031023459509015083\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08383523672819138\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.04369101673364639\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03957507386803627\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04733432084321976\n",
      "Training accuracy 0.98046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 15/36 [59:20<1:23:41, 239.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.0482, acc: 0.3222\n",
      "eval loss 4.048169016838074\n",
      "Training loss: 0.026265541091561317\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011309424415230751\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01517704501748085\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.052010323852300644\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.021365517750382423\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03357347473502159\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026568369939923286\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02700689062476158\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009151950478553772\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.015627412125468254\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01573213003575802\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.020560240373015404\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.018363595008850098\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0065746065229177475\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01795056089758873\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.006048633251339197\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.013390368781983852\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.019537003710865974\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04077450558543205\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.007274843752384186\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.014123182743787766\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01721981167793274\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011162188835442066\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.017924219369888306\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009833323769271374\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008565221913158894\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012259284034371376\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.017685841768980026\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01292125228792429\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017819318920373917\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025547830387949944\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.007117559667676687\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.019969098269939423\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016993075609207153\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04625694826245308\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.014231563545763493\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0334172323346138\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.012754327617585659\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030387282371520996\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02396504022181034\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030145347118377686\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.005876257084310055\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.025408554822206497\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03033261187374592\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025110388174653053\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03597791865468025\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013361899182200432\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.014867456629872322\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.026398329064249992\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.014586875215172768\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03314696252346039\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.014452658593654633\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027534371241927147\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04273209720849991\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.010920902714133263\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.025562290102243423\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0535242035984993\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04065364971756935\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.015029293484985828\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.020029468461871147\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014328301884233952\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.012952372431755066\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.024278849363327026\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.020736567676067352\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017781613394618034\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.009393704123795033\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.013427816331386566\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03303343802690506\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03337649255990982\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02225087210536003\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020807193592190742\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022447163239121437\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027469750493764877\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.045538756996393204\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.011030830442905426\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.033404894173145294\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02167319878935814\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.022954657673835754\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019612621515989304\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013811417855322361\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009127739816904068\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04649147763848305\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0083331149071455\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.045767586678266525\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.029289741069078445\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.006092044059187174\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03461087867617607\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03573396056890488\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030698053538799286\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.020071715116500854\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06290162354707718\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.011716206558048725\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017019139602780342\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01970551162958145\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.052036888897418976\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01862243004143238\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025691000744700432\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01223173551261425\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011753452941775322\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03856516629457474\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0291006937623024\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015350458212196827\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.027149556204676628\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018184129148721695\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.028037190437316895\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.045608650892972946\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02332223579287529\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.020185472443699837\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04055523872375488\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03622862324118614\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.012218636460602283\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.046696070581674576\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.028995778411626816\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04084094986319542\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02047199383378029\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.045181624591350555\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.016811275854706764\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.007911695167422295\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03267287090420723\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012899171561002731\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.024020131677389145\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030092138797044754\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.040862955152988434\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.031157899647951126\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02099696360528469\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07438385486602783\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.016318853944540024\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03482875972986221\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04674013331532478\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.009251227602362633\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03237466514110565\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0565960556268692\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018669649958610535\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023789415135979652\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.039026666432619095\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028220659121870995\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.029966820031404495\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05830707401037216\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04158162698149681\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.044941455125808716\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028497815132141113\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023165805265307426\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.055193379521369934\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021130643784999847\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.019084157422184944\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02539653703570366\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.040521807968616486\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02825363352894783\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02558773197233677\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02428518608212471\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029316019266843796\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011453235521912575\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.009339495562016964\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03973165899515152\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.019079595804214478\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030287539586424828\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03917694464325905\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.020934682339429855\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.040703993290662766\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04232214018702507\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011184505186975002\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.024625547230243683\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04028632119297981\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.056386131793260574\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05524760112166405\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.042167920619249344\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.023225752636790276\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06455419957637787\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0295180082321167\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05971156805753708\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.025635313242673874\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.038343314081430435\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.051626723259687424\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.059017620980739594\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.04267188534140587\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030456354841589928\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05355435237288475\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05472277104854584\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03783096745610237\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03855729475617409\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03695731610059738\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0461205318570137\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.053780581802129745\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.036117296665906906\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02892570197582245\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05067397281527519\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04021952301263809\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07298701256513596\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02812962606549263\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01710309274494648\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05378158390522003\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.044204238802194595\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.051870767027139664\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.020703090354800224\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0507526770234108\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.014521987177431583\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.032643161714076996\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.045359861105680466\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03270217031240463\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03176557645201683\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0363432876765728\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05885942652821541\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06052678823471069\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04497569426894188\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01720340922474861\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.061909936368465424\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028672896325588226\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06509923934936523\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05995216965675354\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.037591252475976944\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04104712978005409\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023722123354673386\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07009029388427734\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.027247821912169456\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.038221731781959534\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.026027869433164597\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07926038652658463\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06660588830709457\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08016578108072281\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.039246462285518646\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.022133799269795418\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03405513986945152\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05360404774546623\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.11079009622335434\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.05637213960289955\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05097740516066551\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03998987376689911\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.031181035563349724\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030054641887545586\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05038854852318764\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.060597654432058334\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.013188889250159264\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.057439565658569336\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05584756284952164\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.028107743710279465\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04756801947951317\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028454191982746124\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.033958662301301956\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.079681895673275\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07605979591608047\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03388563171029091\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08622682094573975\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.058704860508441925\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07662277668714523\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05234227329492569\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.059501633048057556\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03363555669784546\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.032492175698280334\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02781827561557293\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.034299444407224655\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07208159565925598\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06107521802186966\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06036480516195297\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0500488243997097\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.09489275515079498\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.10216973721981049\n",
      "Training accuracy 0.95703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 16/36 [1:03:22<1:20:00, 240.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.9346, acc: 0.2966\n",
      "eval loss 4.934556810185313\n",
      "Training loss: 0.023727789521217346\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012813523411750793\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.024094801396131516\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04720081761479378\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.045240774750709534\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05226549506187439\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03548690304160118\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.027029408141970634\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04255477339029312\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.015279827639460564\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.038449905812740326\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022074440494179726\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01765773631632328\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02203194610774517\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015249434858560562\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027498481795191765\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03740489110350609\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.041202981024980545\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04365818202495575\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.033884577453136444\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025786111131310463\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01842380128800869\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014906670898199081\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.011526830494403839\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01881803572177887\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.027230149134993553\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04260130599141121\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.046598609536886215\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02056894823908806\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0270867757499218\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02623232826590538\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.014223680831491947\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022458363324403763\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01918945088982582\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06359997391700745\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.037741973996162415\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021994076669216156\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05903596431016922\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.030368125066161156\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.023273879662156105\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.034710634499788284\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027220439165830612\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019133148714900017\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04561078920960426\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01905108615756035\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021067539229989052\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03341709449887276\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02590656466782093\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022231901064515114\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018701771274209023\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023251494392752647\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03465583547949791\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0320855937898159\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03292311355471611\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05141079053282738\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03432822227478027\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05550835281610489\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03423161432147026\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0359557643532753\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04741775989532471\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.037068285048007965\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05417050048708916\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06517186760902405\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.025179116055369377\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026488032191991806\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02522459253668785\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04062236472964287\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04458066448569298\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02335658296942711\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02396373450756073\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05650695413351059\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01764824613928795\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024886611849069595\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02004549279808998\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.040654201060533524\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02752639539539814\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.052977435290813446\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.01788877509534359\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.023253263905644417\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04113122820854187\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.037767402827739716\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03172275796532631\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04856002703309059\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01707540638744831\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03653349727392197\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01872684620320797\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03863653540611267\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.013186764903366566\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01969035528600216\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03104378655552864\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03787346929311752\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04696909710764885\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05376557260751724\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.019626544788479805\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.045309584587812424\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.020842809230089188\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.028158335015177727\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06070872023701668\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.030788935720920563\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03927042707800865\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.045988038182258606\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026151437312364578\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0400402694940567\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04109332337975502\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05139286071062088\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03425125032663345\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07079283893108368\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.043705571442842484\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024417059496045113\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02394820936024189\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05919777601957321\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03514739125967026\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.034096721559762955\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04490961506962776\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.036580007523298264\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06708986312150955\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.057328179478645325\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03522748500108719\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06518208235502243\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04863293468952179\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.027209343388676643\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01808765158057213\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0559035986661911\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.044035132974386215\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.035905495285987854\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02506367117166519\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021600380539894104\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03383730351924896\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.042656540870666504\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07362314313650131\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.040547892451286316\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07594334334135056\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03614242374897003\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.026445984840393066\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.034933898597955704\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.039546240121126175\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08651253581047058\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.03781482204794884\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05198177322745323\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.038738664239645004\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03152373433113098\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0410543717443943\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.053303312510252\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.017360853031277657\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.037006083875894547\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0635291337966919\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.11398328095674515\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.05658062547445297\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03827180713415146\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04405500739812851\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06177716702222824\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.020067909732460976\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03637256100773811\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02317477948963642\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.11850974708795547\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.05505276843905449\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04354679957032204\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02701856754720211\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04643775895237923\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030415207147598267\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.052317600697278976\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06651106476783752\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03893793001770973\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.10008442401885986\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.044553760439157486\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022661887109279633\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04129944369196892\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06549395620822906\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08030030131340027\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.050822168588638306\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04429769888520241\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03745278716087341\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.021840473636984825\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06476128101348877\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.054995883256196976\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03730686753988266\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0461275614798069\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.057474952191114426\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.023029033094644547\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04992807283997536\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03267564997076988\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03887655958533287\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07904689759016037\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.049186356365680695\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.060266297310590744\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0579320527613163\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.05158817768096924\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08888212591409683\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.03327319398522377\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.043093327432870865\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.042601775377988815\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07920432090759277\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04648151248693466\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.041529398411512375\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06450197100639343\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05192907154560089\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.041077252477407455\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03980584815144539\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.10172208398580551\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02281990833580494\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06255816668272018\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07043398171663284\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.0666591003537178\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03450155630707741\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03834870830178261\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023592721670866013\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.10286975651979446\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.05865219607949257\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06047695130109787\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.050777584314346313\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030036401003599167\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0516575425863266\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06763783097267151\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.051940109580755234\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05221063643693924\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03646348416805267\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07247966527938843\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.05084826424717903\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06397737562656403\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.051072217524051666\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06003587320446968\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05763566493988037\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05003459006547928\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.09735526889562607\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0842791199684143\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.06856254488229752\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0496935173869133\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05806809291243553\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04024146869778633\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03276204317808151\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021363025531172752\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017644695937633514\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.08241485804319382\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06232575327157974\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.09240957349538803\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06719409674406052\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.05844316631555557\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.09838742762804031\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.0727132186293602\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.056013111025094986\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0655343309044838\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05671697109937668\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.032273437827825546\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.049581512808799744\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.057233165949583054\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04910944402217865\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.11032979190349579\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.09189745783805847\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.07086203247308731\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.08237007260322571\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.09155848622322083\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.0823114812374115\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.07500515133142471\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.028007566928863525\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.08920145034790039\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0738183930516243\n",
      "Training accuracy 0.9765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 17/36 [1:07:23<1:16:07, 240.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.9588, acc: 0.2574\n",
      "eval loss 4.958846002817154\n",
      "Training loss: 0.0270893182605505\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.038302868604660034\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023471377789974213\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03648506850004196\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.031005319207906723\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01021621935069561\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.07760127633810043\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02147294580936432\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03447124734520912\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.038116466253995895\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.020195402204990387\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.026038937270641327\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.054186686873435974\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.049087248742580414\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.040485162287950516\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.040612805634737015\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.014871317893266678\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.028688695281744003\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.033989645540714264\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03995555639266968\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03722340986132622\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02119300328195095\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04173111543059349\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05511968210339546\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07341870665550232\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.02760710008442402\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04066810384392738\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.053249843418598175\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06588345021009445\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.028013167902827263\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05516424775123596\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02159891463816166\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.036528151482343674\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03312377259135246\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02114199660718441\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03116123378276825\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04970044642686844\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04046875610947609\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04356013238430023\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05279349908232689\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07578283548355103\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0414661206305027\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.028759976848959923\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05364192649722099\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.061522021889686584\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.053469277918338776\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05160733312368393\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03419763594865799\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.048840105533599854\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06695335358381271\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.020833982154726982\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.020866747945547104\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030262893065810204\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0186903215944767\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03724689036607742\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.053194623440504074\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05388958007097244\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03274523466825485\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0367099903523922\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.027210989966988564\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021690938621759415\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03351952135562897\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.027363097295165062\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.061217695474624634\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.027610410004854202\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04677675664424896\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.036736469715833664\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0529044046998024\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04901754856109619\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.022313902154564857\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05931273102760315\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025230148807168007\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04814939945936203\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.045788247138261795\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05712968856096268\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05985230579972267\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03488285839557648\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05326418578624725\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0332806296646595\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04733983799815178\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.028117571026086807\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030687833204865456\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019648490473628044\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03080679476261139\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.057040922343730927\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0664980560541153\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.04214897006750107\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07855991274118423\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.04228940233588219\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05058763548731804\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08903011679649353\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.05430110543966293\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06508730351924896\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.017261667177081108\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.012675151228904724\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04007110744714737\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04179065674543381\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05642318353056908\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03525887802243233\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04097672924399376\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08694382011890411\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.06429160386323929\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03030296042561531\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04964696243405342\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03154459595680237\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04343641176819801\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.041927557438611984\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06107407808303833\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07294248044490814\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03539653122425079\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023518547415733337\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03998120129108429\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08850744366645813\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05721732974052429\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03723933547735214\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05037783831357956\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03951174020767212\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01972040720283985\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05403360724449158\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.050625044852495193\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05647400766611099\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05399809032678604\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04874561354517937\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.034279562532901764\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04896322265267372\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06793586164712906\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03426387906074524\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04122031852602959\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03805123642086983\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.058793798089027405\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06969113647937775\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.041793689131736755\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08238483965396881\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.025816868990659714\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.047688957303762436\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.055825985968112946\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.07410258799791336\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05107690021395683\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03774605691432953\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.053295087069272995\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.029428871348500252\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05468092858791351\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.054421596229076385\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07313607633113861\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.03313712403178215\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05311893671751022\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06165938079357147\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03696129471063614\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06483320891857147\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.021027754992246628\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03712274879217148\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03652094304561615\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06911364942789078\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.07066695392131805\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.10142495483160019\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.04723544791340828\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.038897283375263214\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.061867859214544296\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06336181610822678\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04919259622693062\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06271243095397949\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05475899577140808\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06952683627605438\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.09852460026741028\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.029997222125530243\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07478832453489304\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04037449136376381\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05397879332304001\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06465896219015121\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.09822157770395279\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.04048251360654831\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06692663580179214\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.035138510167598724\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.053396061062812805\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.054033249616622925\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08491100370883942\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0769459530711174\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.07730467617511749\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.08071502298116684\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.09738675504922867\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.03665338456630707\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06490200012922287\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.09653237462043762\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.09082106500864029\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04714851826429367\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.044983308762311935\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06982257217168808\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.0888124406337738\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.051946498453617096\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05295570194721222\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.09591387212276459\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05880147963762283\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07606857270002365\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.08599037677049637\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.10881221294403076\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.07840859144926071\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.11188509315252304\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.1194295883178711\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.06174920126795769\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.07389652729034424\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06008780002593994\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0734088122844696\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.08733490854501724\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07001394778490067\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.055684350430965424\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.09520938992500305\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.055909980088472366\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05372937023639679\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08180638402700424\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.05106661096215248\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.081080861389637\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.061550483107566833\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.0544893853366375\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07068496197462082\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07933913171291351\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.12457773089408875\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.061367571353912354\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.0858571007847786\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.07880320399999619\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.09141990542411804\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.09969750046730042\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03290437161922455\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08770513534545898\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.10807105153799057\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.08027651906013489\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.05441119521856308\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.11820727586746216\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.080593541264534\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.08490891754627228\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.10451619327068329\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.0722411647439003\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0979294627904892\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.06492730975151062\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07861941307783127\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.13117150962352753\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.06364215165376663\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08132436126470566\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.11942019313573837\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.08555836975574493\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.10713569074869156\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.04750792682170868\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0738944336771965\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05170811340212822\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06276848167181015\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07125253975391388\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.04905233904719353\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.11732903867959976\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.04431487247347832\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.12515351176261902\n",
      "Training accuracy 0.94921875\n",
      "Training loss: 0.1020827665925026\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06476958096027374\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.054903652518987656\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.10455816984176636\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.04938223585486412\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.10302110016345978\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.09009455144405365\n",
      "Training accuracy 0.96484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 18/36 [1:11:17<1:11:33, 238.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.2290, acc: 0.2466\n",
      "eval loss 5.228980267420411\n",
      "Training loss: 0.016785770654678345\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.039019715040922165\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.032328471541404724\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04881339892745018\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03966021165251732\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02033393457531929\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019274655729532242\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.036767493933439255\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03957613185048103\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.037151630967855453\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0319683738052845\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.045821771025657654\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03439369797706604\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.038978882133960724\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06559137254953384\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.030191903933882713\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02885524183511734\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029431262984871864\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025293223559856415\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.024705739691853523\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04922541230916977\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03229755908250809\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.023035289719700813\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04611922800540924\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.031399134546518326\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02427978813648224\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04673284292221069\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.023185303434729576\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04047423601150513\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01713775098323822\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05440457537770271\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03593829646706581\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.024488180875778198\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06793041527271271\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07601360231637955\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07065390050411224\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.059902045875787735\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04991444572806358\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.014496451243758202\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04582810401916504\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04613650590181351\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04407726973295212\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.038506533950567245\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07310248911380768\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.027757763862609863\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04306988790631294\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06443455070257187\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03436361253261566\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01740843430161476\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.041074998676776886\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03420694172382355\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.048455797135829926\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04777263477444649\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.034578293561935425\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02744622901082039\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030389614403247833\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04053005576133728\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.036248862743377686\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0651705265045166\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06516347825527191\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04429766163229942\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04250224307179451\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.051794882863759995\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03095775470137596\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02316853031516075\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03478269279003143\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.026618536561727524\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02533750608563423\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01523540634661913\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05218050256371498\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.017845572903752327\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.036626819521188736\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030071904882788658\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.036808744072914124\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03411857411265373\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05514555796980858\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04247420281171799\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03307058662176132\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02485918439924717\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0572744719684124\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03300343453884125\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04961678758263588\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.023445148020982742\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028992393985390663\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03818408399820328\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0647134780883789\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.009717299602925777\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04927833005785942\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06534542143344879\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04827670007944107\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.039358317852020264\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.046849604696035385\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.017562812194228172\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.05103669688105583\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04128319025039673\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08254782855510712\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.029460454359650612\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06681816279888153\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.11311273276805878\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06792079657316208\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06287287920713425\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.05657671391963959\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02964363992214203\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02740197628736496\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07088641077280045\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.042965009808540344\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029963739216327667\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02388731576502323\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.058914076536893845\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07565242052078247\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06804830580949783\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03841578960418701\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.030652515590190887\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04196671023964882\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030727144330739975\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03290652111172676\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06333500891923904\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025390950962901115\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024256790056824684\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06623132526874542\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.045641932636499405\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.041356004774570465\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07364742457866669\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.054992206394672394\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03369906544685364\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.028966262936592102\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0341925323009491\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028280487284064293\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03165359050035477\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06243731081485748\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04084700718522072\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.044962327927351\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.030235838145017624\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024883951991796494\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04291510209441185\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04175024479627609\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.023858116939663887\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0320417620241642\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.029868049547076225\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0326477587223053\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.042760442942380905\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0410509817302227\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04311446100473404\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.10334613919258118\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.03846303001046181\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04917913302779198\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.017170226201415062\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.051067180931568146\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03857201710343361\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.051690567284822464\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04262600839138031\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.046794090420007706\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03893136978149414\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03887730464339256\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07326692342758179\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04889685660600662\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06589572876691818\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.037174586206674576\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03354450687766075\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.041823919862508774\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08540531247854233\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.045959096401929855\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.021446719765663147\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04090920090675354\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05276797339320183\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.055392857640981674\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.050370119512081146\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04262339323759079\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07966738939285278\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0449640154838562\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.044404566287994385\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03188888356089592\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06222211942076683\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07503720372915268\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.06338614970445633\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04108786955475807\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.11236072331666946\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.055694080889225006\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.043611373752355576\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0457586906850338\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08295301347970963\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.06990054249763489\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04323958232998848\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0680159330368042\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03419582545757294\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05816753953695297\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07337480038404465\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.060323141515254974\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03695809841156006\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07816030830144882\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.05122969299554825\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.09871222078800201\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.045647479593753815\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08165913075208664\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04940028861165047\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.024733541533350945\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.047552432864904404\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.056715305894613266\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06004951894283295\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.057287026196718216\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.08896448463201523\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.031175093725323677\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05927472561597824\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.10308732837438583\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.059160925447940826\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.044954270124435425\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.12357009947299957\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.05616086721420288\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06274817138910294\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.061766307801008224\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.09445051848888397\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.027075637131929398\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.056607067584991455\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07636019587516785\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.02880292572081089\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.050446417182683945\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07979605346918106\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.08531396836042404\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06223407760262489\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.06191309168934822\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.057233043015003204\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.041871167719364166\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.034568656235933304\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06544608622789383\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03475126251578331\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08437883108854294\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07198083400726318\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04776939004659653\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.07054362446069717\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.05195171386003494\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.09477563947439194\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.058986350893974304\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06491195410490036\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03908069059252739\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.049321018159389496\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.052464473992586136\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.10044631361961365\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.04259202256798744\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.09670113772153854\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.09428349137306213\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.1107778251171112\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.09423665702342987\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.05628448352217674\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05175688862800598\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.10689020901918411\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.08192373812198639\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06611833721399307\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.06853383034467697\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.04568237438797951\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03309725597500801\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07839065045118332\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07342982292175293\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0699630007147789\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04398227855563164\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07385089993476868\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.0744338184595108\n",
      "Training accuracy 0.9765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 19/36 [1:15:11<1:07:09, 237.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.7176, acc: 0.2629\n",
      "eval loss 4.717575090005994\n",
      "Training loss: 0.04417812079191208\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029122330248355865\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01486955676227808\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.031561218202114105\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013333387672901154\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04016845300793648\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022155355662107468\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04838261008262634\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04872622340917587\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.010623343288898468\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.066563181579113\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02077600732445717\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014937454834580421\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02197076566517353\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.029080886393785477\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.034356534481048584\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03548005595803261\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01819418929517269\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.027110179886221886\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.038070499897003174\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03283782675862312\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022753195837140083\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.024558529257774353\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02272931858897209\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019664760679006577\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04171596094965935\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04357396438717842\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02400459162890911\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030371081084012985\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03478866070508957\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02893100492656231\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02820924110710621\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.056908026337623596\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03104919008910656\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02718317322432995\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023925716057419777\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027302350848913193\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.010426835156977177\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030890652909874916\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024295207113027573\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02623065561056137\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01658804900944233\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.05302881449460983\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.059655703604221344\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03214138001203537\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020390035584568977\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.024853717535734177\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02705860137939453\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.055091097950935364\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03833427652716637\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.043119460344314575\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.034027423709630966\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02329053357243538\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.032486364245414734\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03989182785153389\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.027744829654693604\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04525718465447426\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.034142546355724335\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.037816718220710754\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07171133160591125\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02500678040087223\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028683828189969063\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021630169823765755\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04676275700330734\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.027460910379886627\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0426347553730011\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.032841019332408905\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06921899318695068\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.061128031462430954\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03886179253458977\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.028112461790442467\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04175746813416481\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.021597223356366158\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.021946117281913757\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01257901731878519\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.032585423439741135\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.025154128670692444\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016605516895651817\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.021421989426016808\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03137711435556412\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04697301983833313\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.018775533884763718\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.049291279166936874\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.009689191356301308\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03593796491622925\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02138402685523033\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04509612172842026\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019162066280841827\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.036891430616378784\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.023750070482492447\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02046235464513302\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06224041059613228\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0410204753279686\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.036308806389570236\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0550752691924572\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.027563543990254402\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019932741299271584\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024877332150936127\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.046075720340013504\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.042160142213106155\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.012919369153678417\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03808299079537392\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02512557990849018\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.043036170303821564\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03519904613494873\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0104011669754982\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.020601151511073112\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04661457613110542\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.030076462775468826\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03996991366147995\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04990161210298538\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02618073858320713\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07513803988695145\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.008291559293866158\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.023056991398334503\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.031094199046492577\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.031075743958353996\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017423458397388458\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.045530810952186584\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04155438765883446\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01989034190773964\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03548393398523331\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022657781839370728\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07746361196041107\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.024870513007044792\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.057089705020189285\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02441306784749031\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04281332343816757\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.020940957590937614\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.045517150312662125\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.033942244946956635\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03002329170703888\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.10419883579015732\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.03913773596286774\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.024914084002375603\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029872633516788483\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029784351587295532\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.043402235954999924\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0715329647064209\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.02047250047326088\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04654882475733757\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04048784449696541\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04121360555291176\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.060952022671699524\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07616446167230606\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.030642524361610413\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.059583839029073715\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.05457839369773865\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.011784741654992104\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.028297265991568565\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.043217193335294724\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08338657021522522\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.07270020991563797\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.03342417627573013\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011876185424625874\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.029375197365880013\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02829570695757866\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06259535998106003\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.033153459429740906\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.039339613169431686\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03130822256207466\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.043278615921735764\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06070772930979729\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08545887470245361\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.04592689871788025\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03457549586892128\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03362510725855827\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013484201394021511\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02479071170091629\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0710991844534874\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.038057420402765274\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.023251758888363838\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.09156462550163269\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.054066289216279984\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05324172228574753\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016841571778059006\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0491827093064785\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03333175182342529\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04152759909629822\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.031249742954969406\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04546481743454933\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04254428669810295\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03327300399541855\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02597222477197647\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.055865686386823654\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.017148412764072418\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06795471161603928\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05789276957511902\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0339813306927681\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02879568189382553\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03921842575073242\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03819480910897255\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.042041976004838943\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04714537411928177\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04479515552520752\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025496065616607666\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.018813462927937508\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015568781644105911\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.048454686999320984\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0256672203540802\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06393164396286011\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04989401996135712\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04544377699494362\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06062769144773483\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03977074846625328\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0532793365418911\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07970254868268967\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.05086958408355713\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06860606372356415\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.06183328479528427\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.0637652799487114\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.045469362288713455\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04649289697408676\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.059414736926555634\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.06450758874416351\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.023063959553837776\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07007483392953873\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.025830566883087158\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07329079508781433\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.056936074048280716\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.063848115503788\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03034181334078312\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028125999495387077\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03684859350323677\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016179043799638748\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05613037571310997\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.03282725438475609\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05869973450899124\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.09516147524118423\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.07234898209571838\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04101311042904854\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05710669606924057\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.09898862987756729\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.06468652188777924\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0894278958439827\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.042328935116529465\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03290257230401039\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06749479472637177\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04249896481633186\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06706137210130692\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.10931240022182465\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.07869214564561844\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07744933664798737\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05802883580327034\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.07013356685638428\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.050346534699201584\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06931103020906448\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.08516959100961685\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.027073487639427185\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07465628534555435\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.05591559410095215\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.09170815348625183\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.040318530052900314\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.047668684273958206\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07612085342407227\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.05604219809174538\n",
      "Training accuracy 0.98046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 20/36 [1:19:07<1:03:05, 236.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.4136, acc: 0.2737\n",
      "eval loss 5.413603626191616\n",
      "Training loss: 0.0075264303013682365\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011782628484070301\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04989263787865639\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02521735057234764\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015755344182252884\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.023941289633512497\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013322858139872551\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.023692740127444267\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04732084274291992\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.025788994506001472\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.020410653203725815\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03896224871277809\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025618702173233032\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01703307405114174\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.018498487770557404\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.048889774829149246\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.025813261047005653\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02605244889855385\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07403787970542908\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.020772309973835945\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04145956039428711\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.019305914640426636\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019748058170080185\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.023343980312347412\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014283891767263412\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.023444296792149544\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.021854698657989502\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03984435275197029\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.015934757888317108\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03939153254032135\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.053832851350307465\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01365684811025858\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013448129408061504\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015311588533222675\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.053157009184360504\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02958744391798973\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.022555911913514137\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024831660091876984\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015790468081831932\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.029728733003139496\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03263917192816734\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.033393844962120056\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024542173370718956\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.039908517152071\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02085903100669384\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.029920069500803947\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018588049337267876\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.041581861674785614\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.033876046538352966\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04397593438625336\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01599227450788021\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029818397015333176\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03477485105395317\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03903009742498398\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03913654014468193\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.040260497480630875\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.014546150341629982\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.061864059418439865\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.028493208810687065\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.035968612879514694\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024465449154376984\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.033348869532346725\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028946299105882645\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06406421214342117\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01978418044745922\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01973213441669941\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030702676624059677\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025345776230096817\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02143201045691967\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.037102676928043365\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0373171828687191\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.030253883451223373\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02421499229967594\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.00658379727974534\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01687747612595558\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00931913685053587\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.024449117481708527\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011824572458863258\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.038944222033023834\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019870277494192123\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.00944074522703886\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.039329737424850464\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.020936915650963783\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025914521887898445\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03983438387513161\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0465456061065197\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02180344983935356\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.044725701212882996\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016888240352272987\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02251594141125679\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03391309082508087\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.027102215215563774\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02207101881504059\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017201611772179604\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020090196281671524\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.059635572135448456\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04521431028842926\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0422532893717289\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02001096121966839\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019264765083789825\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030877994373440742\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.038157686591148376\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03009657748043537\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.042009904980659485\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.026927093043923378\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022175684571266174\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06796502321958542\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.0379166379570961\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.031044745817780495\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03960453346371651\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.029548099264502525\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03509090468287468\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03371993824839592\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.015907209366559982\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04866126924753189\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016608113422989845\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03671129420399666\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.042558372020721436\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04092716798186302\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023913614451885223\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02269142121076584\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.021385356783866882\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.018663473427295685\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.036359235644340515\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02821684442460537\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02467307634651661\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.020064495503902435\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.050405047833919525\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.022790802642703056\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025185728445649147\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018488392233848572\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05620492994785309\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022688955068588257\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028162160888314247\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.055166251957416534\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08699797838926315\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03449036553502083\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04742789641022682\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.010618109256029129\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.034124452620744705\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03732503578066826\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03227367252111435\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.042008306831121445\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03377170115709305\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03120964579284191\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03725626319646835\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.012830310501158237\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0563754178583622\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04641489312052727\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025738516822457314\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03199619799852371\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.08245320618152618\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.028527459129691124\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0778273195028305\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.045455772429704666\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06617417931556702\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.09388680756092072\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.025696536526083946\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05391358584165573\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0413346104323864\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06879094988107681\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.020470378920435905\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05770684406161308\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.09031883627176285\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.035841651260852814\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04867361858487129\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06861385703086853\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03156360983848572\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03801067918539047\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0407252237200737\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.049172237515449524\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04003725200891495\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05385560169816017\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03275731950998306\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04064662382006645\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03347884118556976\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04147311672568321\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04447142407298088\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02097625471651554\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04446804150938988\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0170030165463686\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.028129326179623604\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05838737636804581\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02045268565416336\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02154921367764473\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.047498539090156555\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025044862180948257\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05625496059656143\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.033650681376457214\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.033528927713632584\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02629764750599861\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05558600649237633\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04060986638069153\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.019459499046206474\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05178023502230644\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02109433151781559\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06817742437124252\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0077450452372431755\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.06601550430059433\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.021836519241333008\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.042318060994148254\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04702189937233925\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.024764452129602432\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.018239622935652733\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.014902961440384388\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.10064800828695297\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06975652277469635\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05092739313840866\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07085541635751724\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.023475119844079018\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.08783888071775436\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0364750511944294\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.1030643954873085\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04423036053776741\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04731902480125427\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05251387506723404\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03272976353764534\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.10866092890501022\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07050040364265442\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0391700342297554\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022814041003584862\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04483886808156967\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04172947630286217\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.043213505297899246\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07143142074346542\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05051826313138008\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03143077343702316\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04862408712506294\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026951966807246208\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02730501815676689\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.039609797298908234\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08455464243888855\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.051236238330602646\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.059180181473493576\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.02947392128407955\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04660777747631073\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022360632196068764\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04310128092765808\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.020225869491696358\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06792683154344559\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06744720786809921\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04728809744119644\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.028595980256795883\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07575422525405884\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.05399537459015846\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0725155845284462\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.0457768552005291\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03660118207335472\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.049860548228025436\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04648709297180176\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05411658063530922\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04192819818854332\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06371670961380005\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07203730195760727\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.026592597365379333\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.040605686604976654\n",
      "Training accuracy 0.98828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 21/36 [1:23:01<59:00, 236.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.2052, acc: 0.2742\n",
      "eval loss 5.205181514844298\n",
      "Training loss: 0.03952502831816673\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0240909643471241\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.028194062411785126\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009767269715666771\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03146667033433914\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04101765900850296\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026734396815299988\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02544049359858036\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05341615527868271\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02954944595694542\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02696564793586731\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03360242396593094\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01429042313247919\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018642300739884377\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01682938076555729\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03007299266755581\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02296435832977295\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.020227154716849327\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.021019719541072845\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.033089928328990936\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025343066081404686\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023317469283938408\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02474379353225231\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.044237226247787476\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.015817563980817795\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02251141518354416\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015268426388502121\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0166421290487051\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02732514590024948\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.033041417598724365\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03625553101301193\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0293546412140131\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.038754161447286606\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04315314441919327\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03802977874875069\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03587539494037628\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04459403082728386\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.060185812413692474\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.016219697892665863\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.040815938264131546\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.013912266120314598\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.009068315848708153\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02543569542467594\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03916105628013611\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.014712614938616753\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02086060121655464\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01271958090364933\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03811320289969444\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.024218961596488953\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013712004758417606\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.06454849988222122\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.034963563084602356\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.024211259558796883\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04168613255023956\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04865588620305061\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.012347526848316193\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01295094471424818\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017849138006567955\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.019966090098023415\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.032917581498622894\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.013308941386640072\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01747243106365204\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03679623454809189\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.024245230481028557\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023312941193580627\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.041589803993701935\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02235596626996994\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03005986101925373\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0779576376080513\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.027432601898908615\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04736559838056564\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.012022655457258224\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.009579799138009548\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0406770259141922\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.013188964687287807\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04130122810602188\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.020594116300344467\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025395553559064865\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03573458269238472\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.016762716695666313\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.08794804662466049\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02452467568218708\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.048945408314466476\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.056249212473630905\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04932691901922226\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.010434596799314022\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.05030389502644539\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03091493807733059\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023585842922329903\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02542904019355774\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0736810564994812\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.054505135864019394\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03330603986978531\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.031472135335206985\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03257950395345688\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04411151260137558\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.060605455189943314\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04873880371451378\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04303961619734764\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.013036319985985756\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.011616568081080914\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02979143336415291\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05319175496697426\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08823107928037643\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.02914159931242466\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.042521726340055466\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05207190662622452\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023818690329790115\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.034916166216135025\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.017078882083296776\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.035135965794324875\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03737659007310867\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04978172481060028\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.016670946031808853\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.027643373236060143\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.033106591552495956\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.029182549566030502\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.054834794253110886\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0431814081966877\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.050736457109451294\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.029910532757639885\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024717625230550766\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01806691847741604\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06194461137056351\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.017817143350839615\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.048180822283029556\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04634927213191986\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04978536069393158\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.029433654621243477\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.044206470251083374\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.050596170127391815\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.028234805911779404\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02289741300046444\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023069528862833977\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03836269676685333\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05455886945128441\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.013927613385021687\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04515732079744339\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03663640096783638\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04727781191468239\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.046156905591487885\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.10871879011392593\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.03094641864299774\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03860338404774666\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02663213014602661\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06458970904350281\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07840820401906967\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04644501209259033\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04624457284808159\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.037660710513591766\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03401503711938858\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.042614638805389404\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04325195774435997\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02823532372713089\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06668800115585327\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.03483697399497032\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.036508958786726\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02798597700893879\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03216898813843727\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014112643897533417\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.028398452326655388\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04745086282491684\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024913495406508446\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06591641902923584\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.041813310235738754\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.037186697125434875\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06539703160524368\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.01966766268014908\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08852926641702652\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03549693524837494\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05113660916686058\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.012874731793999672\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012318842113018036\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03071356751024723\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0475972555577755\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03840523213148117\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05317509546875954\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04516458883881569\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07347045093774796\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.03752870485186577\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.10276273638010025\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04369672015309334\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.020729396492242813\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04644650220870972\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07863733917474747\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.059843771159648895\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0642276182770729\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.07234156131744385\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06251762062311172\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.01739400625228882\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.040752656757831573\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07654298841953278\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04695778712630272\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02327921986579895\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.035952772945165634\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05474453046917915\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.024813299998641014\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027846794575452805\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.09861724823713303\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.020522134378552437\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05152894929051399\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05314866453409195\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06309627741575241\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.059353385120630264\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.040334444493055344\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.038365498185157776\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.059914134442806244\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04444797337055206\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08338453620672226\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04929783567786217\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06393112987279892\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06068887934088707\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.023074563592672348\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.040169164538383484\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04289637506008148\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05157623067498207\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04222811385989189\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07739055901765823\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04235933721065521\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07171182334423065\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06867022067308426\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02498764917254448\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05000091344118118\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.043996475636959076\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.048263486474752426\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.048844147473573685\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06148499622941017\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04286438971757889\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08028948307037354\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.037871334701776505\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03534333407878876\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.038013655692338943\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05499674752354622\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03985792025923729\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03767798840999603\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0778733566403389\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.09449713677167892\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.039724573493003845\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0570577010512352\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.09632766991853714\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.08350646495819092\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.05077369883656502\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03862489014863968\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0286666601896286\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.043855372816324234\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05535343289375305\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03588853031396866\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06723353266716003\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.037283267825841904\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02793983183801174\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030393201857805252\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.10316473245620728\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.0637751966714859\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.034070439636707306\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08754230290651321\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.10333585739135742\n",
      "Training accuracy 0.97265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 22/36 [1:26:56<54:58, 235.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.6991, acc: 0.2740\n",
      "eval loss 5.699117271229625\n",
      "Training loss: 0.02980829030275345\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02230878360569477\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022721828892827034\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03150054067373276\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.010546499863266945\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.024308282881975174\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024838918820023537\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.031624890863895416\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023835215717554092\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.016271168366074562\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0361090712249279\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.024990616366267204\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04711969569325447\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030639899894595146\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023516282439231873\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015886597335338593\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027985965833067894\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021820688620209694\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04603075236082077\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.050225354731082916\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030120577663183212\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03320079296827316\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026385575532913208\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025349227711558342\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03548446297645569\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.026536665856838226\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01758442632853985\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.08221428841352463\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05889786407351494\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0228086169809103\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024142904207110405\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02741330862045288\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016191206872463226\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.019560808315873146\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.033448342233896255\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.014123868197202682\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.014965426176786423\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02045094221830368\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01725916936993599\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014766508713364601\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.016451189294457436\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.035246964544057846\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05689600110054016\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.013482232578098774\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.022085197269916534\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030956106260418892\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02272818423807621\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020623622462153435\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04375925287604332\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02918040193617344\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028057023882865906\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.006458078511059284\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02235177718102932\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02188684046268463\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01791251450777054\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0293046273291111\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06380216032266617\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02508336305618286\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02819053642451763\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.043316520750522614\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.019877411425113678\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04524414241313934\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013212384656071663\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.037139128893613815\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05114991217851639\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.015933547168970108\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.012621237896382809\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017240481451153755\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.034447766840457916\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.013832036405801773\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.012887098826467991\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019822705537080765\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021770238876342773\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.020528772845864296\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05123698711395264\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.026019668206572533\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01544708851724863\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02970530278980732\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06851449608802795\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.029020994901657104\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022133003920316696\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.058945268392562866\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.011503156274557114\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.029648806899785995\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03509262949228287\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02661992236971855\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04613253101706505\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03864399716258049\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.013580715283751488\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.028859594836831093\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06900843977928162\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.05667698010802269\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0507902167737484\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.09805741161108017\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06159790977835655\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01852167397737503\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02573315240442753\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.012862199917435646\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05417689308524132\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04581936448812485\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0680915117263794\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.046162448823451996\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011264076456427574\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03711704909801483\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0323994904756546\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04072282835841179\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028980135917663574\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04613666236400604\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03664149343967438\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05057023838162422\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03549840673804283\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.035824764519929886\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.058968525379896164\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.020497359335422516\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03875343129038811\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013147956691682339\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04190116748213768\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03736173361539841\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04075504466891289\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0649791955947876\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06348557025194168\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.030407382175326347\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06701701134443283\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.035291802138090134\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.039470452815294266\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.029456324875354767\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0622849203646183\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.026421623304486275\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03925413265824318\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06421682983636856\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02571089006960392\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.045938171446323395\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07647207379341125\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03091290406882763\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030771778896450996\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03262120857834816\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0330180786550045\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04858758673071861\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02946101687848568\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.051853008568286896\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04725312814116478\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03301248326897621\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019522231072187424\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03979675844311714\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.056583471596241\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.015448600053787231\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06885810941457748\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03530723601579666\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05104747414588928\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04599681869149208\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04606113210320473\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04172970727086067\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06325341016054153\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.010937566868960857\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.023854274302721024\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03146868944168091\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019149761646986008\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.024709250777959824\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05433296412229538\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030988110229372978\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05702492222189903\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03882061690092087\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01853848807513714\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026258831843733788\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016358254477381706\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03751572221517563\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02507654018700123\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02223939821124077\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05066364258527756\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07917001098394394\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05681214854121208\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05326543748378754\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03660866245627403\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04612640663981438\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03353114798665047\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.010962177999317646\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.05061356723308563\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.031906597316265106\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.034095991402864456\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030014386400580406\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.14717014133930206\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.07139305025339127\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08559981733560562\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04812741279602051\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03340788558125496\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.025436531752347946\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030805394053459167\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015262026339769363\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03306904435157776\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07240770757198334\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0528799369931221\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03587321192026138\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06478321552276611\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07417726516723633\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02882976457476616\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03391055762767792\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04628338664770126\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04597065597772598\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.060855403542518616\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04030487686395645\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04624098539352417\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03129974380135536\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0192694254219532\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0521458275616169\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03529032692313194\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.041083578020334244\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03356773033738136\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016469648107886314\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06594529002904892\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06835591048002243\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05593491345643997\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04648655280470848\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04973800107836723\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03713744878768921\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06398945301771164\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02518630027770996\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05313468351960182\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.05543623864650726\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.016829857602715492\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.016548799350857735\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.049936145544052124\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05908425152301788\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.08832406252622604\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.053371354937553406\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0361342579126358\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.036430031061172485\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02856365591287613\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06702636927366257\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.04772893711924553\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05699486657977104\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0541873574256897\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.050984807312488556\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.08500232547521591\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.039751697331666946\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06468259543180466\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08318884670734406\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06602785736322403\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.051302507519721985\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.034174513071775436\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.039893995970487595\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.10558463633060455\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03055320307612419\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04815356433391571\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.050667475908994675\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.061330899596214294\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.036158908158540726\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02923702448606491\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07313965260982513\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.10507431626319885\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.05365649610757828\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.08282855153083801\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04470941424369812\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08438391238451004\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03866203501820564\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06787670403718948\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07073923945426941\n",
      "Training accuracy 0.98046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 23/36 [1:30:51<51:00, 235.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.4140, acc: 0.3005\n",
      "eval loss 5.413961015641689\n",
      "Training loss: 0.04079505428671837\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015556751750409603\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0418504998087883\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021786702796816826\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02221747487783432\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02124650590121746\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026567360386252403\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01455716136842966\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017386453226208687\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.037728048861026764\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.023699786514043808\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0193517804145813\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.011289509013295174\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.039345644414424896\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.016894690692424774\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0152259087190032\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01006023958325386\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.015937402844429016\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04168299213051796\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02527647651731968\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02121579647064209\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016990214586257935\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07556794583797455\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05913921445608139\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016684886068105698\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04313608631491661\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018795780837535858\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.011637543328106403\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.020607396960258484\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.040742792189121246\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03902691975235939\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03302041068673134\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.051859404891729355\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.00940224714577198\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.031260885298252106\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013740438036620617\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05036786571145058\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.025138817727565765\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.038466110825538635\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.016123885288834572\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.05825987085700035\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.026400115340948105\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.034115299582481384\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03344358876347542\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.050827424973249435\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02967190369963646\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.033181414008140564\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.00817316398024559\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.033771250396966934\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.028235841542482376\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03958016261458397\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.031328633427619934\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015168066136538982\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013395448215305805\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.019850870594382286\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03427651524543762\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023882107809185982\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02702588215470314\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02692541666328907\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021944371983408928\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02104385755956173\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03466225787997246\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.027139395475387573\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04855278506875038\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013425448909401894\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01454204972833395\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0073626297526061535\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.030913813039660454\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.017347117885947227\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02281663566827774\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025779560208320618\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03260824456810951\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03410962224006653\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019928231835365295\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04717721417546272\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.023166168481111526\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06005874276161194\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01798117160797119\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05363774299621582\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.045999087393283844\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.058058761060237885\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01161087118089199\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.052198559045791626\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02291683480143547\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024444472044706345\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05755086988210678\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.034083858132362366\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03122084029018879\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018369248136878014\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.014087808318436146\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03861545771360397\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03209234029054642\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03383731096982956\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.025621099397540092\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03117753379046917\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028654128313064575\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01619640551507473\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05235648527741432\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02925753779709339\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01582881435751915\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.047244928777217865\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.010523088276386261\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.027799194678664207\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03469453379511833\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.023354027420282364\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03178498521447182\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015725230798125267\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.052836570888757706\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.021452851593494415\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015012708492577076\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05776659771800041\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.02095060795545578\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02561996690928936\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.056346792727708817\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.022991709411144257\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04225483909249306\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05806040018796921\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.1008492037653923\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.021311987191438675\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.020758764818310738\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03026628866791725\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05002313107252121\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06738181412220001\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03368135169148445\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03402484208345413\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025279134511947632\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025181416422128677\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02176198922097683\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015355588868260384\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05188652127981186\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02118430845439434\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07187017798423767\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03726450353860855\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.020219316706061363\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04149400815367699\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.037866417318582535\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02443932741880417\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05472826212644577\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.027597377076745033\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0626259446144104\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07132938504219055\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.056949302554130554\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06274739652872086\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.00808307621628046\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03055424429476261\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.035578176379203796\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0280214324593544\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.022803617641329765\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.024308854714035988\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06833184510469437\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01793498918414116\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.058846112340688705\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.034901365637779236\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018772847950458527\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03282645717263222\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03029470704495907\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.11110616475343704\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04250093176960945\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.058517519384622574\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028925882652401924\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04010797291994095\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04283600673079491\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02944401279091835\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03716156631708145\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021402526646852493\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03960736468434334\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.053214170038700104\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05118204653263092\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.061694324016571045\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05252469703555107\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.061301782727241516\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07923515886068344\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.027938932180404663\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07943062484264374\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0599469393491745\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0786169096827507\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.03369111567735672\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.029100852087140083\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04233771190047264\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0259088147431612\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06136580929160118\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0947847068309784\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03333287313580513\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021685969084501266\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04133659228682518\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02961568720638752\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.031233228743076324\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.028272008523344994\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.029330627992749214\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04786079376935959\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07824806123971939\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03782469779253006\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01482744887471199\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.06445744633674622\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.05237782001495361\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04775575175881386\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06280863285064697\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04172508791089058\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02924283593893051\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07148803025484085\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04145894572138786\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07424219697713852\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.015181330032646656\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02963877096772194\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0587482713162899\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03592095524072647\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07842755317687988\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.03687828406691551\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04325246065855026\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07440109550952911\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04639508202672005\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0323789082467556\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.027201876044273376\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028316792100667953\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.037820495665073395\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07360649108886719\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04051586613059044\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.09254807978868484\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.02933366969227791\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02919011190533638\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.047517675906419754\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07593663781881332\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.09948639571666718\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.06218532845377922\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03301054239273071\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02315707318484783\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05016833543777466\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022068781778216362\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04319599270820618\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05243559926748276\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.025643017143011093\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030726773664355278\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020418589934706688\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04161635786294937\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04028018191456795\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05273997038602829\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03526419401168823\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025384996086359024\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.12801368534564972\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.023003993555903435\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.09975921362638474\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.03589564189314842\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08271802216768265\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.060068294405937195\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.05878041684627533\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.047436341643333435\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.025546036660671234\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03803204372525215\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.10386382043361664\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05639800801873207\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04491198807954788\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04146400839090347\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.015046793967485428\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02596820890903473\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.049009520560503006\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04276244714856148\n",
      "Training accuracy 0.984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 24/36 [1:34:47<47:07, 235.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.4017, acc: 0.2986\n",
      "eval loss 5.401690647006035\n",
      "Training loss: 0.020684614777565002\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03786027058959007\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03295908495783806\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.010362775065004826\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00854839850217104\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02777773141860962\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017562368884682655\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014387108385562897\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03266872465610504\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04685920476913452\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03620506823062897\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05667945370078087\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03591816499829292\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01663566380739212\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026807472109794617\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011184737086296082\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.05012764036655426\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0271524116396904\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024317331612110138\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009244046173989773\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.023672398179769516\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030662748962640762\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.008531772531569004\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.025813104584813118\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03586064651608467\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.01624777913093567\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01671754941344261\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015385083854198456\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04618551954627037\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.027871549129486084\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.040361396968364716\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.029881957918405533\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03388363495469093\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011261466890573502\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02823278121650219\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06458580493927002\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04895952343940735\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.009784465655684471\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0422641783952713\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.009097427129745483\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03700143098831177\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02998620830476284\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.011942232958972454\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03273773938417435\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022688550874590874\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.054357487708330154\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.014216107316315174\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018815036863088608\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.027795564383268356\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0165916346013546\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07533863186836243\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.016871128231287003\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02704990841448307\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02434571273624897\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020597930997610092\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0398818776011467\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022623177617788315\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.044394150376319885\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02311660349369049\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.010532301850616932\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03566121309995651\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021627405658364296\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02360624074935913\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.031618133187294006\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028391646221280098\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013466175645589828\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0294501893222332\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01868823915719986\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02647983282804489\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.014638989232480526\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03294721618294716\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025067724287509918\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05139424651861191\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05282063037157059\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022704768925905228\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04473377391695976\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.010464441031217575\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.022520385682582855\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025707198306918144\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.036377452313899994\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03410773351788521\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.050639960914850235\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03941236808896065\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.048208337277173996\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.015714550390839577\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.018150217831134796\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.018099486827850342\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028250450268387794\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02060067653656006\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01968100480735302\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0344904288649559\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018844561651349068\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024454737082123756\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02767382562160492\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.019363081082701683\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01464520301669836\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03127421438694\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.031310733407735825\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.012766598723828793\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02879520319402218\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014009905979037285\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02791273221373558\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015396153554320335\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.010624052956700325\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02080046758055687\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03258058428764343\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016677692532539368\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013323233462870121\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.050190120935440063\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0771498829126358\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03962087631225586\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04139968752861023\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.029845109209418297\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018733179196715355\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.035347405821084976\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020271986722946167\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06326329708099365\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.07791100442409515\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.027812687680125237\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.058755774050951004\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.04427874833345413\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018799304962158203\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04222029820084572\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03191937133669853\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06753653287887573\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.016986573114991188\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03099340945482254\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.031524959951639175\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022936657071113586\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04567939043045044\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017787236720323563\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.06447897851467133\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05159972235560417\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.09751948714256287\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.06196336820721626\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03880087658762932\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.038122981786727905\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.042013019323349\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08112524449825287\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.003169334027916193\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03498152270913124\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04203425720334053\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.040106143802404404\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.040959496051073074\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.012616422027349472\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.018719637766480446\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.019363638013601303\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.032971639186143875\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05783998593688011\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.01912606507539749\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03940828889608383\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02872268669307232\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.027234893292188644\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0076003773137927055\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02646314539015293\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0290815606713295\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009057034738361835\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.022993220016360283\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03541688621044159\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.057574234902858734\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04127463325858116\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03889547288417816\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02731264755129814\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04544450715184212\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.033702507615089417\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025344448164105415\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02026958391070366\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04812273383140564\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04979056119918823\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.029097672551870346\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05202162265777588\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.026038190349936485\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.048873789608478546\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03140389174222946\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06049797683954239\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04955657571554184\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.058092325925827026\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.047168225049972534\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04842045530676842\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02694699726998806\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.029318759217858315\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08263760805130005\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.026840852573513985\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03697246313095093\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0245563555508852\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024576280266046524\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02830110862851143\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03615822270512581\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06556209921836853\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04517485201358795\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07428161799907684\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.07544657588005066\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05937321484088898\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.046557556837797165\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.048238497227430344\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03553817793726921\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029474277049303055\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07509984076023102\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04358888417482376\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04410336911678314\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07844702154397964\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0415031872689724\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.048188600689172745\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.012488916516304016\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06219274923205376\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.025041503831744194\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03520795330405235\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07563699781894684\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.027838533744215965\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0551794059574604\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0366392657160759\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02622043900191784\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.032695796340703964\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06765234470367432\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01934678480029106\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.048182547092437744\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.046216629445552826\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0449686162173748\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06611990183591843\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.09334857016801834\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04047840088605881\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.037606317549943924\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06000085920095444\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.020865507423877716\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07126928865909576\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04421565681695938\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06446299701929092\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.042899101972579956\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04556722193956375\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06620924174785614\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.08505937457084656\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.10249719023704529\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.03490198031067848\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.08601231873035431\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04767587408423424\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05819452926516533\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.02657448872923851\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.056067753583192825\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.031104834750294685\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.08268695324659348\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.04559546336531639\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.043326299637556076\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06496114283800125\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0448370985686779\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.09971904754638672\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.028140325099229813\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.050506629049777985\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.041564688086509705\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.058093711733818054\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.11789440363645554\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.06265725195407867\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04721581190824509\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03629285842180252\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.048874881118535995\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04188237711787224\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04832315817475319\n",
      "Training accuracy 0.984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 25/36 [1:38:44<43:15, 235.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.0380, acc: 0.3285\n",
      "eval loss 5.038015836849809\n",
      "Training loss: 0.00634233932942152\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02544492669403553\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02560158260166645\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012129118666052818\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021865101531147957\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04044255241751671\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03006364032626152\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03840456157922745\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011718581430613995\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.027409832924604416\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015181169845163822\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.016281193122267723\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07443647086620331\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.018924973905086517\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03172782063484192\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03404767066240311\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0322980135679245\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.023397674784064293\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020029718056321144\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.038321759551763535\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.012588058598339558\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018840119242668152\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.024734344333410263\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019567886367440224\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05614076554775238\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02537723071873188\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02295585162937641\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03135174140334129\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02110307849943638\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02150707133114338\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024236947298049927\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011231611482799053\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018219197168946266\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022065363824367523\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01736578904092312\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02261834405362606\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026498418301343918\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.006015332881361246\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.032391507178545\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.012625802308321\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010923324152827263\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02515864185988903\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.020342400297522545\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02489849552512169\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01615443453192711\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05004962906241417\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.038055844604969025\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.010711346752941608\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.038487352430820465\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02716240845620632\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011863885447382927\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012819514609873295\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015060489997267723\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008272224105894566\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.014952998608350754\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.054810140281915665\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03157692030072212\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028188006952404976\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03009212389588356\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04732261225581169\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02541159838438034\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014099049381911755\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.028815006837248802\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01897354982793331\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027807902544736862\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.010886650532484055\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.014941859990358353\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014321678318083286\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.023571543395519257\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02794136479496956\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.038382578641176224\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016826357692480087\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06462760269641876\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.03232252970337868\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0479605570435524\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06119630113244057\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.016819728538393974\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015099846757948399\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0784892812371254\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.030044768005609512\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02340194769203663\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01141685526818037\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.043963588774204254\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07424237579107285\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.009698682464659214\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.07124313712120056\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.02114201709628105\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028064323589205742\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02923295833170414\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04289369285106659\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04994094744324684\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.020689086988568306\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.045727428048849106\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.033698633313179016\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.040687479078769684\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03345654532313347\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04377533867955208\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04896118491888046\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04018990695476532\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.054106567054986954\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.025843273848295212\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02005140669643879\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.027850136160850525\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023040371015667915\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.033241260796785355\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05606494098901749\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03464382141828537\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02538992650806904\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.031121565029025078\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.031244544312357903\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02718498557806015\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02036835066974163\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0342833437025547\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03894513472914696\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.014990510419011116\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03200886771082878\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.053380802273750305\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.030106274411082268\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009616286493837833\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04751286283135414\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0483345165848732\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.035385023802518845\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01680506207048893\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.006390352733433247\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04067408666014671\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0442243367433548\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03926394134759903\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.008974053896963596\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.033216994255781174\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.032938502728939056\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.032944321632385254\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01905795745551586\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04078017547726631\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.030786866322159767\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.015849154442548752\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03821004927158356\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02230672352015972\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019088655710220337\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.049703050404787064\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.012405225075781345\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014870963990688324\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04753531143069267\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.018872259184718132\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06580745428800583\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.014864815399050713\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022065620869398117\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03546692803502083\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03474976122379303\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0608944334089756\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03161521628499031\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.050037700682878494\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03530006855726242\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02618659846484661\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08608728647232056\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03761446103453636\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.021455727517604828\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.052558448165655136\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07270397990942001\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02068893238902092\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07878626883029938\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04588305577635765\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02534736506640911\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05674084275960922\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.018449800089001656\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024837827309966087\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008400693535804749\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.06749364733695984\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.035612739622592926\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0799097791314125\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.030189333483576775\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08503960818052292\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.044465322047472\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02939348854124546\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06560443341732025\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04269048571586609\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05324765294790268\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01201527751982212\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0669267550110817\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0291597843170166\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.10400445014238358\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04843934625387192\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01674671098589897\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018046356737613678\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03283818066120148\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015491022728383541\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.10108033567667007\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.07286933064460754\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05446084216237068\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.044912248849868774\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04289337247610092\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.034722086042165756\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019129715859889984\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022821683436632156\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.014511911198496819\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0627683475613594\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.023791266605257988\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0441330224275589\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08517713844776154\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.03336668387055397\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015241850167512894\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.056284867227077484\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01681631989777088\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0933523029088974\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0431220680475235\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.017062684521079063\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05308692902326584\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0454670786857605\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.060155682265758514\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05327686667442322\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03690699115395546\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.029329126700758934\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06429994106292725\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.037494707852602005\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.046003665775060654\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04328290745615959\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04237016662955284\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03250066563487053\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.014099299907684326\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.012761775404214859\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04734652489423752\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03202047944068909\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02521614171564579\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06334660202264786\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.029879841953516006\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.047411657869815826\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06313628703355789\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.041312139481306076\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08222068101167679\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02184624783694744\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.041863471269607544\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06399627029895782\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.10114948451519012\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02988004870712757\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.051369357854127884\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06992262601852417\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05662411451339722\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05696822702884674\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0300288163125515\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.054549768567085266\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06177577003836632\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07149075716733932\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.02043561078608036\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05808829516172409\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.08213843405246735\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.026633651927113533\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04602675884962082\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.030307793989777565\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03794996440410614\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.059917546808719635\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06482145935297012\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06770225614309311\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03584945946931839\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05345917493104935\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.038464806973934174\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05685519054532051\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.08460278064012527\n",
      "Training accuracy 0.9765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 26/36 [1:42:40<39:19, 235.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.3679, acc: 0.3017\n",
      "eval loss 5.36788828112185\n",
      "Training loss: 0.03863689303398132\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.014108008705079556\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022503778338432312\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.009668318554759026\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.010810033418238163\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01476572547107935\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.032243743538856506\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.024372901767492294\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.005945343058556318\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.030338380485773087\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01266296487301588\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.012282962910830975\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03676043078303337\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04436779022216797\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05754612758755684\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.05843714252114296\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.035842955112457275\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02598029002547264\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04520653560757637\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.015878114849328995\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.010200119577348232\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01651512272655964\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022124584764242172\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02776045724749565\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02227208949625492\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07239202409982681\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.040958601981401443\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.040002692490816116\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025690702721476555\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03378279507160187\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05336460471153259\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.008656148798763752\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.030892740935087204\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.040124062448740005\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.017333438619971275\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017087753862142563\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03453807532787323\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025921473279595375\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03660866618156433\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024676989763975143\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.014677448198199272\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019555589184165\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018044153228402138\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017889533191919327\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023673884570598602\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026944465935230255\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013837870210409164\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0386747345328331\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01860981062054634\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.006594406440854073\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.030115526169538498\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030376393347978592\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014258628711104393\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02849782072007656\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017437521368265152\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01265624538064003\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02197188511490822\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.011951223947107792\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02589549869298935\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.032755956053733826\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.025705326348543167\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03580160439014435\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01351025141775608\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.026886040344834328\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020074745640158653\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02143382467329502\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07248981297016144\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.028540773317217827\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.026214634999632835\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03666917607188225\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.005953190848231316\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009810122661292553\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.051967229694128036\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.014264940284192562\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.004742165096104145\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.027307936921715736\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03167417645454407\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02395164780318737\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.014773372560739517\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.023827292025089264\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019130729138851166\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03287940472364426\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.009030573070049286\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03235631436109543\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01376317162066698\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03885917738080025\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03422779589891434\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03419376164674759\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.027796870097517967\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.052214544266462326\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06335632503032684\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026064878329634666\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06389140337705612\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.022065240889787674\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011754967272281647\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.021442251279950142\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01100740022957325\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.053297821432352066\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05958801507949829\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04962192103266716\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03560347855091095\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02266758307814598\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03015950880944729\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01943245157599449\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.041916001588106155\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.024170640856027603\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01822732575237751\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.034309834241867065\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.031515974551439285\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024109166115522385\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06137200444936752\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.031492460519075394\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.015722032636404037\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.010325539857149124\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03635536506772041\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06584413349628448\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.029501082375645638\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022442735731601715\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0183420330286026\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03898800536990166\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01203510258346796\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0244145430624485\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021034754812717438\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013338756747543812\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.032709669321775436\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.039278555661439896\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0463172011077404\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02215007320046425\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018138302490115166\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.033175691962242126\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.055035948753356934\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.023571083322167397\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0352654866874218\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01952839083969593\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02531260997056961\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03668002039194107\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.056308772414922714\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04225126653909683\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.024926306679844856\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.020581625401973724\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030290715396404266\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.027585802599787712\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0391882061958313\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03847471997141838\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04409608989953995\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04809548333287239\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02968694269657135\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0770273432135582\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0364861786365509\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03449513390660286\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.051004037261009216\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.031552381813526154\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.010466506704688072\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015456889756023884\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0317625068128109\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025514859706163406\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04895544797182083\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05217355117201805\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011220482178032398\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03265325725078583\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018167275935411453\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02455967664718628\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025430938228964806\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07407107949256897\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0743931457400322\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.02905815839767456\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.032040368765592575\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.042341627180576324\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.024624815210700035\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07463174313306808\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.04195813834667206\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05007629841566086\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02060813270509243\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.058970268815755844\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.013607097789645195\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.057899922132492065\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04259301722049713\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04018772393465042\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07342806458473206\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03389744833111763\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028585003688931465\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025469327345490456\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04175383970141411\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.021102899685502052\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04657478258013725\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05005090683698654\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0672149732708931\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06400937587022781\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03730058670043945\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02052040956914425\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02915557287633419\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.052724581211805344\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05526822432875633\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.048492658883333206\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.054128531366586685\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04953339323401451\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04455599561333656\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03968182951211929\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04065586254000664\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05933232977986336\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06004415825009346\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04547267407178879\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04163579270243645\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.023906268179416656\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.056607332080602646\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02441396936774254\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03314375504851341\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06908177584409714\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03620229661464691\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0485072061419487\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.052226658910512924\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.023181963711977005\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07932492345571518\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.054172396659851074\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.039685770869255066\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07113944739103317\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.056713856756687164\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.047583434730768204\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06552670896053314\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.016832683235406876\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.06640893965959549\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06455401331186295\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06621476262807846\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0563078336417675\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025213919579982758\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04712611436843872\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04115470498800278\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.036825377494096756\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.039892446249723434\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.020804261788725853\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024039894342422485\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.024377496913075447\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08310205489397049\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.07779345661401749\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.07366259396076202\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0884968712925911\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0625591054558754\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08080105483531952\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.02480357699096203\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.048461560159921646\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0461382120847702\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04635633900761604\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0479382760822773\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03290393203496933\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.059020936489105225\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02236439287662506\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01935802772641182\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04642418399453163\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.046902239322662354\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06595192104578018\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0661110132932663\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.05446653068065643\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07668855786323547\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0907837301492691\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.038070932030677795\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06578568369150162\n",
      "Training accuracy 0.96875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 27/36 [1:46:35<35:22, 235.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.3938, acc: 0.2802\n",
      "eval loss 6.393768683075905\n",
      "Training loss: 0.030650287866592407\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.010864547453820705\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02959306724369526\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.035550788044929504\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029986845329403877\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04546937718987465\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.011178486049175262\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007064203266054392\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.06324592232704163\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.018277233466506004\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.007005916442722082\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0349719412624836\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.011011180467903614\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0388394333422184\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.008501882664859295\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.024547751992940903\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02045741304755211\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01171437744051218\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.006986790336668491\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011862261220812798\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03423146530985832\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01833423040807247\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.037580735981464386\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022891879081726074\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.035126782953739166\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04438558593392372\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.010967022739350796\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02492409199476242\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.035403769463300705\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.049348097294569016\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.018947584554553032\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.00884790625423193\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04033344238996506\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.012333138845860958\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0373276062309742\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.014923135749995708\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04013616219162941\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03065299615263939\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019954290241003036\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03556341305375099\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.020025748759508133\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.046803105622529984\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03095332905650139\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023157399147748947\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03459823131561279\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012680873274803162\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015565192326903343\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02937333844602108\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.019545814022421837\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.032202765345573425\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.008908262476325035\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03487761318683624\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.011413787491619587\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01483419444411993\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.009733079932630062\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01138470508158207\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03262351453304291\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012693251483142376\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027443228289484978\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08832453191280365\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.05475243553519249\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.023900676518678665\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.1002047210931778\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.019103536382317543\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018224861472845078\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.022814583033323288\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.010137662291526794\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03529774397611618\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02709805965423584\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01230737566947937\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04121579974889755\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03180016204714775\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.005763479508459568\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06821955740451813\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.018524711951613426\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.032291244715452194\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03875630348920822\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03664732351899147\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06191035360097885\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.02128748781979084\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021220935508608818\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02925177849829197\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03089825250208378\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01995909959077835\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05076092109084129\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02317841164767742\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013823551125824451\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029951639473438263\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01659497804939747\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021989645436406136\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03190857544541359\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04885338991880417\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.008098266087472439\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03281548246741295\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.011352228000760078\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00842701829969883\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02131350338459015\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.004840424750000238\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03563622012734413\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03849459066987038\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.024369919672608376\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03427063673734665\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016476111486554146\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05344695225358009\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.027104318141937256\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.061130087822675705\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04279227554798126\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03665819764137268\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.037872184067964554\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025744738057255745\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0368911474943161\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015959210693836212\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.010944966226816177\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02820611372590065\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04752448946237564\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.023781700059771538\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03729764744639397\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03739895299077034\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01983814314007759\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.009397415444254875\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03619524464011192\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.031764693558216095\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030174802988767624\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04439164698123932\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03878132253885269\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02141403593122959\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.018489563837647438\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.036172814667224884\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013260875828564167\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03045196272432804\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.039739176630973816\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.044163696467876434\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02453167364001274\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.018065104261040688\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025325175374746323\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03746606782078743\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0309480968862772\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.041949670761823654\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.054890651255846024\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04100502282381058\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.029506418853998184\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026399511843919754\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.027657730504870415\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.010530375875532627\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.041051413863897324\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04013020172715187\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03744790703058243\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016394874081015587\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04811802878975868\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.046186696738004684\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04712742194533348\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.09285131096839905\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.014533955603837967\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030223753303289413\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05952059105038643\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.018918834626674652\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02221636101603508\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06346321851015091\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05940990522503853\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03883461654186249\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03559136390686035\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.042691659182310104\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030239718034863472\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0396096333861351\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.035522084683179855\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03729686886072159\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.022417355328798294\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04762139171361923\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.022720836102962494\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06479941308498383\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06769656389951706\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.026069536805152893\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04461969807744026\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04014020785689354\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04010548070073128\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08302851766347885\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.016216279938817024\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.041787054389715195\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.011433680541813374\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03137745335698128\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.051527176052331924\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.044618356972932816\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.016983358189463615\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06485991179943085\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03603433817625046\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08999522030353546\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04958022013306618\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05873151496052742\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04524965584278107\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06783726811408997\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.041470300406217575\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.028004271909594536\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030172914266586304\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028857771307229996\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03958742693066597\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06009295955300331\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.024948935955762863\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05826760455965996\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02085983380675316\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02313133142888546\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05527101084589958\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.030098920688033104\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04350560903549194\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022405076771974564\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.047130387276411057\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.052316635847091675\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023274125531315804\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.034350939095020294\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04755949601531029\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02354155108332634\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06383669376373291\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.057681187987327576\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.10456988960504532\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.029530834406614304\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03396190330386162\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0557168610394001\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04570283368229866\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07489489018917084\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03984629362821579\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03820553049445152\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.039140380918979645\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03623783588409424\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02124905399978161\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05164475739002228\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.036899033933877945\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08632367104291916\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.08736056834459305\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.029006309807300568\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07385130226612091\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06988253444433212\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.017848746851086617\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021137341856956482\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.08768908679485321\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02792126126587391\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.039804607629776\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05981046333909035\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0321185365319252\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07667043060064316\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07186643034219742\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0724906250834465\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02202972024679184\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05779476463794708\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05982068553566933\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05009036511182785\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02833586186170578\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.036265626549720764\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07141322642564774\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07431590557098389\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.07848682999610901\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.09907518327236176\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02229171432554722\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.045120060443878174\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.042188677936792374\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.09404486417770386\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0395580418407917\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.053583260625600815\n",
      "Training accuracy 0.9765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 28/36 [1:50:31<31:27, 235.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.3864, acc: 0.2672\n",
      "eval loss 6.3863685466349125\n",
      "Training loss: 0.030650893226265907\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.052845548838377\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02660747431218624\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029880061745643616\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025348830968141556\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02293199487030506\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.018223518505692482\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.016607550904154778\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.043032459914684296\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03930090740323067\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013221928849816322\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017658967524766922\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.08581938594579697\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02264806628227234\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.009659787639975548\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.010890686884522438\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008244329132139683\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0211284551769495\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.058749258518218994\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01451793685555458\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.014241891913115978\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026689128950238228\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01170679833739996\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017955344170331955\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.012593043968081474\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027379881590604782\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04804302379488945\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.013246962800621986\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018268166109919548\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.034714747220277786\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015549665316939354\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008285792544484138\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.030352719128131866\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.014875691384077072\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03763144090771675\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0421442985534668\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.012379668653011322\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.025891151279211044\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015847230330109596\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022667087614536285\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03804062679409981\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028594795614480972\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026597613468766212\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020776184275746346\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05575460195541382\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05890466645359993\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.009813034906983376\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.047528792172670364\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.017701558768749237\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0443727932870388\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0048926109448075294\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.047896452248096466\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04230779409408569\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.012252544052898884\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.024736933410167694\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0413566455245018\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.015111822634935379\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0648607537150383\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.006642900872975588\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.028067803010344505\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.029985368251800537\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.008809982798993587\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03221245855093002\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011034421622753143\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.035560134798288345\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04760070890188217\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.056632254272699356\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02348601073026657\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01910175010561943\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07599326223134995\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07116532325744629\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08009820431470871\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.017219815403223038\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030108587816357613\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.010198934003710747\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04496617615222931\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07174034416675568\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0176551453769207\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026460865512490273\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015256281942129135\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.042157094925642014\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026903273537755013\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.042376354336738586\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01319015584886074\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.034380000084638596\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03310003876686096\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04148787260055542\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02780136466026306\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030362507328391075\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.014196589589118958\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.043831560760736465\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01925782300531864\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02532031387090683\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011221497319638729\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03785957023501396\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0076403929851949215\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.037682950496673584\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02049417421221733\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020798254758119583\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018314091488718987\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02810145542025566\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03798956051468849\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03309525549411774\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03957047313451767\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03390621021389961\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.041039399802684784\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03919987007975578\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03009791113436222\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.031484078615903854\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.024010716006159782\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.032552480697631836\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01032150536775589\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04011467844247818\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0175671074539423\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.010223697870969772\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.0320817194879055\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.008277482353150845\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.021951919421553612\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02484244666993618\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024455588310956955\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05779775604605675\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.050164416432380676\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03005746379494667\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05217500403523445\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011372613720595837\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.016710612922906876\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03424820676445961\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03550821170210838\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.015026191249489784\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.035786695778369904\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04421389102935791\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03158653527498245\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01204616017639637\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.07286227494478226\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.016097400337457657\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04108472913503647\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05874884873628616\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.018132809549570084\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.012135226279497147\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.045708999037742615\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.041128721088171005\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06697994470596313\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03857766464352608\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02907903864979744\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.039374999701976776\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.044865988194942474\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03691289201378822\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.027650466188788414\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.036550480872392654\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.021704740822315216\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026373419910669327\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.005999751854687929\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.043355342000722885\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04394124448299408\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03393137454986572\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03450721874833107\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04385966807603836\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.029820049181580544\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.010260935872793198\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04730730131268501\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013518817722797394\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.027675580233335495\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.061439309269189835\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.06428571045398712\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0421968549489975\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04641491174697876\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03698690980672836\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05511918291449547\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.033317092806100845\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.040468744933605194\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03954460099339485\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03817002475261688\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06520825624465942\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.055200882256031036\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025359107181429863\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0609067901968956\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05104589834809303\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.058740418404340744\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022914132103323936\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04406442865729332\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03483026102185249\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03664190322160721\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06754441559314728\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06273820996284485\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07557603716850281\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.04019124433398247\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03805605694651604\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03152332454919815\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07009278982877731\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05427322909235954\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04826818034052849\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05247505381703377\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08035273849964142\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.060093075037002563\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03733805939555168\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03543970733880997\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03279832750558853\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0351591520011425\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.034494370222091675\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.029005175456404686\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03591035306453705\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.044112429022789\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03238249197602272\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05865681543946266\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.039593398571014404\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03570320084691048\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05357050523161888\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06231104955077171\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.08205409348011017\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04555407539010048\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03867239132523537\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.046059317886829376\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02956583723425865\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08813571184873581\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.034084614366292953\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08286826312541962\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0363924540579319\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.051906391978263855\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.030375737696886063\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03105149045586586\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04447860270738602\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.033857207745313644\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08497083187103271\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04041299968957901\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04384986683726311\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.035973113030195236\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07752641290426254\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05818449705839157\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.025237711146473885\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.055261194705963135\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02810591459274292\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.047277722507715225\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07209765166044235\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04387084022164345\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06276842206716537\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0452287532389164\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03482262045145035\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05127405375242233\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03113948553800583\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05751287564635277\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.041163939982652664\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07796770334243774\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.02871265448629856\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03025887906551361\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.028300626203417778\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023146269842982292\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.043330393731594086\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01661807857453823\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04350049048662186\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08731622993946075\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.04074258357286453\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0466400682926178\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.051467083394527435\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04348411038517952\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05127432569861412\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04808783903717995\n",
      "Training accuracy 0.984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 29/36 [1:54:25<27:26, 235.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.1139, acc: 0.2959\n",
      "eval loss 6.11386763677001\n",
      "Training loss: 0.029866041615605354\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.00607330072671175\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01281109731644392\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06960392743349075\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.015723221004009247\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030182339251041412\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009359379298985004\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04168303310871124\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.015122488141059875\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.011424127034842968\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.00935825053602457\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03719567134976387\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.009433050639927387\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011666996404528618\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.012963451445102692\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.036545682698488235\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.025027422234416008\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01948864385485649\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013783145695924759\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013794722966849804\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01132972165942192\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.032291170209646225\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.041462402790784836\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02271806262433529\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01929456926882267\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.028081784024834633\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06483286619186401\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01545683853328228\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024109473451972008\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019223112612962723\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.020979909226298332\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.042505793273448944\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.005931786261498928\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02224849723279476\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01033361628651619\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008675639517605305\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03281979262828827\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022045528516173363\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.012544534169137478\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01658112369477749\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011277547106146812\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017709674313664436\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01550803892314434\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.010340210050344467\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0376615971326828\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0163145512342453\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.027543621137738228\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026713823899626732\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011924663558602333\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.006383751519024372\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02740710973739624\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03364695981144905\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01770184561610222\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06186778098344803\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.01615677773952484\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013814205303788185\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.012851385399699211\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07831921428442001\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.024752018973231316\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04729094356298447\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.023169638589024544\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013901456259191036\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011760634370148182\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.013716964982450008\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.011166353709995747\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04630305990576744\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016929879784584045\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029967062175273895\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01040355209261179\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02692754939198494\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03855113685131073\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.026559732854366302\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08847362548112869\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06794945150613785\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0061982558108866215\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03605254366993904\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05638659745454788\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.013275617733597755\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015314418822526932\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014982207678258419\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04218220338225365\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0680912658572197\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.047094665467739105\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.007698045112192631\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03865300118923187\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017075419425964355\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.012273519299924374\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.046028636395931244\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.010430173017084599\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04505910351872444\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02285073511302471\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02126474864780903\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.035802777856588364\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0443846769630909\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.027584634721279144\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05799204111099243\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.023887891322374344\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04976124316453934\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026829494163393974\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012829677201807499\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.012300298549234867\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03665104880928993\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.020222214981913567\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.025126855820417404\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024798689410090446\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02458631619811058\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02877163700759411\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.036868736147880554\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022977834567427635\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019397379830479622\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02533022314310074\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.033825960010290146\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01187617052346468\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.044778913259506226\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.041979238390922546\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.020527267828583717\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016404196619987488\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.021630309522151947\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.006076999474316835\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.027972087264060974\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03693189099431038\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.036077726632356644\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.059431686997413635\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06978131830692291\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02914564125239849\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02143922634422779\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04057426378130913\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.038094744086265564\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.014233105815947056\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02116948924958706\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016249047592282295\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06575029343366623\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.10832934081554413\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.016718970611691475\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03044245019555092\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021787285804748535\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04025733843445778\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05811687558889389\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.043957315385341644\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04405162110924721\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01564428210258484\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015515320003032684\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03201507404446602\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.041048865765333176\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.036359935998916626\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014129167422652245\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03876201808452606\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.014699619263410568\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008299466222524643\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.023046204820275307\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03705981373786926\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02992120571434498\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021361052989959717\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01731940731406212\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04358154535293579\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02044951356947422\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03787096217274666\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0626586377620697\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.045340362936258316\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04186796396970749\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.040168918669223785\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017540881410241127\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05228853598237038\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.022348027676343918\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05333168804645538\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01983090303838253\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.018998298794031143\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04373839870095253\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05106623098254204\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.020109087228775024\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029063256457448006\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.043167442083358765\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04198862239718437\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04408228397369385\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.055634304881095886\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022450411692261696\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03755958378314972\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013935551047325134\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03791387379169464\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.022534547373652458\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030317965894937515\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03970152884721756\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.060987915843725204\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.02654843032360077\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015452549792826176\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.048803169280290604\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02949124202132225\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.033522170037031174\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03247160464525223\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.017186302691698074\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02906815893948078\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.046672139316797256\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06970442831516266\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01749565452337265\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.055186927318573\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02790938876569271\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022499490529298782\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0297919362783432\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018018728122115135\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04724448174238205\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.028723154217004776\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0670146569609642\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.102898508310318\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03221035748720169\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025708399713039398\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05100609362125397\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021591562777757645\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019521282985806465\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029200563207268715\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021046029403805733\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06305768340826035\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03707289695739746\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03779461979866028\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06868799030780792\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.040171921253204346\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07713942229747772\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.04416857287287712\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024024922400712967\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05938102304935455\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06557250767946243\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.028829488903284073\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05248874053359032\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.017652137205004692\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.033564649522304535\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01592891477048397\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04346073791384697\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.061031993478536606\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.023259799927473068\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04153769835829735\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.025796199217438698\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03484117239713669\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.027661509811878204\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01761287823319435\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03549739718437195\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04951370134949684\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05120471119880676\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03152834624052048\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02264339290559292\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06292323023080826\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06345266848802567\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04443252459168434\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03838947042822838\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.045208074152469635\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.015266815200448036\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02668735943734646\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.034100066870450974\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06352729350328445\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.06660183519124985\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.03205556422472\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018656475469470024\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03371249511837959\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.039931200444698334\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0284710880368948\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0273103266954422\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03512605279684067\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.029631853103637695\n",
      "Training accuracy 0.9921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 30/36 [1:58:22<23:34, 235.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.8423, acc: 0.3057\n",
      "eval loss 5.842277090996504\n",
      "Training loss: 0.014542377553880215\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011604237370193005\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02273908443748951\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03268013522028923\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018456973135471344\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.027152065187692642\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015009310096502304\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025279484689235687\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02155003696680069\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009700505062937737\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.044816624373197556\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.010957944206893444\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03722378984093666\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.005123916082084179\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009600158780813217\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02882087603211403\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.014272257685661316\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.023387866094708443\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.020444801077246666\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03623836487531662\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0240855123847723\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04202961549162865\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011627592146396637\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.006931185722351074\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01855454221367836\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027519548311829567\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05396691709756851\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.016375547274947166\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.022530071437358856\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01634185202419758\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.012598957866430283\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.012147023342549801\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008959266357123852\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013596419245004654\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016492854803800583\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.012564213946461678\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0209824126213789\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012056219391524792\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.025015274062752724\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03272900730371475\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011584452353417873\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.027358481660485268\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012683309614658356\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011160715483129025\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.007824184373021126\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008540145121514797\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.019357044249773026\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02466350793838501\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013936041854321957\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.019741401076316833\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.025254052132368088\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04068849980831146\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02028597705066204\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018635118380188942\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.048353176563978195\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0061090621165931225\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00517728878185153\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009000048972666264\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.035043321549892426\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.016764463856816292\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.014811389148235321\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03536108881235123\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04553733766078949\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015082876197993755\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01923835463821888\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02569378912448883\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05201173201203346\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0565633550286293\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.029866673052310944\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011582648381590843\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05285978689789772\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03095124289393425\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019063573330640793\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02456539124250412\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02114522084593773\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01169201172888279\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030566701665520668\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015082879923284054\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03811366856098175\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.008104280568659306\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01560924295336008\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026545584201812744\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029593946412205696\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019213363528251648\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015990903601050377\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.018079660832881927\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015257177874445915\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01754012517631054\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011522150598466396\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03781726211309433\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019722895696759224\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03598484396934509\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.036042895168066025\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019393153488636017\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03966037929058075\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02397260256111622\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02927165850996971\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05811188742518425\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06487026065587997\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.041156791150569916\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02017519809305668\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0038723498582839966\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.041383687406778336\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.024841638281941414\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018486304208636284\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.028027841821312904\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03343404829502106\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024574628099799156\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03720482811331749\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021633949130773544\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019464680925011635\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017957203090190887\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.054811347275972366\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06328336894512177\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019925052300095558\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01598381996154785\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026796748861670494\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.042096879333257675\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.036650337278842926\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.016093047335743904\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027132242918014526\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024747338145971298\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05873029679059982\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01967635378241539\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05849337577819824\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06211605295538902\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04451288655400276\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04087980464100838\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04374226555228233\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04805293306708336\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.017550028860569\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030170945450663567\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021632228046655655\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013961627148091793\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.021810121834278107\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.047624293714761734\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.007726219017058611\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.021775992587208748\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011350912041962147\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0494837611913681\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01503998227417469\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.051030099391937256\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.019815785810351372\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02266806922852993\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.044649552553892136\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.030548380687832832\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.043707527220249176\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.024356435984373093\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021475132554769516\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05068111792206764\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.038536541163921356\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.010464380495250225\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.035674355924129486\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02585311233997345\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03791581839323044\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0399213582277298\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.018947472795844078\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.033358391374349594\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.039160218089818954\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0216844342648983\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04369395226240158\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05484457314014435\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02301318570971489\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03575419634580612\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04405492544174194\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0244307704269886\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02758323773741722\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04269171878695488\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.016972016543149948\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.026566287502646446\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05175488442182541\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06531068682670593\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.010054240003228188\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.058755554258823395\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.053586460649967194\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04319867119193077\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04416009411215782\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05598325654864311\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028921665623784065\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.034318771213293076\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.005788976326584816\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03385664522647858\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03301879018545151\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019609574228525162\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04192695394158363\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0687621682882309\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.015980945900082588\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0526156947016716\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04281618073582649\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.010785982944071293\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02680988982319832\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01523465383797884\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.016905169934034348\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.052953172475099564\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05614323914051056\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.028000183403491974\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.027280498296022415\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.030620213598012924\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.043836113065481186\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.027852172031998634\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012300705537199974\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011170901358127594\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.040715888142585754\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.028231080621480942\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05684119835495949\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.026420097798109055\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.043827008455991745\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03602081909775734\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04440027102828026\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.042166683822870255\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05586190149188042\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02304663509130478\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.035516344010829926\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.009671049192547798\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03683178126811981\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.016466327011585236\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00868542492389679\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03512350469827652\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024048924446105957\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028835397213697433\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.025974567979574203\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013069991953670979\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.06043209508061409\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04424034431576729\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04341287910938263\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04712468758225441\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01882312074303627\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0562962107360363\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05296077951788902\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07748322188854218\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04367867484688759\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07720491290092468\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.11572739481925964\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.03606615215539932\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07010975480079651\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.027673188596963882\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.025282902643084526\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04635422304272652\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02031789720058441\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02547765150666237\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.10276790708303452\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.02720971591770649\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.038206230849027634\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.045812759548425674\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015233435668051243\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.040457677096128464\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0573083870112896\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07790830731391907\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.048734068870544434\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03683462738990784\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.050502710044384\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.016415582969784737\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015546838752925396\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03154321387410164\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07984916120767593\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07703696936368942\n",
      "Training accuracy 0.97265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 31/36 [2:02:22<19:45, 237.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.7797, acc: 0.2987\n",
      "eval loss 5.779699953272939\n",
      "Training loss: 0.015206946060061455\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02111104317009449\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0037664691917598248\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.018757376819849014\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017204754054546356\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009755893610417843\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018313908949494362\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.011611697264015675\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016397960484027863\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02714085765182972\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.014260943047702312\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015799392014741898\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01129396166652441\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.009441586211323738\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01584697514772415\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01848745159804821\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03284338861703873\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03142152354121208\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.020677851513028145\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05435807257890701\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.035002391785383224\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.009952924214303493\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.015937449410557747\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.019468406215310097\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02987566962838173\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02073369175195694\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.006666031200438738\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00530539033934474\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.021928858011960983\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.032025955617427826\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013731643557548523\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03059779852628708\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016572128981351852\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009379064664244652\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.018927890807390213\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.025016067549586296\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.00940459780395031\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.052464183419942856\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.010044584050774574\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.018154285848140717\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.006815699394792318\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.028391158208251\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.005683267954736948\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.044337060302495956\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.029865706339478493\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021555917337536812\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.012440606020390987\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030335595831274986\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.025607667863368988\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018208950757980347\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.010141095146536827\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01669183559715748\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06203426420688629\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.017852643504738808\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030036132782697678\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024039220064878464\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01993660070002079\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.014719176106154919\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.012494625523686409\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.003119461238384247\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02840648777782917\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.026389868929982185\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020600302144885063\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014871095307171345\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.011727016419172287\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01780867576599121\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018595077097415924\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.018688727170228958\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00989585742354393\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007185276597738266\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02517492137849331\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022107727825641632\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04008415341377258\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01716890186071396\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029736222699284554\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.008229321800172329\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.016757335513830185\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01288932841271162\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.008115902543067932\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01574881747364998\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04028690233826637\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03907572105526924\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06020445004105568\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03441980853676796\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011166935786604881\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.040630925446748734\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04508434981107712\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.017484646290540695\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022324543446302414\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03023860976099968\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.030607907101511955\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018560513854026794\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019434576854109764\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02115972340106964\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02377530187368393\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02550516650080681\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.00779125839471817\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.022154051810503006\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.010569482110440731\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.057488419115543365\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01695619337260723\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.00987945031374693\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04895753413438797\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03843292221426964\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014730963855981827\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015917839482426643\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02515765093266964\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02975716069340706\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.00633374135941267\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.05408883094787598\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.019826671108603477\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01830289140343666\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013078244403004646\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.028679300099611282\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.005143255926668644\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.022865822538733482\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01513128075748682\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01979745365679264\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03282279521226883\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07210688292980194\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.047712504863739014\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03941216692328453\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.015758903697133064\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.019826451316475868\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03631354868412018\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016309451311826706\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.034300293773412704\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023720599710941315\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019994240254163742\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02452653832733631\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01817602850496769\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016846725717186928\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.021777991205453873\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018490754067897797\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026814168319106102\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02924015000462532\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.034415166825056076\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.10541889816522598\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02782476879656315\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03309497609734535\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.022364569827914238\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05877753347158432\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.020760618150234222\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009818199090659618\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03548983111977577\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.033738039433956146\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.015807293355464935\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05037859454751015\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03192521631717682\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024361375719308853\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02727050706744194\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04002281650900841\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07762463390827179\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01520933024585247\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.028550349175930023\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.026342134922742844\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03053389862179756\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03688989579677582\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02909904718399048\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0509723462164402\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03721550852060318\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.027072325348854065\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.042433880269527435\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028157468885183334\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.016818922013044357\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04840125888586044\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.048888273537158966\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.055692706257104874\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0070167724043130875\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03268593177199364\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04524393379688263\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.052865009754896164\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.029183072969317436\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07810729742050171\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.010276059620082378\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.010936044156551361\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.036087099462747574\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.009491699747741222\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03831637278199196\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.021045677363872528\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03333272784948349\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.043035440146923065\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03128611668944359\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.024387892335653305\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06857874989509583\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.022364461794495583\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0348706878721714\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.018780134618282318\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017054034397006035\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04242001846432686\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04515295475721359\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03160560503602028\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03778912499547005\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.048478636890649796\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0386975072324276\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03326503559947014\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03928603231906891\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04435165971517563\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04869964346289635\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06478822231292725\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05254323035478592\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07099828124046326\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.013742293231189251\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.061239033937454224\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.027453044429421425\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05364757776260376\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03878936916589737\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016167640686035156\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04982239380478859\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.016592225059866905\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.030229797586798668\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01945161633193493\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.034495141357183456\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07359275966882706\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.01603846810758114\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.044413067400455475\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04613203927874565\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04371792450547218\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07270898669958115\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.029904648661613464\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.09806135296821594\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.02149105817079544\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07415148615837097\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0402255579829216\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03714499622583389\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.015823468565940857\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.034467171877622604\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05491406098008156\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05359508842229843\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01298210583627224\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.044850368052721024\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08516383171081543\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05436985194683075\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.0540151447057724\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03868231177330017\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.039512958377599716\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06564584374427795\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06740137934684753\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.047115523368120193\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.030343135818839073\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04659620672464371\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.029812000691890717\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.054246686398983\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.020961401984095573\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07118092477321625\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03802920877933502\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023606758564710617\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05098229646682739\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06154891848564148\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.051983706653118134\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05680488049983978\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.055260587483644485\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0330536887049675\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015006196685135365\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05948866903781891\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0693679079413414\n",
      "Training accuracy 0.9765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 32/36 [2:06:22<15:51, 237.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.4083, acc: 0.3118\n",
      "eval loss 5.4082997515797615\n",
      "Training loss: 0.02283494733273983\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.052667874842882156\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.021884454414248466\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.026811616495251656\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03234873712062836\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.008924228139221668\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.018097003921866417\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0260249525308609\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012628039345145226\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.016721727326512337\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024146107956767082\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0366361029446125\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.020375343039631844\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02940228208899498\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011367416009306908\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028553083539009094\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05874576419591904\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02642018161714077\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04686003923416138\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016297081485390663\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02192152850329876\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013340397737920284\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03541610762476921\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.022439440712332726\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02900341898202896\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02567652426660061\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0356844887137413\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.061673734337091446\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.016706114634871483\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04430118203163147\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02415870875120163\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02404550276696682\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024219339713454247\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.011952592991292477\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013243195600807667\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.008857375010848045\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05411037430167198\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019079266116023064\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.024158619344234467\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.010859917849302292\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01769357919692993\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03514161705970764\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01876639388501644\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04627912491559982\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03817090019583702\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.011075466871261597\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05243365466594696\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.024941107258200645\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022066999226808548\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.045587968081235886\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.009996083565056324\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.033385541290044785\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015057198703289032\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04187079891562462\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.015488983131945133\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.043159160763025284\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.030763836577534676\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.058905743062496185\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.018680071458220482\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03459731861948967\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.033470675349235535\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07670453190803528\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03348885104060173\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04357558488845825\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.031851980835199356\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01957864686846733\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.047758813947439194\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0077565205283463\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04029551148414612\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02792043425142765\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03852558881044388\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04926135018467903\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.044116318225860596\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025747772306203842\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04108410328626633\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02560284174978733\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03313068300485611\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04238595813512802\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.027731461450457573\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025310484692454338\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03648265823721886\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012517188675701618\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04786708205938339\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.028950512409210205\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02674558013677597\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02403469756245613\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03797313570976257\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02358146943151951\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03919075056910515\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04198678582906723\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03512176126241684\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01944739557802677\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01422989834100008\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05150754377245903\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03144564479589462\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022102609276771545\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022878121584653854\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04939299449324608\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011892707087099552\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03206291422247887\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024010280147194862\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.029319433495402336\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.014741836115717888\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01916818507015705\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03488241881132126\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03514835983514786\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03531896322965622\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.049529723823070526\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04464070126414299\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.09216360747814178\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.031025942414999008\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06848429888486862\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.052648868411779404\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.011343645863234997\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02348439022898674\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.027660656720399857\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01666293665766716\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04121618717908859\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.031159356236457825\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05178333818912506\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03383107855916023\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.08250849694013596\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06619719415903091\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.03561924025416374\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06821189820766449\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.053785767406225204\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05939203500747681\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05805603042244911\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07687527686357498\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0461459681391716\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.019362999126315117\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05222995579242706\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.050803303718566895\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.020206745713949203\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02855147235095501\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06020874157547951\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.038792602717876434\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.015462419018149376\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.052992574870586395\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.059895195066928864\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04149419069290161\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.018673306331038475\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02514071762561798\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023010900244116783\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04493730887770653\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.051304496824741364\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.018771925941109657\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05654739961028099\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.044737741351127625\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05482438579201698\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04125962406396866\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.10288931429386139\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.0372389480471611\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07758070528507233\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01730416715145111\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04682346060872078\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04481707513332367\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0207559484988451\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03686804324388504\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04931386187672615\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04501672834157944\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03916165232658386\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05050349980592728\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04629945382475853\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.034823257476091385\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.031760796904563904\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03457983210682869\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.025996332988142967\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024011267349123955\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02919488586485386\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07422760874032974\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.060815341770648956\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.030939429998397827\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04616975039243698\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.052618637681007385\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03205098956823349\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05207664892077446\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07294444739818573\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.052562788128852844\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04596730321645737\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.051847681403160095\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04383963346481323\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06847674399614334\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04690123721957207\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0435979925096035\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.023750610649585724\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.026343995705246925\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03820415958762169\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03346492350101471\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04708322882652283\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05065307393670082\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.013358629308640957\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.10031397640705109\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02737618423998356\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03322119638323784\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.10636714845895767\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.052246399223804474\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03287714347243309\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.037157077342271805\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06024237722158432\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0703430324792862\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03584279119968414\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05430010333657265\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08129806816577911\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03813818469643593\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06155531853437424\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.022305728867650032\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05484560877084732\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07778634876012802\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06572214514017105\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05118916928768158\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.016582857817411423\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0590769462287426\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02605380304157734\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018328577280044556\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.08273159712553024\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.06255384534597397\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.043981730937957764\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05391593277454376\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.024415070191025734\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03536325693130493\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.09312578290700912\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.032718077301979065\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.034987181425094604\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07073196768760681\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07528664171695709\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.033066753298044205\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.07679013907909393\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06277074664831161\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04503481090068817\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.08576777577400208\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.04823727905750275\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.062346771359443665\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.049094799906015396\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019540008157491684\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.034056156873703\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.09015090018510818\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.020279565826058388\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06794667989015579\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.052993111312389374\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.029459917917847633\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03354264795780182\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.09069185703992844\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.015247036702930927\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028602786362171173\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02948397770524025\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.09553965926170349\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.09229215979576111\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03118588961660862\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04519987106323242\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02679072692990303\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.08058968186378479\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.054909978061914444\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0723261684179306\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.049379292875528336\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0431295782327652\n",
      "Training accuracy 0.9921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 33/36 [2:10:19<11:52, 237.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.8562, acc: 0.3150\n",
      "eval loss 5.856206335127354\n",
      "Training loss: 0.01658676192164421\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.00975214783102274\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04499545693397522\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.020517464727163315\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02215767279267311\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023391691967844963\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030707864090800285\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03383002430200577\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026261253282427788\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0068138521164655685\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.009610114619135857\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.019656578078866005\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03207988664507866\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03189660981297493\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05280393362045288\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.010508029721677303\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04544876888394356\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015837080776691437\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02017020434141159\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02657477557659149\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03823678568005562\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.042829450219869614\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.044646669179201126\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.029392698779702187\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017739271745085716\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015087627805769444\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05169805511832237\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.031823381781578064\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.030683832243084908\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02343955636024475\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.018912388011813164\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02953910082578659\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02869284525513649\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.031746380031108856\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.044029273092746735\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02936517260968685\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0391727015376091\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.034546397626399994\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.057690173387527466\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012681277468800545\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02581627666950226\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01760317012667656\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04800238087773323\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.012778480537235737\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.011037610471248627\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.05051770433783531\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.006226689554750919\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.024670381098985672\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019048556685447693\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02291273884475231\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.044890739023685455\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.013201114721596241\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.007890350185334682\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012724549509584904\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0319385752081871\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02328892983496189\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02335459366440773\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03061707504093647\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01311949547380209\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04124714434146881\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023101147264242172\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022298485040664673\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04128945618867874\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01241894531995058\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026546811684966087\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022777872160077095\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.033386312425136566\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04832589998841286\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04321332275867462\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05287867411971092\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.027095748111605644\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03581618517637253\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.033721428364515305\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015490522608160973\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013800798915326595\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.007728647440671921\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007403966970741749\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04040822386741638\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02153310365974903\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0187021866440773\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05325959250330925\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.027508964762091637\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04117126762866974\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04342937842011452\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.032160669565200806\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021516302600502968\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03985980525612831\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.027498526498675346\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.11472750455141068\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.042692963033914566\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03984532877802849\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0304674431681633\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.07479196786880493\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0437668152153492\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.014830639585852623\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.19257581233978271\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.027519946917891502\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05167417973279953\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01320740208029747\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.022040145471692085\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02630615420639515\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026502838358283043\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0760776624083519\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.01166650652885437\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04042636975646019\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023777419701218605\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.032130297273397446\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020568355917930603\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.048584822565317154\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.012274224311113358\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.023375429213047028\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04763537645339966\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.039128679782152176\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.049328021705150604\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.029268069192767143\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.047244518995285034\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02795454114675522\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009845676831901073\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.029513796791434288\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0300016850233078\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02247159741818905\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016574833542108536\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.060521967709064484\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.023092618212103844\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03293321654200554\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04500340670347214\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.034285396337509155\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.019602715969085693\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0201027300208807\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.060516729950904846\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03406442329287529\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02369113825261593\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07613995671272278\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.026145845651626587\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.033344101160764694\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029764285311102867\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04190104454755783\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.060636889189481735\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03762293606996536\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.020927226170897484\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0583171471953392\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03492064028978348\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01578691601753235\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02430005744099617\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02842201106250286\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.010002613998949528\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02792423404753208\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.029370756819844246\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04171263426542282\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.036785516887903214\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.040028493851423264\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.09664829075336456\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.10546643286943436\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.018848149105906487\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.043597761541604996\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.021111803129315376\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03702289238572121\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.044628798961639404\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.053371936082839966\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03548618033528328\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02555261366069317\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.006773063447326422\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.07638099789619446\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.011705855838954449\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.05669185146689415\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07138630747795105\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.0726187452673912\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.07986313849687576\n",
      "Training accuracy 0.96484375\n",
      "Training loss: 0.051420021802186966\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.020223472267389297\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.049729615449905396\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05051751434803009\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.060342006385326385\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.046947307884693146\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024199599400162697\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023952821269631386\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021536123007535934\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02950553596019745\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02196907065808773\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.0626005232334137\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.032246895134449005\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02880479395389557\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025676148012280464\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.031119583174586296\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02562086284160614\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02181393839418888\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.070621058344841\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.026119928807020187\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02851748839020729\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.030549831688404083\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.057682424783706665\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02240857109427452\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0277986079454422\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08103477954864502\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.043991878628730774\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03487687185406685\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017922447994351387\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03849819302558899\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019171861931681633\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05841178074479103\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04494905471801758\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.013848243281245232\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04061740264296532\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02731919102370739\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.10013674944639206\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.02935323677957058\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018681704998016357\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05530690774321556\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02932819351553917\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04422258213162422\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.058202747255563736\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.016891805455088615\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01891607604920864\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.023319968953728676\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03376010060310364\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03755246102809906\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029034944251179695\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013698161579668522\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0581129752099514\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.0502651184797287\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05789053812623024\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05809566006064415\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.039765600115060806\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.018385503441095352\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03520994260907173\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.050655148923397064\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.037209369242191315\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018029145896434784\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0849764347076416\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.0803593322634697\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04516743868589401\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05477103963494301\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.01845303364098072\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018145142123103142\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03203294798731804\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0192516278475523\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0442277267575264\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.029681550338864326\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06204819679260254\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03589756786823273\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04500948265194893\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02393982745707035\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.041771840304136276\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04743475094437599\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05554715543985367\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06617672741413116\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05612652748823166\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02983110211789608\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04487692937254906\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06669358164072037\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06892185658216476\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.07465463876724243\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04101983830332756\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02983899973332882\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009512393735349178\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.05002782866358757\n",
      "Training accuracy 0.98828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 34/36 [2:14:14<07:54, 237.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.1467, acc: 0.3139\n",
      "eval loss 5.146650904789567\n",
      "Training loss: 0.009577224962413311\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.014831026084721088\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03107038512825966\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013331103138625622\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.032775696367025375\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.027243513613939285\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026477644219994545\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024512823671102524\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.019989773631095886\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01656075194478035\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027641557157039642\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0064423661679029465\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04130452126264572\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016822516918182373\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02235664799809456\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01394836325198412\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017854765057563782\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.037872496992349625\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024456188082695007\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028700247406959534\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02657933346927166\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.008405645377933979\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.011709604412317276\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026994500309228897\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.00677397008985281\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.025351179763674736\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.025436576455831528\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.034003421664237976\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05160239711403847\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.032032594084739685\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02819838561117649\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02628670632839203\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.00865964312106371\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.031634390354156494\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.009219483472406864\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013687502592802048\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.023038091138005257\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.034249380230903625\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03475160896778107\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013537505641579628\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.019904740154743195\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.040741223841905594\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012725193053483963\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.005195107311010361\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.02370426245033741\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.010200589895248413\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0419696606695652\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.006784028839319944\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.053920600563287735\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.010697995312511921\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.016665060073137283\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03604910150170326\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024626871570944786\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030170952901244164\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.020147638395428658\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01642821915447712\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07459227740764618\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03707755357027054\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.029463358223438263\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015755515545606613\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015106424689292908\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.020064812153577805\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06846178323030472\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04051804915070534\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01719621755182743\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.014747519977390766\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03430784493684769\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.034345466643571854\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.010055040009319782\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04692777246236801\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03388844057917595\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02858666703104973\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011597387492656708\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.021086519584059715\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0455348826944828\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01576092652976513\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03566144034266472\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.036646194756031036\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018965672701597214\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04575938731431961\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.045146167278289795\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03520575538277626\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01140840444713831\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.026048725470900536\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016063226386904716\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.059205275028944016\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.027910955250263214\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011915315873920918\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.021339520812034607\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.006795424968004227\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.007401437498629093\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.021730178967118263\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.048300888389348984\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03151984140276909\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.034757476300001144\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011189503595232964\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.019240064546465874\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03870085999369621\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03439469262957573\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.013902614824473858\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023237112909555435\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.043759509921073914\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.026704978197813034\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05067693069577217\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.024966873228549957\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.036167215555906296\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04155929386615753\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02887675352394581\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021616190671920776\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.007000304758548737\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.014631903730332851\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.01347103901207447\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.047230273485183716\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0448022186756134\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03481294587254524\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03518706560134888\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.044831328094005585\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02720683254301548\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03439045697450638\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.042659513652324677\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.020579608157277107\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03029509447515011\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03595104441046715\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025535624474287033\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02712632529437542\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.010880997404456139\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01789010874927044\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03693130239844322\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.042102206498384476\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021171458065509796\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015538571402430534\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.036374643445014954\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.049445148557424545\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01928534545004368\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.031594857573509216\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022440271452069283\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017648642882704735\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04656488448381424\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.045695461332798004\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0655127689242363\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05375232920050621\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.04979390278458595\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03700195997953415\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02781764790415764\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01545571070164442\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02531866915524006\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.005417340435087681\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00835899356752634\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04565298929810524\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.055452749133110046\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03136185184121132\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03423373028635979\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015237578190863132\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04247734695672989\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02317311055958271\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025373097509145737\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.09581761062145233\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.02675100788474083\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02626957558095455\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04371495917439461\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.038690585643053055\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013159213587641716\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.014835754409432411\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02134360745549202\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028390485793352127\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04893796890974045\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.030709097161889076\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04134911298751831\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05597794055938721\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.036512263119220734\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.027752602472901344\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02791653573513031\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024438457563519478\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02460201270878315\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017762895673513412\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.00891542062163353\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03632495179772377\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.044547922909259796\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04569888114929199\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.022178974002599716\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04715495556592941\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04082312434911728\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05765828117728233\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.036079395562410355\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.07408052682876587\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.01807003654539585\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03946732357144356\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.038442373275756836\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.09378911554813385\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.06442560255527496\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.006691769231110811\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.029322460293769836\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.08835083991289139\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.030380357056856155\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.058798521757125854\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03270646557211876\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03237024322152138\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04225893318653107\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05482373386621475\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.03966965898871422\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.06718208640813828\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06931304931640625\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.049298010766506195\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04754003882408142\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03171264007687569\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01923694647848606\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03601440414786339\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0335979238152504\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.056598782539367676\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.02557816542685032\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04025593772530556\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04230118542909622\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03759519010782242\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02216828428208828\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04783046245574951\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04320475086569786\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.028559206053614616\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04690217599272728\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08006918430328369\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.037896353751420975\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.027781376615166664\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01175412256270647\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.06252424418926239\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.03718874230980873\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.054051198065280914\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01965896040201187\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.014790001325309277\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05521267652511597\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05472772940993309\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.07050982862710953\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03045845590531826\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.014576295390725136\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.034299515187740326\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.046244680881500244\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.10033433139324188\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.0907277911901474\n",
      "Training accuracy 0.95703125\n",
      "Training loss: 0.0391274057328701\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025007709860801697\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.022675909101963043\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03187756612896919\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.10840150713920593\n",
      "Training accuracy 0.9609375\n",
      "Training loss: 0.05841178819537163\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06238485872745514\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.024231702089309692\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05380239710211754\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.021291296929121017\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05124358460307121\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.029917191714048386\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018581857904791832\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.029221512377262115\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06420938670635223\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.044221941381692886\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.03535481169819832\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04175044968724251\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.017186157405376434\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02479599416255951\n",
      "Training accuracy 0.9921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 35/36 [2:18:14<03:57, 237.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.7789, acc: 0.3231\n",
      "eval loss 5.778931964188814\n",
      "Training loss: 0.012252062559127808\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01080595888197422\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03898809105157852\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05362093076109886\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01941283792257309\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.038277216255664825\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03880506753921509\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.025048429146409035\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.043933968991041183\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.017695065587759018\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.009050079621374607\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03039083629846573\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013033954426646233\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.012209679931402206\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02688976190984249\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.023590533062815666\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011199438013136387\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.019143149256706238\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.0192599855363369\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.013055716641247272\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.07669129967689514\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.02980727143585682\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.010889633558690548\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01826685480773449\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01959158107638359\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02038157358765602\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01592113822698593\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.024564025923609734\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03851862996816635\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.031179320067167282\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04376538470387459\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02755984477698803\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04728250950574875\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04317070171236992\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.018978318199515343\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027107641100883484\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011071487329900265\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.010702713392674923\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03790264204144478\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.004643096122890711\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.018406260758638382\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03077508509159088\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01339782401919365\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.049008872359991074\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.06416168063879013\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.034286607056856155\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.008688080124557018\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03272368386387825\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02476750873029232\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.020630862563848495\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.026600880548357964\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03351842612028122\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024689234793186188\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03156014904379845\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01327990647405386\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.023101193830370903\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.008515226654708385\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.014331704936921597\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02018602378666401\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0598372183740139\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015176583081483841\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.022307267412543297\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03517867997288704\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.051757652312517166\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05203855410218239\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018713010475039482\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018378399312496185\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03760329633951187\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01415767427533865\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.047755759209394455\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02851596288383007\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012801367789506912\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01256850827485323\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01754162460565567\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03186716511845589\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.034223537892103195\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016140883788466454\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.012322244234383106\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04372657090425491\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024404270574450493\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01791291870176792\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.029163848608732224\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028187861666083336\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.02001754567027092\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.015926077961921692\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.017487553879618645\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.012650398537516594\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03356451168656349\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0258273184299469\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.060675252228975296\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.025233760476112366\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01509501226246357\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024951213970780373\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04241199791431427\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01311427727341652\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.028566669672727585\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.016934214159846306\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008982199244201183\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03135162591934204\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.008491461165249348\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.034459441900253296\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.008245333097875118\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.013070604763925076\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.1028023287653923\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.015041262842714787\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03828240558505058\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.019384311512112617\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013298089616000652\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06356283277273178\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.02932218834757805\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.021004270762205124\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.018145447596907616\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.039717480540275574\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.054134517908096313\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.025182576850056648\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.013237685896456242\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.016945693641901016\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.021124545484781265\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.021140532568097115\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0242872666567564\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.03191431611776352\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.028012370690703392\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.008186341263353825\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.00892343744635582\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.05491652712225914\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06292010098695755\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.015754874795675278\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04714391380548477\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.028631266206502914\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.010190078988671303\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.020473172888159752\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.024806436151266098\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04304277151823044\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.029768764972686768\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.06529931724071503\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.01548764668405056\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.016683155670762062\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.031047016382217407\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04077959805727005\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.01582273095846176\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02538364753127098\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.02626066468656063\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.010445878840982914\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.024112842977046967\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.046407680958509445\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06138477101922035\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.009973232634365559\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.09922923892736435\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.0570482574403286\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.035534657537937164\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.06598757952451706\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.007368630263954401\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.06471262872219086\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.041657332330942154\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05744786933064461\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.06755763292312622\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04809311032295227\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.03372632712125778\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.050781700760126114\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.022674696519970894\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03739042207598686\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.02999822609126568\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.046793799847364426\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04591331630945206\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04948807135224342\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.021838990971446037\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03964730724692345\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.011707332916557789\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.03869820758700371\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.026038428768515587\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03985501825809479\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03129267692565918\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.08665797859430313\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.025631271302700043\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05503479763865471\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.02800597995519638\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.016841815784573555\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.027356456965208054\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.036706630140542984\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.026150517165660858\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0497513972222805\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.016009610146284103\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011819293722510338\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.026223603636026382\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.01879367232322693\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.01895962655544281\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.044522881507873535\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.009556497447192669\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.06941408663988113\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.009891332127153873\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.04241318628191948\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05782033130526543\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.053918078541755676\n",
      "Training accuracy 0.97265625\n",
      "Training loss: 0.01424419041723013\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.051111217588186264\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.04881663993000984\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.026532188057899475\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.050205767154693604\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03530554100871086\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05054379999637604\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.024452628567814827\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.011189231649041176\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.04858475923538208\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.030641354620456696\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.01720554567873478\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017628535628318787\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.033940933644771576\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.017980346456170082\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.018745986744761467\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.05171891301870346\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.030963651835918427\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03779524564743042\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.036077067255973816\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04265391454100609\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.03089260123670101\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.024910692125558853\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04626372456550598\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.044494424015283585\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.036729056388139725\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.014052635990083218\n",
      "Training accuracy 0.99609375\n",
      "Training loss: 0.07033197581768036\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.05416707322001457\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0680529996752739\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.04496292024850845\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.05192049965262413\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.008779479190707207\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.05439145117998123\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04271645471453667\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.03859129920601845\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.022590281441807747\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.029075711965560913\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.017259204760193825\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.04347126930952072\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.08628164976835251\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.027106618508696556\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.045120902359485626\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.056505344808101654\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.012434637174010277\n",
      "Training accuracy 1.0\n",
      "Training loss: 0.029298903420567513\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.038040291517972946\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.0640326514840126\n",
      "Training accuracy 0.96875\n",
      "Training loss: 0.026182902976870537\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.0361839160323143\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.04654378071427345\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.03973420709371567\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.018345627933740616\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05233690142631531\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.09737490862607956\n",
      "Training accuracy 0.9765625\n",
      "Training loss: 0.04041746258735657\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.013713971711695194\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.05169949680566788\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.04418978840112686\n",
      "Training accuracy 0.98828125\n",
      "Training loss: 0.05455159395933151\n",
      "Training accuracy 0.98046875\n",
      "Training loss: 0.018510783091187477\n",
      "Training accuracy 0.9921875\n",
      "Training loss: 0.043682899326086044\n",
      "Training accuracy 0.984375\n",
      "Training loss: 0.05437077209353447\n",
      "Training accuracy 0.9765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [2:22:15<00:00, 237.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.0510, acc: 0.3200\n",
      "eval loss 6.051029317080975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_tasks_range = [2**i for i in range(4, 24)]  # Range of number of tasks\n",
    "hidden_size_range = [2**i for i in range(4, 10)]  # Range of hidden sizes\n",
    "#pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
    "results_list = []\n",
    "for hidden_size in hidden_size_range:\n",
    "    for num_tasks in num_tasks:\n",
    "        config[\"num_of_tasks\"] = num_tasks\n",
    "        config[\"in_context_learner\"][\"dim\"] = hidden_size\n",
    "        config[\"in_context_learner\"][\"inner_dim\"] = config[\"in_context_learner\"][\"dim\"] * 4\n",
    "        model_optim = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], eps=config[\"eps\"])\n",
    "        model = InContextLearner(**config[\"in_context_learner\"]).to(config[\"device\"])\n",
    "\n",
    "        for epoch in tqdm.tqdm(range(start_epoch,config[\"epochs\"])):\n",
    "            model.train()\n",
    "            for i, (x, y) in enumerate(train_loader):\n",
    "                x, y = x.to(config[\"device\"]), y.to(config[\"device\"])\n",
    "                y_hat = model(x)\n",
    "                if config[\"whole_seq_prediction\"]:\n",
    "                    loss = F.cross_entropy(y_hat.view(-1, 10), y.view(-1))\n",
    "                else:\n",
    "                    loss = F.cross_entropy(y_hat, y)\n",
    "                loss.backward()\n",
    "                model_optim.step()\n",
    "                model_optim.zero_grad()\n",
    "\n",
    "                # update the plot\n",
    "                # if i % 10 == 9:\n",
    "                #     if config[\"whole_seq_prediction\"]:\n",
    "                #         acc_over_seq = (y_hat.argmax(dim=-1) == y).float().mean(dim=0) # (seq_len,)\n",
    "                #         acc_max_improvement_within_seq = \\\n",
    "                #             ((y_hat[:,1:,:].argmax(dim=-1) == y[:,1:]).float().max(dim=-1).values \\\n",
    "                #             - (y_hat[:,0,:].argmax(dim=-1) == y[:,0]).float()).mean().item()\n",
    "                    #     live_plot.update({\"train_acc_over_seq\": acc_over_seq.tolist()}, reset=True)\n",
    "                    #     live_plot.update({\"train_acc_max_improvement_within_seq\": acc_max_improvement_within_seq})\n",
    "                    # live_plot.update({\"train_loss\": loss.item(), \"train_acc\": (y_hat.argmax(dim=-1) == y).float().mean().item()})\n",
    "                    # live_plot.draw()\n",
    "                print(f\"Training loss: {loss.item()}\")\n",
    "                print(f\"Training accuracy {(y_hat.argmax(dim=-1) == y).float().mean().item()}\")\n",
    "\n",
    "            ### evaluate\n",
    "            eval_loss, eval_acc, acc_over_seq, acc_max_improvement_within_seq = eval(model, test_loader)\n",
    "            #live_plot.update({\"eval_loss\": eval_loss, \"eval_acc\": eval_acc})\n",
    "            if config[\"whole_seq_prediction\"]:\n",
    "                print(acc_max_improvement_within_seq)\n",
    "                print(acc_over_seq)\n",
    "            #     live_plot.update({\"eval_acc_max_improvement_within_seq\": acc_max_improvement_within_seq})\n",
    "            #     live_plot.update({\"eval_acc_over_seq\": acc_over_seq}, reset=True)\n",
    "            # live_plot.draw()\n",
    "            print(f\"eval loss {eval_loss}\")\n",
    "            ### save model\n",
    "            if epoch % config[\"ckpt_freq\"] == 0:\n",
    "                save_checkpoint(epoch=epoch, model=model, optimizer=model_optim, loss_train=loss.detach(), loss_eval=eval_loss, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.355344772338867\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.352262258529663\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4360461235046387\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.429872989654541\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3914549350738525\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.406686544418335\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3858869075775146\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3935742378234863\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3675293922424316\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3444132804870605\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3926217555999756\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.414520025253296\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4145994186401367\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4070088863372803\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.338331937789917\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3870654106140137\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3952207565307617\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3604726791381836\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.373647928237915\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4179587364196777\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3874950408935547\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4164795875549316\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.409937620162964\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3700802326202393\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3902783393859863\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4361703395843506\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3461110591888428\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3946404457092285\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3699471950531006\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.371783494949341\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.359330654144287\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.391409397125244\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.410343885421753\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3965108394622803\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4050405025482178\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.376025438308716\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3935482501983643\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3854892253875732\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4184494018554688\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.432981491088867\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.448150157928467\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.379838705062866\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.394958019256592\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3668394088745117\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3579585552215576\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.382383346557617\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3507115840911865\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3859167098999023\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.389712333679199\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.390331983566284\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4058220386505127\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4116625785827637\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3480498790740967\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3550357818603516\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.385099411010742\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4067511558532715\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.360114097595215\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3494935035705566\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4210588932037354\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4147608280181885\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.420219659805298\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3811287879943848\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4064579010009766\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4220736026763916\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3631319999694824\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3880491256713867\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4067306518554688\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.396789312362671\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.399153232574463\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3736448287963867\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.377314805984497\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3843657970428467\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4060773849487305\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3851535320281982\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.361748218536377\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.351611375808716\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3423712253570557\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3768911361694336\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3550755977630615\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.39921236038208\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3578531742095947\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.408212423324585\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3956544399261475\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.363384246826172\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.44575572013855\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3884847164154053\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.39713454246521\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.377332925796509\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3918092250823975\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4434432983398438\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.383716583251953\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3758931159973145\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.379350423812866\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4369304180145264\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.385636568069458\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3562099933624268\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3967108726501465\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.339357852935791\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.399445056915283\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.380990743637085\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.363095283508301\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3921258449554443\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.421034097671509\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3959498405456543\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.371196746826172\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.361280918121338\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.414062261581421\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.436412811279297\n",
      "Training accuracy 0.046875\n",
      "Training loss: 2.4020142555236816\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3741092681884766\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3936057090759277\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3753645420074463\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3407843112945557\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.348851203918457\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.371410846710205\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.405153512954712\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.383840799331665\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.37507963180542\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4317307472229004\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4210364818573\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4122326374053955\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.352240562438965\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.328775644302368\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.3582324981689453\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4027135372161865\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4105093479156494\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3768680095672607\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3962600231170654\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3784596920013428\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3540408611297607\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3787827491760254\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3938963413238525\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3811168670654297\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.362419366836548\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.418254852294922\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.386597156524658\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.416241407394409\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3629097938537598\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.4118666648864746\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3997647762298584\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3784236907958984\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.388866901397705\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3862709999084473\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4291293621063232\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3807873725891113\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3839519023895264\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.413499116897583\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.421499252319336\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.429584264755249\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.406475067138672\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.413574695587158\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.403623342514038\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.42643141746521\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4214797019958496\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3970391750335693\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4108099937438965\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.380845785140991\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3802547454833984\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.374326229095459\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.352241039276123\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.368403673171997\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.390554904937744\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4143338203430176\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4424595832824707\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4032602310180664\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4342901706695557\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4356305599212646\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3890202045440674\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.388449192047119\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3361270427703857\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.384896993637085\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3732340335845947\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.369727611541748\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4078304767608643\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3594906330108643\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4130630493164062\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3670318126678467\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3575122356414795\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4112658500671387\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3707540035247803\n",
      "Training accuracy 0.1640625\n",
      "Training loss: 2.426617383956909\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4143874645233154\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4594414234161377\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4373977184295654\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.388352870941162\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.410632848739624\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4569880962371826\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3695826530456543\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.344050168991089\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3847808837890625\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4038469791412354\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3810596466064453\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.397930860519409\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4333553314208984\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4283242225646973\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3979690074920654\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.361222505569458\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.4181230068206787\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4280662536621094\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.3898940086364746\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.38788104057312\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3517777919769287\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4087090492248535\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3831825256347656\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4367165565490723\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.374746084213257\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4250869750976562\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4266622066497803\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3590242862701416\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4231510162353516\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.397453784942627\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.378391981124878\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3762378692626953\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3306891918182373\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.408168077468872\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4415385723114014\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4000139236450195\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4157209396362305\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.391730546951294\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4047627449035645\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.370203733444214\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3564653396606445\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3778164386749268\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.35256290435791\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.307784080505371\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.3751986026763916\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3741519451141357\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.375913381576538\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.35652494430542\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3857996463775635\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3595130443573\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.395193338394165\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.379516363143921\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.39481258392334\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4252889156341553\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4073739051818848\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3765363693237305\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.352147102355957\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.458726644515991\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.413456439971924\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4028587341308594\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4352526664733887\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.348252296447754\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.407916784286499\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3772494792938232\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3793327808380127\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.364917278289795\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4246034622192383\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4200379848480225\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.420604944229126\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3866608142852783\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.39141845703125\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.423701286315918\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.385869264602661\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.424471378326416\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4295477867126465\n",
      "Training accuracy 0.109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/36 [03:57<2:18:40, 237.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041442647576\n",
      "Training loss: 2.402101516723633\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3867921829223633\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.400524616241455\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3843066692352295\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.458031415939331\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.382154703140259\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3665246963500977\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.414022207260132\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.354626417160034\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4218506813049316\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3988699913024902\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3937790393829346\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3834228515625\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4007279872894287\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.397016763687134\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.404574394226074\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.354872226715088\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.356839418411255\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.361898183822632\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3849618434906006\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4414279460906982\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4168860912323\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3814780712127686\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.383631467819214\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.402256488800049\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.403691053390503\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.376546859741211\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4072346687316895\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4399304389953613\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3822317123413086\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.400556802749634\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4050498008728027\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.416647434234619\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3757362365722656\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4429757595062256\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.395904302597046\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4335947036743164\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.393716812133789\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3329834938049316\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.389026641845703\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.41763973236084\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.374211311340332\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3880250453948975\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3940963745117188\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3675460815429688\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.405264139175415\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.393345594406128\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3489603996276855\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3547778129577637\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.401737689971924\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3907175064086914\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.400055170059204\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3853089809417725\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4410293102264404\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3947927951812744\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.376943349838257\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3690218925476074\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3859331607818604\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3988394737243652\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3911232948303223\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4196910858154297\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3791120052337646\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4050214290618896\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.369926929473877\n",
      "Training accuracy 0.15625\n",
      "Training loss: 2.4037251472473145\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.37355899810791\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4011871814727783\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3860201835632324\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3864638805389404\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.441168785095215\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4021787643432617\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.410625457763672\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.366382598876953\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.383742570877075\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3981893062591553\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.384263277053833\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4115169048309326\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3852620124816895\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.381631374359131\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3812882900238037\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.404843807220459\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3894033432006836\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.382136583328247\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.422220230102539\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.393017053604126\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.380176544189453\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.396040916442871\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.360711097717285\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.432816982269287\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.353771209716797\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.316476345062256\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4040985107421875\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4153525829315186\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3421807289123535\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.396162748336792\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.427070140838623\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3941376209259033\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3906736373901367\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.41799259185791\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4217991828918457\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3307011127471924\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3386759757995605\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3770947456359863\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3573524951934814\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3968231678009033\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.424683094024658\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3943393230438232\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.389371156692505\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3813319206237793\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.432842254638672\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3831629753112793\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.366698741912842\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.35707426071167\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.411952257156372\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.446085214614868\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.399721145629883\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3881986141204834\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3873040676116943\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.403614044189453\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.437202215194702\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4006776809692383\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.41304612159729\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.387606382369995\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4175572395324707\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.374023199081421\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.375382423400879\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3761281967163086\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.408964157104492\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4103147983551025\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.381614923477173\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3892014026641846\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.380575180053711\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4104630947113037\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4044814109802246\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3313307762145996\n",
      "Training accuracy 0.15625\n",
      "Training loss: 2.363809585571289\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4083497524261475\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3804049491882324\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.434595823287964\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4269630908966064\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.406167984008789\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4179482460021973\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.340075731277466\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.38026762008667\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.398892879486084\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.367927074432373\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.349944829940796\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.409010648727417\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.381316661834717\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.38537335395813\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.387425184249878\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3788938522338867\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3991007804870605\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.343966484069824\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3584866523742676\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.436814308166504\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.335019826889038\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3829269409179688\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3966712951660156\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3804683685302734\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.424496650695801\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.407001495361328\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4197607040405273\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.382091999053955\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3874688148498535\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3816192150115967\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3526599407196045\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3995842933654785\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.41426420211792\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4157674312591553\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4075019359588623\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3974709510803223\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.371413230895996\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.36301589012146\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4092931747436523\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.403259754180908\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.388784408569336\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.374673843383789\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.375112295150757\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.394927978515625\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4260270595550537\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4167022705078125\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.427358388900757\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.332460403442383\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3763723373413086\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.375227451324463\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.386082172393799\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.418135166168213\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3347182273864746\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4087963104248047\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3339576721191406\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.392594814300537\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3819832801818848\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.417656183242798\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3396966457366943\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.449786901473999\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3993942737579346\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.409886121749878\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.368029832839966\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.378552198410034\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.42008638381958\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.358159065246582\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3846514225006104\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3717095851898193\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4694862365722656\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.397195339202881\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3584437370300293\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3604602813720703\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4153451919555664\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.397141456604004\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3495686054229736\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.386975049972534\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.429180383682251\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4291493892669678\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4121665954589844\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3708689212799072\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.375324249267578\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3833730220794678\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4333531856536865\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3651764392852783\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.399683952331543\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4007387161254883\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.423818588256836\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.460886001586914\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.440640687942505\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3990771770477295\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4067468643188477\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.372758626937866\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.363926887512207\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4270739555358887\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.43125581741333\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.334726333618164\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.407951593399048\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3838841915130615\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.410426378250122\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4001948833465576\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.413173198699951\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4060890674591064\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3545384407043457\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3763108253479004\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.363684892654419\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.375971555709839\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4035747051239014\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3679556846618652\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.415161609649658\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3755741119384766\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.387857437133789\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.379439353942871\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.404125690460205\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3349335193634033\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.372513771057129\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.369680881500244\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4214539527893066\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4079766273498535\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3448357582092285\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4000394344329834\n",
      "Training accuracy 0.12109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2/36 [07:54<2:14:31, 237.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041163250804\n",
      "Training loss: 2.3944671154022217\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3821804523468018\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4226279258728027\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3808064460754395\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4046623706817627\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.416703224182129\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.427029609680176\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3776116371154785\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.341310977935791\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.390014410018921\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3886547088623047\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.371968984603882\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4038736820220947\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.371154308319092\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.416314125061035\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4120311737060547\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3985655307769775\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.423895835876465\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.414458990097046\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.416158676147461\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4141180515289307\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.376739263534546\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4001262187957764\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3777644634246826\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.395714282989502\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4381017684936523\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.428417444229126\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.374368667602539\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3973960876464844\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3602709770202637\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3697283267974854\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.368210792541504\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3787777423858643\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.386640787124634\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.419966220855713\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4007763862609863\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4266772270202637\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3957576751708984\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.373678684234619\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.398996591567993\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.412036418914795\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.426548719406128\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4065184593200684\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.391766309738159\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.396535873413086\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4262630939483643\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.396851062774658\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.403981924057007\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3928346633911133\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4106645584106445\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4117443561553955\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3716933727264404\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.381035804748535\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3986976146698\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.378227710723877\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.398082733154297\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3953237533569336\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4439785480499268\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.376786708831787\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.352292060852051\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4108469486236572\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3768153190612793\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4183032512664795\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.371401309967041\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3922202587127686\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.378253221511841\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.343933343887329\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.372138500213623\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.356813669204712\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3822619915008545\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3902554512023926\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3866031169891357\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4181933403015137\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4068901538848877\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3640995025634766\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4180281162261963\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3605990409851074\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3491275310516357\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.41754150390625\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3672726154327393\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.364466905593872\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.351806402206421\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4244117736816406\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3477799892425537\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.435901641845703\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.355313777923584\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.388364553451538\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4441723823547363\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.33516788482666\n",
      "Training accuracy 0.15625\n",
      "Training loss: 2.403898239135742\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.391291618347168\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3696208000183105\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.374709129333496\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3853981494903564\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3910622596740723\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4154984951019287\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.376537561416626\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4076364040374756\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.402954578399658\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.433640956878662\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.434619903564453\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.40948486328125\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.430236577987671\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.385061025619507\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.418689012527466\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.446601152420044\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4011588096618652\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3804097175598145\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3976407051086426\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.410527467727661\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3943870067596436\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3569395542144775\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3826608657836914\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3828094005584717\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4135828018188477\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.403986930847168\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4018497467041016\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3752336502075195\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3821845054626465\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3478710651397705\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.369147777557373\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.333076000213623\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.415371894836426\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3997325897216797\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4097235202789307\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.396106481552124\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.399585485458374\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3387184143066406\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3870978355407715\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.384800434112549\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.379551410675049\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4405672550201416\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3793346881866455\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.374438762664795\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3890864849090576\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4076740741729736\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.404928207397461\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.367098331451416\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.376016616821289\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.423261880874634\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.370786666870117\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.430696487426758\n",
      "Training accuracy 0.05078125\n",
      "Training loss: 2.386941909790039\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3987367153167725\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3799221515655518\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.39898943901062\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3978421688079834\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4064674377441406\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.356062173843384\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4175515174865723\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3677735328674316\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3698601722717285\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4048359394073486\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3940887451171875\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3438215255737305\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4056410789489746\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.398721218109131\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.416048288345337\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.349567413330078\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.392198085784912\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.454003095626831\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3825132846832275\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.394516706466675\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4175243377685547\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4097180366516113\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3690216541290283\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.398747444152832\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4352664947509766\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4201745986938477\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4001312255859375\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.401625156402588\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3739638328552246\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.384643077850342\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4079113006591797\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.359950065612793\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4129977226257324\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.390423536300659\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4000613689422607\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4253451824188232\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3696377277374268\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.394803285598755\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3396530151367188\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.388188123703003\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3482654094696045\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4202678203582764\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.39377760887146\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3431012630462646\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.38919734954834\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.373899459838867\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3605854511260986\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.366299629211426\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4254093170166016\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.410273313522339\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4266741275787354\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.397469997406006\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.382910966873169\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3875341415405273\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3628342151641846\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.414045572280884\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3748297691345215\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.37914776802063\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3583779335021973\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4038751125335693\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4016361236572266\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.417895555496216\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.386831760406494\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.41064715385437\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3760392665863037\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3838610649108887\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.385664701461792\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.405097007751465\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3909401893615723\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.449924945831299\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.389864206314087\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4285061359405518\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3956825733184814\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4091241359710693\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4024980068206787\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3944199085235596\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.43755841255188\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3675734996795654\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3976759910583496\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.369347095489502\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4181532859802246\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3763442039489746\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.441498041152954\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.350315570831299\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3935091495513916\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.368898630142212\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.346832036972046\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.377683401107788\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3998258113861084\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4131953716278076\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.414048671722412\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.37793231010437\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3448665142059326\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3656527996063232\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3585968017578125\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3842403888702393\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3748393058776855\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.411386251449585\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4289114475250244\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4239695072174072\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3616104125976562\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3932223320007324\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3461763858795166\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.397660970687866\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.373805284500122\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3766396045684814\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.353994607925415\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4121201038360596\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3775784969329834\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3805227279663086\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.360053300857544\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.378420114517212\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3705193996429443\n",
      "Training accuracy 0.09765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/36 [11:47<2:09:49, 236.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.412904117256403\n",
      "Training loss: 2.3954379558563232\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.43498158454895\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.432666778564453\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.374749183654785\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4262213706970215\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.403332233428955\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3980507850646973\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.366955518722534\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3691229820251465\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.431201934814453\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3775107860565186\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4093503952026367\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3916101455688477\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3799424171447754\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3967812061309814\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.400538921356201\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3640332221984863\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.399275302886963\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.434605360031128\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.401057243347168\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.378995180130005\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3967795372009277\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.438267230987549\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3712663650512695\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.374047040939331\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3585281372070312\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.402888774871826\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4237592220306396\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4395251274108887\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3622264862060547\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.417189598083496\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.429706335067749\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3831958770751953\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4069149494171143\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3516154289245605\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.439720869064331\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4177322387695312\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.315136671066284\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3932299613952637\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3534693717956543\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4261133670806885\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.394819498062134\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.439051389694214\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4255387783050537\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3600423336029053\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.428719997406006\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.386908531188965\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.406567335128784\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3865928649902344\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3572449684143066\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3497960567474365\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.378293752670288\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3703062534332275\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3600611686706543\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4115777015686035\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3783531188964844\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4199893474578857\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.396090507507324\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3853442668914795\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.415950298309326\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3829784393310547\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.341371774673462\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4011032581329346\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3921873569488525\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.395636796951294\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.408567190170288\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.387042760848999\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4159021377563477\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.354398012161255\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3953380584716797\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3346304893493652\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4153549671173096\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4064924716949463\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.405869483947754\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.421478033065796\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4059503078460693\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3916573524475098\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3876259326934814\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.432696580886841\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.391091823577881\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4434711933135986\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.439530849456787\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3524487018585205\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3920974731445312\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.372852087020874\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3422014713287354\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3881425857543945\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3892886638641357\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4112837314605713\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.399750232696533\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4156508445739746\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3659956455230713\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3943583965301514\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3984220027923584\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4004409313201904\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.375579357147217\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4225502014160156\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3538739681243896\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3555283546447754\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3859517574310303\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3216657638549805\n",
      "Training accuracy 0.16015625\n",
      "Training loss: 2.355083703994751\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3699886798858643\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3683602809906006\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.466571092605591\n",
      "Training accuracy 0.04296875\n",
      "Training loss: 2.361515522003174\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.395850896835327\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.389991521835327\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.375264883041382\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4081828594207764\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.355715274810791\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.442056894302368\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.406517505645752\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.406162977218628\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3635284900665283\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.386807441711426\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4050204753875732\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.383629322052002\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.366976737976074\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.386974334716797\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3723385334014893\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.39992356300354\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.390592336654663\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.397265672683716\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3819048404693604\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3860676288604736\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4441184997558594\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3462252616882324\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.416126012802124\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4119765758514404\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.386720657348633\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3801283836364746\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.400136947631836\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.399224281311035\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.393613815307617\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3906002044677734\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.441084384918213\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3861026763916016\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3899779319763184\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.423657178878784\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.372837543487549\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.34812331199646\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3640716075897217\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.39923357963562\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4095802307128906\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.383233070373535\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.432309627532959\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3771679401397705\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.397939920425415\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.396780252456665\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.405402183532715\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4467129707336426\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.3898401260375977\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3986377716064453\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.388097047805786\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3889665603637695\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.425283432006836\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.304569721221924\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3544421195983887\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.407191276550293\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.385565996170044\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.39201021194458\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3238441944122314\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3344428539276123\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4060258865356445\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3815977573394775\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4065022468566895\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3788046836853027\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4237632751464844\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3443493843078613\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3928723335266113\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4106318950653076\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4089062213897705\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4497921466827393\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3951566219329834\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3965866565704346\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4260413646698\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4035403728485107\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3607616424560547\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.370471477508545\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3428847789764404\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4443225860595703\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.395414113998413\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3791866302490234\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3995344638824463\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.42353892326355\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.4289515018463135\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4324991703033447\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3586549758911133\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3875908851623535\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.415100336074829\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3819804191589355\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.367849826812744\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.389186382293701\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.411196708679199\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.374765157699585\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3913981914520264\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3865015506744385\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.38889217376709\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.421111583709717\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3690967559814453\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3818812370300293\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.377592086791992\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3878700733184814\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3908488750457764\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.326747179031372\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.420524835586548\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.378943920135498\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3330445289611816\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3563530445098877\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.370105743408203\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3571455478668213\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.395770788192749\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.368440628051758\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.343996286392212\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4342422485351562\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.422556161880493\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4213578701019287\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.457662582397461\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.415036678314209\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.339890241622925\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.381535053253174\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3885879516601562\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.411044120788574\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3935816287994385\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4015674591064453\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.393840789794922\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4124786853790283\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3959591388702393\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4088408946990967\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.412951707839966\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.382256031036377\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.408825635910034\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.454117774963379\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.40458607673645\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.376498222351074\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3509438037872314\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3684780597686768\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4343485832214355\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3958051204681396\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.344146251678467\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.406846046447754\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4181363582611084\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3750534057617188\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.356665849685669\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.386019229888916\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3776328563690186\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.353245258331299\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.405648708343506\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.332926034927368\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.366267204284668\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4037909507751465\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3803844451904297\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.374293327331543\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3751044273376465\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4156908988952637\n",
      "Training accuracy 0.1015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 4/36 [15:40<2:05:22, 235.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129040958359838\n",
      "Training loss: 2.353107213973999\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.417139768600464\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4098081588745117\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.394566774368286\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.432612657546997\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3757143020629883\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.395479440689087\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.395639419555664\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3699774742126465\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4092941284179688\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.404554843902588\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3979010581970215\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3861217498779297\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4112935066223145\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3923356533050537\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.365516185760498\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4087557792663574\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3851890563964844\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.385970115661621\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3808202743530273\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.364553213119507\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4068689346313477\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3382861614227295\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.3886265754699707\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3957126140594482\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3583550453186035\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3636581897735596\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4048287868499756\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.392011880874634\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.364112377166748\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3348851203918457\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.42183780670166\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3625309467315674\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.365078926086426\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3973228931427\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.381965398788452\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3620083332061768\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.394976854324341\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3896443843841553\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4270846843719482\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3823251724243164\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.404892921447754\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4266517162323\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.381186008453369\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3905282020568848\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3824942111968994\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.391038656234741\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4092319011688232\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4137096405029297\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.367734432220459\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.392411708831787\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.394728899002075\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3588125705718994\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4033446311950684\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3925459384918213\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4434633255004883\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3743765354156494\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.389353036880493\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.412977695465088\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4305362701416016\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.372617483139038\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3637986183166504\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.398592948913574\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3654327392578125\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4055986404418945\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4321131706237793\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.419337034225464\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.40173602104187\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3392677307128906\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.413302421569824\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3916664123535156\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.360588788986206\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3738648891448975\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3717596530914307\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.412151336669922\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.394956111907959\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.395371675491333\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.382293224334717\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3802342414855957\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.373199462890625\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.420475721359253\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3896899223327637\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4088780879974365\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.444596290588379\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4442224502563477\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.339820623397827\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4222211837768555\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.381300926208496\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.364238977432251\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3927266597747803\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.354081630706787\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.399846076965332\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4363551139831543\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3876912593841553\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4280335903167725\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3978190422058105\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.381382703781128\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.399569511413574\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3745481967926025\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4083235263824463\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4116947650909424\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.42152738571167\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.413257598876953\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4280035495758057\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.379249095916748\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3951101303100586\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3841300010681152\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.426142930984497\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.404346227645874\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.395109176635742\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4080729484558105\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3882339000701904\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3811230659484863\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3807592391967773\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.390402317047119\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3849706649780273\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3947484493255615\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.3669562339782715\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.416775941848755\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4125444889068604\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.385389566421509\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3394196033477783\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3825948238372803\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3659322261810303\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4193551540374756\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.381986618041992\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.424484968185425\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3547909259796143\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3828630447387695\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3518292903900146\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3939719200134277\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3930916786193848\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3855056762695312\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3941752910614014\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.447645664215088\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.383427619934082\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3824851512908936\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3499631881713867\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.3752334117889404\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.383788824081421\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.353724241256714\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.418626308441162\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3977222442626953\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.37009859085083\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3408989906311035\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.393734931945801\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3678832054138184\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3833999633789062\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4081451892852783\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.360799789428711\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4076528549194336\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.362506151199341\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.393810987472534\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4002673625946045\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3646175861358643\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.365866184234619\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3937177658081055\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.401655435562134\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.451871156692505\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.333552598953247\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.4089808464050293\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.389723300933838\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4345974922180176\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.381026268005371\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.354566812515259\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.396235466003418\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3943376541137695\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3641483783721924\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.409257650375366\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4149365425109863\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4174413681030273\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3839478492736816\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.408748149871826\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4266815185546875\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3932812213897705\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.421405076980591\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4334890842437744\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4191954135894775\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4079034328460693\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4005370140075684\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4303746223449707\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.33223032951355\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4263088703155518\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3931689262390137\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3461761474609375\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3715996742248535\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3999392986297607\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4155092239379883\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3627049922943115\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.436872959136963\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3525805473327637\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3624067306518555\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.354459047317505\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.399263620376587\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.412313938140869\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.418137311935425\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.415004014968872\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3878681659698486\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3999459743499756\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4238250255584717\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3689889907836914\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.4205920696258545\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3958919048309326\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4161057472229004\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3813507556915283\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.438262462615967\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4261789321899414\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3676257133483887\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.397442102432251\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.416757106781006\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.372102975845337\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.384166717529297\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3827667236328125\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4210524559020996\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3717591762542725\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4201226234436035\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.399763822555542\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.382822036743164\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.383963108062744\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3872597217559814\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.38814640045166\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3752927780151367\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4200432300567627\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3805432319641113\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3562960624694824\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.427074670791626\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.392073154449463\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.398465156555176\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4129865169525146\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4277737140655518\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3425326347351074\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4114298820495605\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.38539981842041\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.380587577819824\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4005067348480225\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.387253761291504\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.372652292251587\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.375671863555908\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.373136043548584\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3633830547332764\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3832435607910156\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3897545337677\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.358468770980835\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4305672645568848\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4243686199188232\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.358396053314209\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3869457244873047\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.382021188735962\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3790040016174316\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.395766019821167\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.360466957092285\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.385511636734009\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.405048370361328\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.378697395324707\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.397197723388672\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3723902702331543\n",
      "Training accuracy 0.1171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 5/36 [19:36<2:01:44, 235.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.412904088385403\n",
      "Training loss: 2.3795058727264404\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.394928455352783\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3909716606140137\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.367161512374878\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3982529640197754\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.348865270614624\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.39925479888916\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3928403854370117\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4032251834869385\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.448211908340454\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4113881587982178\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3674814701080322\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.445173740386963\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.35119366645813\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.402186632156372\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.38383150100708\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4091360569000244\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3767027854919434\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3840155601501465\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3607614040374756\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.397676944732666\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.375518798828125\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3662257194519043\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4183952808380127\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3912107944488525\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.372999668121338\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3828814029693604\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.36747407913208\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3812708854675293\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3890480995178223\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.425184965133667\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.373492479324341\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.371854305267334\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.405471086502075\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3771681785583496\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4033145904541016\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.380230665206909\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.359609842300415\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.373936414718628\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4006447792053223\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3930487632751465\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.374847412109375\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.389815330505371\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.370561122894287\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4290757179260254\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.35446834564209\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4125709533691406\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.38769268989563\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.436864137649536\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.357318162918091\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4146311283111572\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4142558574676514\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.40903902053833\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3865060806274414\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.34645676612854\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3737123012542725\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4020299911499023\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.381826400756836\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3785626888275146\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.367692708969116\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.401109457015991\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3712007999420166\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3658530712127686\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.439858913421631\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.42328143119812\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.421287775039673\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3992793560028076\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.413712501525879\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.415980577468872\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.368258237838745\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3523380756378174\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.362471580505371\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.438882827758789\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3829963207244873\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4124059677124023\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3831443786621094\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.398501396179199\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.379230499267578\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.415663003921509\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.378436803817749\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3626794815063477\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.2981154918670654\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4035205841064453\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4195122718811035\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4068241119384766\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3851590156555176\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3691084384918213\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4328806400299072\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4055986404418945\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.397829294204712\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.347454786300659\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.418220043182373\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3987674713134766\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4265763759613037\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3905069828033447\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3650424480438232\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.407029628753662\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3537137508392334\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.408571243286133\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3953304290771484\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3720829486846924\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3968617916107178\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.396824598312378\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.394460916519165\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.402944326400757\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.393407106399536\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.42883038520813\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3923521041870117\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.336712598800659\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3544132709503174\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3955368995666504\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.38704514503479\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.382823944091797\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.363114833831787\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.413780450820923\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4163389205932617\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3563809394836426\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3903732299804688\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4337821006774902\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.330734968185425\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3720786571502686\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.398648500442505\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.371870994567871\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3412911891937256\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3551464080810547\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3728301525115967\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.36276912689209\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3285837173461914\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.370656728744507\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3871941566467285\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.397794246673584\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.386749505996704\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3750555515289307\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.384526014328003\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4218790531158447\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.351518392562866\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3825902938842773\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3740692138671875\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3910956382751465\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3252668380737305\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.4023795127868652\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.382507801055908\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3839240074157715\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4359726905822754\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3819971084594727\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3728322982788086\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.435563564300537\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3696610927581787\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4107871055603027\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.404099225997925\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3798649311065674\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.427159070968628\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.388902425765991\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3981873989105225\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3695068359375\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.394137382507324\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4477851390838623\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.428213357925415\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.391223430633545\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4194657802581787\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3984763622283936\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3643906116485596\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.418415069580078\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3766820430755615\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3832292556762695\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.41329026222229\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3628437519073486\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.390782117843628\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3951380252838135\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.369859218597412\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4369146823883057\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3412270545959473\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3875203132629395\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4846270084381104\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.388036012649536\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3532516956329346\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.366887092590332\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3643503189086914\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.400780439376831\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3923726081848145\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4010887145996094\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.417695999145508\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3472495079040527\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3998594284057617\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4299819469451904\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4363911151885986\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.4027156829833984\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3844447135925293\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.404498338699341\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.40082049369812\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3850948810577393\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4144227504730225\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4100728034973145\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.390408754348755\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3412961959838867\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3697926998138428\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3978142738342285\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.420443058013916\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.324099540710449\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.4148614406585693\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.375704288482666\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4126100540161133\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.436771869659424\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4198100566864014\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3792309761047363\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3893473148345947\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4150500297546387\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4141924381256104\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3991222381591797\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3488755226135254\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.34458065032959\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.33760929107666\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.423251152038574\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4478600025177\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.408973455429077\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.357438564300537\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3653218746185303\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4058263301849365\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.361523389816284\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3615734577178955\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4094247817993164\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.366962432861328\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3998022079467773\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.401318073272705\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.401421070098877\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4489049911499023\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4012022018432617\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4067351818084717\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4038591384887695\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.361269474029541\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.440096616744995\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.4017081260681152\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4484286308288574\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.366931200027466\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.361544370651245\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4006893634796143\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.391247034072876\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4163434505462646\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.434689521789551\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4491395950317383\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.396920680999756\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.360323667526245\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.402189016342163\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.364772319793701\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.371375799179077\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.418766498565674\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4011552333831787\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4235100746154785\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.391573667526245\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4091901779174805\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.441038131713867\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.385324716567993\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.402191162109375\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4076175689697266\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.394063711166382\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.345925807952881\n",
      "Training accuracy 0.13671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 6/36 [23:32<1:57:45, 235.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041247069836\n",
      "Training loss: 2.4033825397491455\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.411324977874756\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.367640733718872\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.379533290863037\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3256633281707764\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3924918174743652\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4306113719940186\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4081599712371826\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.362910032272339\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3679518699645996\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.406468152999878\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4295032024383545\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4003496170043945\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.372969150543213\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.369086503982544\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4129841327667236\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3689517974853516\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.40920352935791\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.364640951156616\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3738515377044678\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4564247131347656\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.400585412979126\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3932454586029053\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4638164043426514\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3745315074920654\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3903586864471436\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3796920776367188\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4228644371032715\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.401064157485962\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.387857675552368\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.359515905380249\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3779044151306152\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.410616636276245\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.430750846862793\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4020040035247803\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.420125722885132\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.367955207824707\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.366240978240967\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3965981006622314\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3538918495178223\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4025630950927734\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3825318813323975\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3863635063171387\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.404491424560547\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.351943254470825\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4198646545410156\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4062836170196533\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3359215259552\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.430341958999634\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3867132663726807\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.402193069458008\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3904614448547363\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3687336444854736\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.35567569732666\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4475128650665283\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.455152988433838\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.388045072555542\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4207584857940674\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3630480766296387\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.386934280395508\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.370587110519409\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.397786855697632\n",
      "Training accuracy 0.046875\n",
      "Training loss: 2.425699234008789\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3970489501953125\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3472723960876465\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.396433115005493\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3610849380493164\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3839519023895264\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4205172061920166\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.399789333343506\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4147605895996094\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4357950687408447\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.383758544921875\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4029006958007812\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.371156692504883\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.365659236907959\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.367635726928711\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.383606195449829\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4335861206054688\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4079978466033936\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.406778573989868\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3692383766174316\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.390955924987793\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3744330406188965\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3978679180145264\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.388256788253784\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.393843173980713\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3550519943237305\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4137537479400635\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3548378944396973\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.396843194961548\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3874363899230957\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3929035663604736\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4295692443847656\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4010872840881348\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3590548038482666\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.392725706100464\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.393766164779663\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.393542766571045\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3598546981811523\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.419341564178467\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.403351068496704\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.402698516845703\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3768718242645264\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.407867908477783\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3756065368652344\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.347064256668091\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.389455795288086\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3452186584472656\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.408181667327881\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4000134468078613\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.398550033569336\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3753576278686523\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4086453914642334\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.363949775695801\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3961610794067383\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3885605335235596\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3654747009277344\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3866021633148193\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3400521278381348\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4295568466186523\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4290261268615723\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.383253335952759\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4261066913604736\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.386040449142456\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3769371509552\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3928277492523193\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3976337909698486\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.352942943572998\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4149386882781982\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.339766025543213\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3700926303863525\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.379725456237793\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3717310428619385\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.386435031890869\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3713667392730713\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.42354154586792\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3950095176696777\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3984906673431396\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.359069347381592\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.390866994857788\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4175491333007812\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.359349012374878\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4009203910827637\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.396223306655884\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3955254554748535\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3969264030456543\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3721272945404053\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.432827949523926\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3674418926239014\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.398747205734253\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.395026445388794\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3774099349975586\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.39667010307312\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.421633720397949\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.4212214946746826\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3758633136749268\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.356502056121826\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.428800582885742\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.344180107116699\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.401334762573242\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4067258834838867\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3841183185577393\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.401320219039917\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.431891918182373\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4346179962158203\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.372690200805664\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.405069589614868\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.350304365158081\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3815295696258545\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3737833499908447\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3959784507751465\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.391418695449829\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3796396255493164\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.41727876663208\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4385886192321777\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.385845899581909\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4140827655792236\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4015183448791504\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.371532440185547\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.399768114089966\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.355738639831543\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4424304962158203\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4100029468536377\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3913991451263428\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.377765417098999\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3637218475341797\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.375288963317871\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.329101324081421\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.4033541679382324\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3844316005706787\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3753275871276855\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.365262269973755\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4056859016418457\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4619693756103516\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3961737155914307\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3583145141601562\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4393577575683594\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3659980297088623\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.425708293914795\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.364274263381958\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.383730173110962\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.394021511077881\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.43208909034729\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.404355525970459\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.356025218963623\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3950562477111816\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3320581912994385\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.350942373275757\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3711323738098145\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3746957778930664\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.435415506362915\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3631443977355957\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.376936435699463\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.412290096282959\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3404459953308105\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4184982776641846\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4102513790130615\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4218032360076904\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4207875728607178\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.4205899238586426\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.398144245147705\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3693370819091797\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.391813278198242\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3626160621643066\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.385270118713379\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.394002914428711\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3819572925567627\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.386823892593384\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4008140563964844\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.395235776901245\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.368445873260498\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.44514536857605\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4233109951019287\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3801989555358887\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.3784029483795166\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.375718832015991\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.388274669647217\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4137930870056152\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4221906661987305\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3595259189605713\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.375342607498169\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.387073040008545\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3996732234954834\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3405842781066895\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.400536060333252\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3986263275146484\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4249730110168457\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.366720676422119\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3962886333465576\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.394843816757202\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.390730857849121\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.38364315032959\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.464132070541382\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3672564029693604\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3672800064086914\n",
      "Training accuracy 0.09765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 7/36 [27:27<1:53:45, 235.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041181877255\n",
      "Training loss: 2.325066566467285\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.395253896713257\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3915114402770996\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4324748516082764\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4150969982147217\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3998029232025146\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4117560386657715\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3782777786254883\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3730368614196777\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.409128427505493\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4310638904571533\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.410374164581299\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4128477573394775\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4062395095825195\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.415412187576294\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.409271001815796\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4220101833343506\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3554022312164307\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.375948190689087\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4084653854370117\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3978426456451416\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.388770341873169\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.37994122505188\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.311387777328491\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.3495469093322754\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.383801221847534\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3764660358428955\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3982975482940674\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4291765689849854\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3783373832702637\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3940303325653076\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.400144577026367\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.374870777130127\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3625409603118896\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.379689931869507\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4057021141052246\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4177467823028564\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.408818244934082\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.376351833343506\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.387099266052246\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.434536933898926\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3555617332458496\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.340794086456299\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.410834312438965\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.373955011367798\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3787033557891846\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.36788272857666\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4294486045837402\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3937199115753174\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3700151443481445\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3924756050109863\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3833565711975098\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4062464237213135\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4329967498779297\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3990187644958496\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.418956756591797\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3708598613739014\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3925695419311523\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.349945306777954\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3547544479370117\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.409677505493164\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4200944900512695\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3564934730529785\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.358381748199463\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3840115070343018\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.38985013961792\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.411118745803833\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.401642084121704\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.358487606048584\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3850691318511963\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.413745641708374\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3892898559570312\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.393454074859619\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.336757183074951\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.38311767578125\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.365522861480713\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.353635311126709\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3474550247192383\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.372004747390747\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4515411853790283\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3960800170898438\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4455904960632324\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.36348819732666\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.374934196472168\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4153871536254883\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3852739334106445\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3762917518615723\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.38326358795166\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.366598129272461\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.392179250717163\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.420077085494995\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4028446674346924\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.393535852432251\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4060556888580322\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.372805595397949\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.350337266921997\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3675923347473145\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.387457847595215\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3871712684631348\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3831417560577393\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4032387733459473\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.40883469581604\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3784091472625732\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4470484256744385\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.389914035797119\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4406020641326904\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4342541694641113\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3654727935791016\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4262030124664307\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.428589344024658\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3766753673553467\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4283061027526855\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.360874891281128\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4401772022247314\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4490506649017334\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.420469284057617\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.365818977355957\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.326491355895996\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3634376525878906\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3765597343444824\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4098849296569824\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4010629653930664\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3731918334960938\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4407479763031006\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.397498369216919\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3926146030426025\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3510868549346924\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3644111156463623\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.339797019958496\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4030654430389404\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3736791610717773\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.376418352127075\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4069278240203857\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.369474411010742\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3981499671936035\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3907365798950195\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3664731979370117\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3686468601226807\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4010190963745117\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3519580364227295\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3997750282287598\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.401613235473633\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.407090425491333\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.376007556915283\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3772566318511963\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3915786743164062\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4092135429382324\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3947925567626953\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4200527667999268\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3437347412109375\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3651013374328613\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4130702018737793\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4487736225128174\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.364840507507324\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.376371145248413\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3991949558258057\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4094576835632324\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.401142120361328\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3999783992767334\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3687758445739746\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3560566902160645\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.390392303466797\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.427730083465576\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.4240033626556396\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.416029453277588\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4501686096191406\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.381526470184326\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.386697769165039\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.399740695953369\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3486764430999756\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.413961887359619\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4092226028442383\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.364795446395874\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3928894996643066\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.371975898742676\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4253010749816895\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.436384439468384\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.402771472930908\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.393681764602661\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3997621536254883\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.400747776031494\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.355556011199951\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3808867931365967\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4262871742248535\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3789620399475098\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.430743932723999\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.38197922706604\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3978452682495117\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.376377820968628\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3580727577209473\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.38974928855896\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3656601905822754\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4319071769714355\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3989248275756836\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3644750118255615\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4075324535369873\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.402059555053711\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.397731065750122\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.441450595855713\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3474133014678955\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.355311632156372\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.381056308746338\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3407835960388184\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.385868549346924\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.419039011001587\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3926632404327393\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.399768352508545\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3952910900115967\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3962173461914062\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.42792010307312\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.41506028175354\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3978021144866943\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4002561569213867\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.392650604248047\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3903748989105225\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.369702100753784\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.339916229248047\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4105751514434814\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.41782546043396\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4542627334594727\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.392113447189331\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.412363290786743\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3993091583251953\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.434951066970825\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.38218355178833\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3693461418151855\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4570789337158203\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4007773399353027\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3803422451019287\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3812434673309326\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.384986639022827\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.413128137588501\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3962275981903076\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3793721199035645\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.358548641204834\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.37491512298584\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.344355344772339\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3735241889953613\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.392159938812256\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3687613010406494\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4026434421539307\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.370568037033081\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4034059047698975\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.376295328140259\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4073522090911865\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.379269599914551\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3479623794555664\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3798365592956543\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.379030227661133\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4066452980041504\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3915183544158936\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.379728317260742\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.392592668533325\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4043445587158203\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4293372631073\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.426731824874878\n",
      "Training accuracy 0.09375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 8/36 [31:23<1:49:57, 235.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041079431772\n",
      "Training loss: 2.397141218185425\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3777272701263428\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4021568298339844\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.42167592048645\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4218904972076416\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.37416934967041\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3913283348083496\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.369051456451416\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3968050479888916\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3804969787597656\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.429932117462158\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4510438442230225\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.39684796333313\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3968122005462646\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.383843421936035\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4000282287597656\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.375920534133911\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3843069076538086\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.405895709991455\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.424130439758301\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3733129501342773\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3653814792633057\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.410585880279541\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4016001224517822\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.414804458618164\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.370823860168457\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4021952152252197\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4077866077423096\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3832125663757324\n",
      "Training accuracy 0.15625\n",
      "Training loss: 2.3839612007141113\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.402679920196533\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.406728506088257\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4182677268981934\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.422930955886841\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4201583862304688\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3960912227630615\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3784141540527344\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.404702663421631\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.382995128631592\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4039666652679443\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3620805740356445\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3893418312072754\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.342522621154785\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3577303886413574\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3945231437683105\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.37542462348938\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3980207443237305\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.410811424255371\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4030532836914062\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.389009475708008\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.420233964920044\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.403106451034546\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4034082889556885\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.391934394836426\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.451873540878296\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4235737323760986\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3966481685638428\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3793535232543945\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.348909378051758\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.345296859741211\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3881359100341797\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4241156578063965\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3915321826934814\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.416921615600586\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3846662044525146\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3929762840270996\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.387943744659424\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3992807865142822\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3556430339813232\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.3707363605499268\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4018921852111816\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.409573793411255\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.411426544189453\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.394340753555298\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4496004581451416\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4201948642730713\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.371124505996704\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4216883182525635\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3704254627227783\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3675594329833984\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3859944343566895\n",
      "Training accuracy 0.16015625\n",
      "Training loss: 2.3827474117279053\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.298293352127075\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3567254543304443\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3670616149902344\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.386910915374756\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.342076301574707\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4412670135498047\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.435795545578003\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.435880661010742\n",
      "Training accuracy 0.046875\n",
      "Training loss: 2.3329713344573975\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.411503791809082\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4132492542266846\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.347712993621826\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3908660411834717\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4134562015533447\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3971028327941895\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4568891525268555\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.358733892440796\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.358987331390381\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3927834033966064\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3704757690429688\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3676810264587402\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3898444175720215\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3605408668518066\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.33577823638916\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.392528533935547\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4183108806610107\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.394542694091797\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4334566593170166\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3903374671936035\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3850483894348145\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3633170127868652\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.407101631164551\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.40952730178833\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.420980453491211\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3857004642486572\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3590219020843506\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4145736694335938\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.358848810195923\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.458639621734619\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3954572677612305\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3543689250946045\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4170703887939453\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.336221218109131\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3521382808685303\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.362511396408081\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3880467414855957\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.365711212158203\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.430447578430176\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3921077251434326\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.420901298522949\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.377910852432251\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.38598370552063\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.413245439529419\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3903675079345703\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3422796726226807\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4099037647247314\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3649497032165527\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4182662963867188\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.371767997741699\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4323906898498535\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.329047679901123\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.383084535598755\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3938357830047607\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.409341812133789\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.33512020111084\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3647308349609375\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3673877716064453\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.331435441970825\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.410130023956299\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.39119553565979\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3352551460266113\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4180264472961426\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.351555347442627\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4158363342285156\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.404766082763672\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.356243371963501\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3519842624664307\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4097142219543457\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.384321928024292\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3580713272094727\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.364147663116455\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4140727519989014\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.408320903778076\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3881707191467285\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3630735874176025\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3845293521881104\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4077959060668945\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4143762588500977\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4080538749694824\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3993659019470215\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.389996290206909\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3963623046875\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.426656723022461\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3946502208709717\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.41253924369812\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3913159370422363\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.395904779434204\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.380077600479126\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3891196250915527\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3922200202941895\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3552281856536865\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3662471771240234\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3866896629333496\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.381751298904419\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4113917350769043\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4198896884918213\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.471910238265991\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.413914203643799\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.358232021331787\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3686769008636475\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3615164756774902\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3971898555755615\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.418966293334961\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4081997871398926\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.377981185913086\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3935537338256836\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.336364984512329\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4385788440704346\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3928306102752686\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3710484504699707\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4020001888275146\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.436099052429199\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.412402629852295\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.425614595413208\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.380129098892212\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3959782123565674\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.402812957763672\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.350876808166504\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.404801368713379\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3827714920043945\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.440380573272705\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4031262397766113\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3622207641601562\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.415144681930542\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3669536113739014\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.366062879562378\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3768279552459717\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3845508098602295\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.363430976867676\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3973543643951416\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3886098861694336\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4115679264068604\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3469672203063965\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4662535190582275\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.389829158782959\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3755476474761963\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3790364265441895\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4219374656677246\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3863096237182617\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3990702629089355\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.396056890487671\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3870627880096436\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3909430503845215\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3893775939941406\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.350430965423584\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.3761706352233887\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3726766109466553\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3282670974731445\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3736863136291504\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3276736736297607\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.372685432434082\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.366244077682495\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.370609998703003\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.416428327560425\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.422969102859497\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.424744129180908\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.421942710876465\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.418761968612671\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3481554985046387\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.397254467010498\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4041261672973633\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3699560165405273\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.434797525405884\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4498608112335205\n",
      "Training accuracy 0.0859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 9/36 [35:15<1:45:34, 234.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041088745\n",
      "Training loss: 2.409196376800537\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.395447015762329\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.381392240524292\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.394442081451416\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.406592607498169\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4301750659942627\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.38197922706604\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.464085817337036\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.396892547607422\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4038822650909424\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4030377864837646\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4517862796783447\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.373047113418579\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3868117332458496\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.341313600540161\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.366227865219116\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4176464080810547\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.441112995147705\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3616926670074463\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.368823289871216\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.434891939163208\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.416679620742798\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3627755641937256\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.350473165512085\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.39905047416687\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4262664318084717\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4216346740722656\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4146995544433594\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.3631553649902344\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3923354148864746\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3249764442443848\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4105844497680664\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4024806022644043\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.383435010910034\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3964715003967285\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3887782096862793\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3926703929901123\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3423891067504883\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.382991075515747\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3591654300689697\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.395171880722046\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.390842914581299\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4605860710144043\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3549931049346924\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3785860538482666\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3846945762634277\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.379063367843628\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4075374603271484\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.321880578994751\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.335378408432007\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.387026309967041\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3633105754852295\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.370753765106201\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3900227546691895\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.395801544189453\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3673038482666016\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.395129680633545\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.403146743774414\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4026753902435303\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.411649703979492\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.409637451171875\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4387454986572266\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3649179935455322\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4273769855499268\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.417323589324951\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4354870319366455\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.355562210083008\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3977582454681396\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3944694995880127\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4163057804107666\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.416613817214966\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.334972381591797\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3627212047576904\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.382948398590088\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3614561557769775\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.380781412124634\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.395921230316162\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.377068042755127\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.401489019393921\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.379639148712158\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.406048059463501\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3846323490142822\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3714849948883057\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.442922830581665\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3919506072998047\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.401137113571167\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.392719030380249\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4099323749542236\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.389876365661621\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3849172592163086\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.407989978790283\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.39353346824646\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.367671251296997\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.415158748626709\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.4059720039367676\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3726446628570557\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3609611988067627\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.383126735687256\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3906819820404053\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3786816596984863\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.446707010269165\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.42954421043396\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4098520278930664\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3486173152923584\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4261562824249268\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3750956058502197\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.459573984146118\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3620352745056152\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.399912118911743\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3795931339263916\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.404740571975708\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.407099485397339\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.399235725402832\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4253456592559814\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4199628829956055\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3987619876861572\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4076242446899414\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3939199447631836\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3719964027404785\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4122893810272217\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.397502899169922\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4181947708129883\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.395458221435547\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4261465072631836\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.354111433029175\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4039101600646973\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.455559015274048\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3790180683135986\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.376404285430908\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.380859136581421\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.380542278289795\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4037580490112305\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3966948986053467\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3900201320648193\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3685977458953857\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.375128984451294\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.326873540878296\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.3769679069519043\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4200422763824463\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3909499645233154\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.363861322402954\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4159340858459473\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4194300174713135\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3455231189727783\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3586816787719727\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4022421836853027\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.383478879928589\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.342111110687256\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3958277702331543\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3545260429382324\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.407139301300049\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.343330144882202\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4014041423797607\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3976857662200928\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.376469612121582\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4246582984924316\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3920819759368896\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.430356025695801\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3593921661376953\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.418732166290283\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.393249034881592\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3715388774871826\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3562023639678955\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3772525787353516\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3660120964050293\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.416325569152832\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3815104961395264\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.393536329269409\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3905575275421143\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4143900871276855\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3878705501556396\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4292032718658447\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.383115768432617\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3392715454101562\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.364672899246216\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.409881830215454\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.397179365158081\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.393717050552368\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3710110187530518\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4531126022338867\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.36891770362854\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.379274845123291\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.373068332672119\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3616957664489746\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3783974647521973\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.413026809692383\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.369521141052246\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.399674892425537\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3556625843048096\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.375779628753662\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3508825302124023\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.394221067428589\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3702497482299805\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.348139762878418\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.373006820678711\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.377819538116455\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4112884998321533\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3676259517669678\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3904812335968018\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4305505752563477\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3634679317474365\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4034194946289062\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.416750192642212\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3983983993530273\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3965892791748047\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3421719074249268\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.412919759750366\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.440889358520508\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.379283905029297\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4105262756347656\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4094808101654053\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3604371547698975\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3921005725860596\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3746700286865234\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3948047161102295\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.365351438522339\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.380195379257202\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4070751667022705\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3677823543548584\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3807568550109863\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3996481895446777\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.407794952392578\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3813884258270264\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3810927867889404\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3870089054107666\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3582706451416016\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.417889356613159\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4195873737335205\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.357346773147583\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.411404609680176\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.434117317199707\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.397134780883789\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4510152339935303\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4068706035614014\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4184393882751465\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4005026817321777\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.347844123840332\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4043445587158203\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3967976570129395\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3285343647003174\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.4117517471313477\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3961234092712402\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.401477336883545\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4002561569213867\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3992550373077393\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.409472942352295\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4215118885040283\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.378535270690918\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.418596029281616\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4508020877838135\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4022881984710693\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.362919807434082\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3735291957855225\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.369784355163574\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.386209726333618\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.391045331954956\n",
      "Training accuracy 0.10546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 10/36 [39:09<1:41:31, 234.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041051492095\n",
      "Training loss: 2.4202189445495605\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4036972522735596\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3877851963043213\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.351964235305786\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3874270915985107\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.38865065574646\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4016449451446533\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3381569385528564\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.374263286590576\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4505984783172607\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.405759811401367\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.373542308807373\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3772194385528564\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.413726568222046\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.407484531402588\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3827295303344727\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3429012298583984\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3452484607696533\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4035143852233887\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.449369192123413\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4185307025909424\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3940603733062744\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.372065305709839\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3639566898345947\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4046432971954346\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4333109855651855\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3587470054626465\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.410191059112549\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4055254459381104\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.408399820327759\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3718342781066895\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.393742561340332\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4073104858398438\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3962368965148926\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3838987350463867\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.407393455505371\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.392049789428711\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3107430934906006\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.362718105316162\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.393714666366577\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3920607566833496\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3839023113250732\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3732733726501465\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.452089309692383\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3632144927978516\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3528780937194824\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.361257553100586\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.376051425933838\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3997979164123535\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3316638469696045\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.428194761276245\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4144327640533447\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4272427558898926\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.368587017059326\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4024667739868164\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.429075002670288\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.369439125061035\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3998844623565674\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4331815242767334\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.401186943054199\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3872148990631104\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3753037452697754\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.39632248878479\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3544371128082275\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.316387176513672\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3966119289398193\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.351430654525757\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3835887908935547\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3665616512298584\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.401259660720825\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.445718288421631\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.409761905670166\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3887557983398438\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4078965187072754\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3321592807769775\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4157443046569824\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.33188796043396\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3438186645507812\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.371673822402954\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.42057204246521\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.385293960571289\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3693387508392334\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3897316455841064\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4545624256134033\n",
      "Training accuracy 0.0390625\n",
      "Training loss: 2.358900547027588\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4391801357269287\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4026410579681396\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.395977020263672\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3912148475646973\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.363448143005371\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3837406635284424\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3971290588378906\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3787336349487305\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.395298957824707\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.382147789001465\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.395989418029785\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4001667499542236\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.37925124168396\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.405470371246338\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3840932846069336\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4663729667663574\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.376133680343628\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3884329795837402\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.39936900138855\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3991525173187256\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.404101610183716\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.401514768600464\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3528969287872314\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4612679481506348\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3864028453826904\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.412912368774414\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.411377191543579\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.402254581451416\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3953633308410645\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.423496723175049\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3753578662872314\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.452577829360962\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.405869960784912\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.397702932357788\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.386606216430664\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4072659015655518\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4281256198883057\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3783111572265625\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4046149253845215\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4203643798828125\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3892927169799805\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.385235548019409\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4262306690216064\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.368042230606079\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3854169845581055\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3737401962280273\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3833816051483154\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4206631183624268\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4266152381896973\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.396977663040161\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4355156421661377\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3546881675720215\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.379209518432617\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.392590045928955\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4054465293884277\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.378607988357544\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.421718120574951\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4208505153656006\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3643605709075928\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3586227893829346\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.40864896774292\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4092330932617188\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4079532623291016\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.387007236480713\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4138145446777344\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3348166942596436\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3908681869506836\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.396549940109253\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3928442001342773\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.409724235534668\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.361872434616089\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3968405723571777\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.395089626312256\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.383938789367676\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.409853219985962\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.372138738632202\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.418509006500244\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3908958435058594\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3762893676757812\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.377946376800537\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.419415235519409\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.360008716583252\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.404250144958496\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.363328695297241\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.392582893371582\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.375166177749634\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3342366218566895\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3537724018096924\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3996365070343018\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3703980445861816\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4108102321624756\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3835153579711914\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4119656085968018\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4051759243011475\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.4229869842529297\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3814847469329834\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.373789072036743\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.360779047012329\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3565893173217773\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.358992099761963\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4206864833831787\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4047975540161133\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.390362024307251\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4005916118621826\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3642690181732178\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.431715488433838\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3521387577056885\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.4260711669921875\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.372339963912964\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3680505752563477\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3792476654052734\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3814756870269775\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4150912761688232\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3885743618011475\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.400534152984619\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.385956048965454\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.453693389892578\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4280624389648438\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.37004017829895\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4271063804626465\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4104912281036377\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3934383392333984\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3866560459136963\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.357877016067505\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3813910484313965\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3642313480377197\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4103353023529053\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3794541358947754\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3937597274780273\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4258103370666504\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.420452356338501\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.372342586517334\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3604912757873535\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4514145851135254\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4216744899749756\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4015579223632812\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3345625400543213\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3805978298187256\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.366150379180908\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3577821254730225\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.391618251800537\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3862380981445312\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.371574878692627\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.385861873626709\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3956968784332275\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.375504970550537\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4629178047180176\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.410787343978882\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.350717782974243\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4045727252960205\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4038138389587402\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.406193256378174\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4258663654327393\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.396223783493042\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.409393072128296\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3667685985565186\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3880796432495117\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.390957832336426\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3952581882476807\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.387915849685669\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3603615760803223\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3881630897521973\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3984243869781494\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4072494506835938\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3520865440368652\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4378254413604736\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.389674186706543\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.358800172805786\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3837192058563232\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.376077175140381\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.413645029067993\n",
      "Training accuracy 0.08984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 11/36 [42:58<1:37:03, 232.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041275009513\n",
      "Training loss: 2.35638427734375\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4124999046325684\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3706247806549072\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3958139419555664\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4036478996276855\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.442797899246216\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3850152492523193\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3467183113098145\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.425842761993408\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.396897792816162\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3288979530334473\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.360232353210449\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3786120414733887\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4317731857299805\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3928678035736084\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3892929553985596\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3906290531158447\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.356210470199585\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.37803316116333\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4227287769317627\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.399911642074585\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3691561222076416\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4020512104034424\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.384310007095337\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3614981174468994\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.389615535736084\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.376217842102051\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4139647483825684\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3994240760803223\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3841207027435303\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.371062994003296\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.401237964630127\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3488540649414062\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3714752197265625\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4270071983337402\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3589670658111572\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.372683525085449\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4019722938537598\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.365610122680664\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.413830518722534\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.410233974456787\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3771071434020996\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3433918952941895\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.364212989807129\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.396641731262207\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3536243438720703\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4044578075408936\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.378066062927246\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.390554428100586\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3853981494903564\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.376087188720703\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.427994966506958\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3763937950134277\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3548743724823\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3979883193969727\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.42580246925354\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.391890048980713\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4024245738983154\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4002327919006348\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.439944267272949\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4111695289611816\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.405812978744507\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4298250675201416\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3952553272247314\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3828065395355225\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4120397567749023\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.410670280456543\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.393157720565796\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.416149139404297\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.381143569946289\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.40022873878479\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.388005256652832\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3901143074035645\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3739988803863525\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4129796028137207\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.388763427734375\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.367069959640503\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.351411819458008\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3370814323425293\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3753111362457275\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3803396224975586\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4626309871673584\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4061412811279297\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3661510944366455\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3690919876098633\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3947951793670654\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3823471069335938\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4054877758026123\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3403730392456055\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.349191665649414\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.394134044647217\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4002161026000977\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.407064199447632\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3964288234710693\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3599047660827637\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3340461254119873\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3971610069274902\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3465774059295654\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3958852291107178\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4133267402648926\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3974318504333496\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4275014400482178\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.3592801094055176\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3656671047210693\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3987057209014893\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.396878719329834\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.338181257247925\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4076902866363525\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.390401601791382\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.411947727203369\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3383066654205322\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.387665033340454\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3787336349487305\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4217021465301514\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.355933666229248\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4244518280029297\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3789074420928955\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4428062438964844\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3841772079467773\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.391087055206299\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.420818567276001\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3991763591766357\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4055073261260986\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4171299934387207\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3891851902008057\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.362244129180908\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.376270294189453\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3892829418182373\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3579494953155518\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3898704051971436\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.385707139968872\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.386655569076538\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3898448944091797\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4011518955230713\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4526922702789307\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.422612190246582\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3816776275634766\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3971993923187256\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3685569763183594\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.376089334487915\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.420956611633301\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3824665546417236\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4254112243652344\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3907690048217773\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4345521926879883\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4041013717651367\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.408348560333252\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.372197389602661\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4203741550445557\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.362156391143799\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.466343641281128\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.371087074279785\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3501365184783936\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.368199348449707\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.363100528717041\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.381296157836914\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3916213512420654\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3957414627075195\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.419921636581421\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.404736042022705\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3619558811187744\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.396069049835205\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3610053062438965\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4089598655700684\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.378432273864746\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.358916759490967\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4024159908294678\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.404177665710449\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.375093936920166\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.424549102783203\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.361506462097168\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.386256694793701\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4273455142974854\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.385251998901367\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.354315996170044\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4180355072021484\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3566267490386963\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.387341022491455\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.412463903427124\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.434314489364624\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3894968032836914\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4343528747558594\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3817224502563477\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3520121574401855\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.405839443206787\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3701064586639404\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4220774173736572\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4208316802978516\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3863468170166016\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3873720169067383\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4333484172821045\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.37257719039917\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4366488456726074\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3708081245422363\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4354429244995117\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.444995641708374\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.4122350215911865\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.423044443130493\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.382796287536621\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3911969661712646\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.400425910949707\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4011332988739014\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4137299060821533\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3906896114349365\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.394951343536377\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3703219890594482\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4316577911376953\n",
      "Training accuracy 0.05078125\n",
      "Training loss: 2.3698389530181885\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.394338369369507\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.395559549331665\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3717949390411377\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.435670852661133\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4051249027252197\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3922553062438965\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.368730068206787\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.362051248550415\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.414031744003296\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4042654037475586\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3843233585357666\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.424673557281494\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3813815116882324\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4163880348205566\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3509252071380615\n",
      "Training accuracy 0.18359375\n",
      "Training loss: 2.4182822704315186\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3899896144866943\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.419138193130493\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4189653396606445\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3970296382904053\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3765923976898193\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3716673851013184\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.400376796722412\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.446173667907715\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4115309715270996\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3996310234069824\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.39143443107605\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.366537094116211\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4020252227783203\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4051029682159424\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.388352632522583\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.354095697402954\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.403061628341675\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.392721652984619\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3783509731292725\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.419215679168701\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.408540964126587\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.380479574203491\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3844640254974365\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.411740303039551\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.355400800704956\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.422839403152466\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.344832181930542\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4166228771209717\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.390965700149536\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3895232677459717\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.396677017211914\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4040143489837646\n",
      "Training accuracy 0.09375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 12/36 [46:48<1:32:42, 231.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.412904106080532\n",
      "Training loss: 2.342292547225952\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.37184476852417\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.404876470565796\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.418252944946289\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3631041049957275\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3592910766601562\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.356555461883545\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.393688678741455\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4023425579071045\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.397200345993042\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.399550199508667\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3705697059631348\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.466407299041748\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4227826595306396\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3962740898132324\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3959250450134277\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.350620985031128\n",
      "Training accuracy 0.1640625\n",
      "Training loss: 2.404296875\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.395961046218872\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.393986940383911\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.407866954803467\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.372868537902832\n",
      "Training accuracy 0.15625\n",
      "Training loss: 2.3886799812316895\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3838162422180176\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3678557872772217\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3864927291870117\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.35262131690979\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4429709911346436\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3811745643615723\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3642919063568115\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4283499717712402\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3990979194641113\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4013867378234863\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3887252807617188\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.367985963821411\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3667490482330322\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3809216022491455\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4437708854675293\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4053165912628174\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.36293363571167\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.373706102371216\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4469656944274902\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.421861410140991\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4150209426879883\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.383465528488159\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3963210582733154\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.379249095916748\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4641079902648926\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4472923278808594\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.400383472442627\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3781070709228516\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.338296890258789\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.378586530685425\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4009854793548584\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.402289867401123\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.393115282058716\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.350419282913208\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3690783977508545\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.404158353805542\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3777356147766113\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4148755073547363\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3735604286193848\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.390449285507202\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.43446946144104\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.381830930709839\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3548285961151123\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.414144515991211\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3993544578552246\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3878896236419678\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3786487579345703\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4258687496185303\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3995723724365234\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4153857231140137\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.384408473968506\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3776814937591553\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.411118268966675\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4157090187072754\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3761301040649414\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.430729866027832\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3859212398529053\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4196605682373047\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.389646053314209\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.350536346435547\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4252500534057617\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3501129150390625\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.386889696121216\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.333512306213379\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.396451950073242\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.400808095932007\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4260780811309814\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.391470432281494\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3939504623413086\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3862743377685547\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.362490177154541\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3911538124084473\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.364894390106201\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4136335849761963\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4028942584991455\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3523318767547607\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.403961658477783\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.429015636444092\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4420876502990723\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3858625888824463\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4114019870758057\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4070122241973877\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3775386810302734\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3606832027435303\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3543004989624023\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3773276805877686\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3984155654907227\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3912787437438965\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4055087566375732\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.386704444885254\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.379605293273926\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3798720836639404\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4169840812683105\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3931093215942383\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.340841293334961\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3905482292175293\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.364943027496338\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4045779705047607\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4289779663085938\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4214916229248047\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.396759271621704\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3970210552215576\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4516007900238037\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.389793872833252\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3731722831726074\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.35933780670166\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3431594371795654\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3809003829956055\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.346013307571411\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3659560680389404\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3786678314208984\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3789827823638916\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.402064800262451\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.383805751800537\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4214892387390137\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.395956516265869\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3919615745544434\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.379330635070801\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.357415199279785\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.378312587738037\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3593146800994873\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3390376567840576\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3868484497070312\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3724722862243652\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3922815322875977\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.385478973388672\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.451458215713501\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.4314424991607666\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.40299654006958\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.413977861404419\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3769500255584717\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.373758316040039\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.397195816040039\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.376307487487793\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.374340295791626\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3500258922576904\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.367877721786499\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3921937942504883\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4047844409942627\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.417124032974243\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.37933349609375\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4155118465423584\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3379034996032715\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3819565773010254\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3955090045928955\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3789916038513184\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.419339895248413\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.354607105255127\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.39689564704895\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3352625370025635\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.4148988723754883\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3752872943878174\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.395425796508789\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4267594814300537\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3622031211853027\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.371596574783325\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3961265087127686\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3663699626922607\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4154140949249268\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4006800651550293\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4253976345062256\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.444056987762451\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3984408378601074\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.367307186126709\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3846006393432617\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.417640447616577\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.415289878845215\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3727831840515137\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.466306686401367\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3593759536743164\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3886849880218506\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3883399963378906\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4035253524780273\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.391227960586548\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3708102703094482\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.384430408477783\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3832454681396484\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.415220260620117\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3542752265930176\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3905463218688965\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.400315284729004\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4077141284942627\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4484689235687256\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3945071697235107\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3402388095855713\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3913345336914062\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3882033824920654\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.382143974304199\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3866941928863525\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3692524433135986\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.373662233352661\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3808584213256836\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4089953899383545\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.371927261352539\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4116599559783936\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.358335018157959\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4116764068603516\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3427655696868896\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4167237281799316\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3804168701171875\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.378436326980591\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.386852502822876\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.418247699737549\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3552188873291016\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.42002534866333\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4054572582244873\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.368074655532837\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.379497528076172\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.374080181121826\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.344125270843506\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3719117641448975\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.393397569656372\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4176933765411377\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3824493885040283\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.390387535095215\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4379758834838867\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.444329023361206\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3915927410125732\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.40476393699646\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.400886058807373\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.389982223510742\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.384065628051758\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3759958744049072\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.364222764968872\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.374905586242676\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4158875942230225\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3854923248291016\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.36575984954834\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4106268882751465\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.399179458618164\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3804290294647217\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.444715976715088\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3734655380249023\n",
      "Training accuracy 0.1015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 13/36 [50:38<1:28:38, 231.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.412904106080532\n",
      "Training loss: 2.397911310195923\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3874220848083496\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4007439613342285\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4448537826538086\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.397733211517334\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3898558616638184\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.396409273147583\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.433565139770508\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.39092755317688\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.369791269302368\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.394446849822998\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.372877359390259\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4329285621643066\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.42270827293396\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.367948055267334\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3612897396087646\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.39689564704895\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4017443656921387\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.368412733078003\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4346485137939453\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.390582799911499\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3473737239837646\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4048655033111572\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.429663896560669\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.39155650138855\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4383187294006348\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4231207370758057\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3456926345825195\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3592183589935303\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3927433490753174\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.427638530731201\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.392301082611084\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3787007331848145\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.408803939819336\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.427985429763794\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.378274440765381\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3869471549987793\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.376396894454956\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.374204397201538\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4253623485565186\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.414803981781006\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4138987064361572\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.427554130554199\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.408679246902466\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3589134216308594\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4094648361206055\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.336430072784424\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.402229070663452\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3874237537384033\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.393460512161255\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.401618242263794\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3976006507873535\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.411097526550293\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3727080821990967\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.393648147583008\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3853671550750732\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3870255947113037\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4045515060424805\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4062066078186035\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3736095428466797\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3571956157684326\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.4159297943115234\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3932015895843506\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3401365280151367\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.405186891555786\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3745782375335693\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4104838371276855\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3925788402557373\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3882155418395996\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.402850389480591\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.434774398803711\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.34012770652771\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3811731338500977\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.425443172454834\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.410527229309082\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.371495246887207\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.37137770652771\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4056296348571777\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.337502956390381\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.409348249435425\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3872580528259277\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3913300037384033\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.386589288711548\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4113667011260986\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.363335132598877\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.392345428466797\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3744516372680664\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3960747718811035\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.406148672103882\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.406348705291748\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.38771915435791\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.387115001678467\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3772215843200684\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4034714698791504\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.36226224899292\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3633127212524414\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3799803256988525\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3994455337524414\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4033420085906982\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.390465497970581\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.40724778175354\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3936479091644287\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3508355617523193\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.391199827194214\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.451559066772461\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.399662971496582\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3742294311523438\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4247634410858154\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3854806423187256\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3600270748138428\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3776750564575195\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.403543710708618\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.363764524459839\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3729209899902344\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.365473508834839\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3837437629699707\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.371196746826172\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.420325517654419\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3921561241149902\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3888514041900635\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3651509284973145\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3735804557800293\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4032065868377686\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.380366086959839\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.338519811630249\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.407449722290039\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.331557512283325\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.413231372833252\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.37221097946167\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3797638416290283\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3749210834503174\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3979849815368652\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.382948875427246\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.413123607635498\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3703131675720215\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3529562950134277\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.438316822052002\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.415952682495117\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.434767007827759\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4200856685638428\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4288501739501953\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4054155349731445\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3720216751098633\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.381472110748291\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.378173589706421\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.367372751235962\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.349320650100708\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4088149070739746\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.384977340698242\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3948440551757812\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.420999526977539\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.401740550994873\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3801109790802\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4117431640625\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.40238881111145\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.418802261352539\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.406482219696045\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3997864723205566\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.421448230743408\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3552238941192627\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.40325665473938\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.395223379135132\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3771955966949463\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3470754623413086\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.372821569442749\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.401951313018799\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4171791076660156\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3852932453155518\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3506267070770264\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3853540420532227\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4018404483795166\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.419271469116211\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.416780471801758\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.416797399520874\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3465335369110107\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.3533859252929688\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3743209838867188\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4592435359954834\n",
      "Training accuracy 0.03125\n",
      "Training loss: 2.4122307300567627\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3661341667175293\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.397825241088867\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4335978031158447\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3797240257263184\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.352036237716675\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.379087448120117\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4009761810302734\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3635995388031006\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3736934661865234\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4166998863220215\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.373710870742798\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.402451276779175\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.429854393005371\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3576393127441406\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.407031774520874\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.406668186187744\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.414544105529785\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.394047260284424\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3712377548217773\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3972816467285156\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.453537702560425\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.418658971786499\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3632688522338867\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3880186080932617\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3917150497436523\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3710474967956543\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.373337745666504\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4042911529541016\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3882267475128174\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3549695014953613\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4034483432769775\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.382606267929077\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3359758853912354\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3988637924194336\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4283108711242676\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.410583019256592\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.375849485397339\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4120945930480957\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4245476722717285\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3812081813812256\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.360837459564209\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.374972105026245\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.398982286453247\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.408474922180176\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.374932050704956\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4293627738952637\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.376636505126953\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3742527961730957\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3925819396972656\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4038209915161133\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.392259120941162\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.385669469833374\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.377777576446533\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3390309810638428\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3686139583587646\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4309847354888916\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.373196840286255\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.390578269958496\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.450591802597046\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3993911743164062\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.370412826538086\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.424879789352417\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3623299598693848\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3971078395843506\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.340505361557007\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.42269229888916\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4110465049743652\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.393771171569824\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.375200033187866\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4347383975982666\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.346284866333008\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3941872119903564\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.386814594268799\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3689825534820557\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3792901039123535\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3942768573760986\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4025485515594482\n",
      "Training accuracy 0.0859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 14/36 [54:27<1:24:38, 230.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.41290411259979\n",
      "Training loss: 2.393465518951416\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4198057651519775\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4472758769989014\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.418153762817383\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4294989109039307\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.408376693725586\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3694546222686768\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.415321111679077\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.384219169616699\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.438476800918579\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3780670166015625\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.348864793777466\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4210705757141113\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.348954200744629\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3882336616516113\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4026989936828613\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3679561614990234\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.332916736602783\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3644354343414307\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.467897653579712\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3946785926818848\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4239344596862793\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3431711196899414\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4013760089874268\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3956053256988525\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.459886074066162\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.397338390350342\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3939757347106934\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.417945384979248\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3833236694335938\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3635904788970947\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3658785820007324\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.404574394226074\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.372467517852783\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3908212184906006\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.439711332321167\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.39084529876709\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.409672975540161\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3545315265655518\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4394257068634033\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.383768081665039\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3913209438323975\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.393946647644043\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.432830333709717\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.409421920776367\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.367021083831787\n",
      "Training accuracy 0.15625\n",
      "Training loss: 2.4275708198547363\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.421376943588257\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3415279388427734\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3639798164367676\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.379565954208374\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3637900352478027\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3858394622802734\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4015488624572754\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.336493730545044\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3698527812957764\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4152944087982178\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3920035362243652\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.392076253890991\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3652632236480713\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3724942207336426\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3899991512298584\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.399550676345825\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4127750396728516\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.409343957901001\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.401024103164673\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.364576816558838\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.39679217338562\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3664236068725586\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.424748659133911\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.380202293395996\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.356013536453247\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4035470485687256\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4076504707336426\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.406859874725342\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4443092346191406\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4453306198120117\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3644208908081055\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.390566110610962\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.416541337966919\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.387732744216919\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.380458354949951\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.386359453201294\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4212210178375244\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3750975131988525\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.370934247970581\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.419724941253662\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.360583782196045\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4084231853485107\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.417940378189087\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.323381185531616\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3757314682006836\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4039976596832275\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.389841079711914\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.392951488494873\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.371321678161621\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.379140853881836\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4031755924224854\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.375261068344116\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.363198757171631\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.348797559738159\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.326519012451172\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.361643075942993\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4298243522644043\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.346389055252075\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3744826316833496\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.387859582901001\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.39210844039917\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3715474605560303\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4090991020202637\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.39929461479187\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.395859956741333\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.418816328048706\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.365544319152832\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3732283115386963\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3752145767211914\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3794641494750977\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.378541946411133\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3885838985443115\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.357632875442505\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3867390155792236\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3968586921691895\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.414029836654663\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.407731294631958\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3560850620269775\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4212868213653564\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.386540651321411\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4220945835113525\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4019176959991455\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4123246669769287\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4158289432525635\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.369739532470703\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.398712635040283\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3584868907928467\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.380223274230957\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3674070835113525\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3655247688293457\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3919925689697266\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.370633363723755\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4124300479888916\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3623077869415283\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.389407157897949\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3996193408966064\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4051201343536377\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4151811599731445\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3733716011047363\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3541743755340576\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.434019088745117\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3687596321105957\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.421395778656006\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4129719734191895\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.392899990081787\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.43992018699646\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3748779296875\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4042751789093018\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4104368686676025\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.409158945083618\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4121646881103516\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.394285202026367\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3767778873443604\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.365415334701538\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.35184645652771\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3673653602600098\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.37495756149292\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4073190689086914\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.395998239517212\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.400137186050415\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.419706106185913\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3513426780700684\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3496313095092773\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4082884788513184\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.380904197692871\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4180119037628174\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.396702527999878\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4223129749298096\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4028267860412598\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.42484188079834\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3764755725860596\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3759098052978516\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3954012393951416\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4306344985961914\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3950998783111572\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.362225294113159\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.422177314758301\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4043538570404053\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3519556522369385\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3917884826660156\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.405956268310547\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3732986450195312\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.3823609352111816\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3975067138671875\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4006710052490234\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4222137928009033\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4079322814941406\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3711371421813965\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.423710823059082\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.413073778152466\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3783323764801025\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3979599475860596\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3816232681274414\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3986659049987793\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3678338527679443\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3751485347747803\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4086556434631348\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.398240804672241\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4083433151245117\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3855135440826416\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.413530111312866\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.408806562423706\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3747429847717285\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.444725513458252\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.38785457611084\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.352421283721924\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.3832614421844482\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.380431652069092\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.394664764404297\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4042017459869385\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4167978763580322\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.391826868057251\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.362790107727051\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.425436019897461\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3988888263702393\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.401709794998169\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3650197982788086\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3895976543426514\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.363301992416382\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.408360004425049\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.435614585876465\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4003372192382812\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.416308879852295\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3487110137939453\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.374729633331299\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3880882263183594\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3677170276641846\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4157001972198486\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.401942014694214\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3796467781066895\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4115231037139893\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4429757595062256\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.423153877258301\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3510680198669434\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3876149654388428\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4158308506011963\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.365252733230591\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4052867889404297\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.421896457672119\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3774759769439697\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4417240619659424\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.357121229171753\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.379350185394287\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4092917442321777\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.386768102645874\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.348804473876953\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3582239151000977\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.376586437225342\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.356053352355957\n",
      "Training accuracy 0.1015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 15/36 [58:17<1:20:40, 230.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041079431772\n",
      "Training loss: 2.369959592819214\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.378530502319336\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.404635190963745\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.39998722076416\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.356877326965332\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.379085063934326\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.414008855819702\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.389888048171997\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3636159896850586\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.392918109893799\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3987157344818115\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3683645725250244\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3866665363311768\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3989365100860596\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.420840263366699\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4428884983062744\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3976426124572754\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3908984661102295\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.400977849960327\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4677793979644775\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.400527000427246\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4044556617736816\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3827078342437744\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4234089851379395\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3380658626556396\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3843154907226562\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3948745727539062\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.365751028060913\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4077277183532715\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3633174896240234\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4378085136413574\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3798410892486572\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3951427936553955\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.408815383911133\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4088401794433594\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3248343467712402\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.405851125717163\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.403014659881592\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.386826276779175\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3896024227142334\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.363013982772827\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4031825065612793\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3564329147338867\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4205784797668457\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.430180311203003\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.393852710723877\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4242148399353027\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.363399028778076\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4103798866271973\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.403982639312744\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.42061710357666\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.384523868560791\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3449466228485107\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.4128453731536865\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.387503147125244\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3949100971221924\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3199269771575928\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4675676822662354\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3532562255859375\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4149627685546875\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3758111000061035\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3886289596557617\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4144561290740967\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.385523796081543\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4033279418945312\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.377798557281494\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4149115085601807\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4032485485076904\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.371762990951538\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4115841388702393\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3864428997039795\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3818111419677734\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.366610527038574\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.353421926498413\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4105610847473145\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3979971408843994\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4154319763183594\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.366377353668213\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.405027151107788\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4555912017822266\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3700687885284424\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3880815505981445\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4043540954589844\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3335132598876953\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.3758974075317383\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.377634286880493\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3976194858551025\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.425959348678589\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.390246629714966\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4050002098083496\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4255270957946777\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4000022411346436\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3878049850463867\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4102296829223633\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.372230291366577\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.417433261871338\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3485021591186523\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3339855670928955\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.389894962310791\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3524587154388428\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.422940492630005\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.431520462036133\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.398327350616455\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.396235704421997\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3971455097198486\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.387338876724243\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4040024280548096\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.381845712661743\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4421558380126953\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3622899055480957\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4054577350616455\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.422873020172119\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.340967893600464\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.406280279159546\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.366908073425293\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.393545150756836\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3428776264190674\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4055075645446777\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.381244421005249\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3682861328125\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.369818925857544\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3588221073150635\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4289402961730957\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.406545400619507\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.379640579223633\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4140772819519043\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3923003673553467\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3989503383636475\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3948211669921875\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3725013732910156\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.37075138092041\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.398002862930298\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3733644485473633\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4092774391174316\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.39105486869812\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.401456117630005\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3674097061157227\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.418292760848999\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.398066282272339\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3937387466430664\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3678605556488037\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.390345811843872\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3602304458618164\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4112086296081543\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.368032932281494\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4002649784088135\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3740949630737305\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.378222942352295\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3774075508117676\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3907487392425537\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.367357015609741\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.383667469024658\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.355975389480591\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4358296394348145\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.384756326675415\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.435741424560547\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4305579662323\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.341968059539795\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4010262489318848\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3559248447418213\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3603150844573975\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4268147945404053\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4316797256469727\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3604607582092285\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.411797285079956\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3718996047973633\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3996739387512207\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.364642858505249\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4048357009887695\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3652937412261963\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.362553596496582\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.393749713897705\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.406771421432495\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.369565486907959\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.364490032196045\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.37931227684021\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3932106494903564\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.373033285140991\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3650949001312256\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.417419195175171\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4325764179229736\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.378566026687622\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4263806343078613\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.368098258972168\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.419414758682251\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4236574172973633\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4311647415161133\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4402811527252197\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.360004186630249\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3475301265716553\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3718690872192383\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3876397609710693\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4094951152801514\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3847663402557373\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4552202224731445\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4066720008850098\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4165751934051514\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4275832176208496\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.407576084136963\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3512332439422607\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4177796840667725\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3813130855560303\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4214625358581543\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3691048622131348\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3432347774505615\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.424304962158203\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3870298862457275\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3948733806610107\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3767194747924805\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4152469635009766\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4088902473449707\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.424816608428955\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.411317825317383\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.453009843826294\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4220688343048096\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3436660766601562\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3575997352600098\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.38862681388855\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3814127445220947\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4051878452301025\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3657259941101074\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3959295749664307\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3435137271881104\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.390824317932129\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4012928009033203\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3885886669158936\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4077956676483154\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3846545219421387\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.41367244720459\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3567473888397217\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3669912815093994\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.375255823135376\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.37026309967041\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.38429594039917\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.415104389190674\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4283554553985596\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3889145851135254\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.365006923675537\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.417158603668213\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3716859817504883\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3824520111083984\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4030332565307617\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3884470462799072\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4517712593078613\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.398688554763794\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4318552017211914\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3611884117126465\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3846702575683594\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.380093812942505\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.392024040222168\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.421692132949829\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3904800415039062\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3594117164611816\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.38926362991333\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.374248504638672\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3509817123413086\n",
      "Training accuracy 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 16/36 [1:02:07<1:16:47, 230.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.412904128432274\n",
      "Training loss: 2.4019112586975098\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3841865062713623\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3711538314819336\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.390407085418701\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3768959045410156\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4229142665863037\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3921713829040527\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4485790729522705\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4053053855895996\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3840324878692627\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.408492088317871\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.390899419784546\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.315340518951416\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3831233978271484\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3636655807495117\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4609482288360596\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4247021675109863\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.448993682861328\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3634238243103027\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.375066041946411\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.415797472000122\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3773834705352783\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.384871006011963\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3663222789764404\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.389925003051758\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.415091037750244\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3964998722076416\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3798677921295166\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.382871627807617\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3819162845611572\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3181490898132324\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.37461256980896\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.372541666030884\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3679049015045166\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4021501541137695\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.439112424850464\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3523619174957275\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3580548763275146\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.334834337234497\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4133005142211914\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3455650806427\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4001352787017822\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.375344753265381\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.418083429336548\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3665575981140137\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.402631998062134\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3604896068573\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3763277530670166\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.363157033920288\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4013407230377197\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.387826442718506\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3711211681365967\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4061150550842285\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3697402477264404\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.404639482498169\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3951101303100586\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4205942153930664\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.3780105113983154\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3755035400390625\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3744428157806396\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3784921169281006\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4087648391723633\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3880200386047363\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3387327194213867\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3934621810913086\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4295921325683594\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3770062923431396\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4312469959259033\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.383683443069458\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.377666473388672\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3756191730499268\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4114444255828857\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3862314224243164\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4021613597869873\n",
      "Training accuracy 0.05078125\n",
      "Training loss: 2.3939433097839355\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.462446689605713\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3954954147338867\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.390874147415161\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.36250376701355\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.376588821411133\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3918416500091553\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.398210048675537\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.399390697479248\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3775954246520996\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3721563816070557\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.357325315475464\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.365760087966919\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3986315727233887\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4298741817474365\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.389869451522827\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.408304452896118\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.420950174331665\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3731377124786377\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.355978488922119\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4123237133026123\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3747611045837402\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.381348133087158\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.432215690612793\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3879823684692383\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.413658618927002\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4365880489349365\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4074370861053467\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4410924911499023\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3994972705841064\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.368741989135742\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3610379695892334\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3775415420532227\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3923559188842773\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.438328266143799\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3260321617126465\n",
      "Training accuracy 0.171875\n",
      "Training loss: 2.3880553245544434\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.424450397491455\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3866822719573975\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4152069091796875\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.379972219467163\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3966286182403564\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.401702880859375\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3525259494781494\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.417189359664917\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3931174278259277\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4120943546295166\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3881382942199707\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4016926288604736\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.412564754486084\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.352827787399292\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.378868341445923\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3885436058044434\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3927369117736816\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.376133441925049\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.33782958984375\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.399585247039795\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4263226985931396\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4214916229248047\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.420057535171509\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3976333141326904\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.36734676361084\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4178500175476074\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3886964321136475\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.366680860519409\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.373328924179077\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.395251989364624\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.395028829574585\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4303829669952393\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3574979305267334\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3938517570495605\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3837761878967285\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3413848876953125\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4393837451934814\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.368023633956909\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.460191488265991\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4080538749694824\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.368608236312866\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4012813568115234\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.419400930404663\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4061551094055176\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.378966808319092\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.384136199951172\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.3833510875701904\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.390669822692871\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3558387756347656\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.365309000015259\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4329473972320557\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.384718418121338\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3995025157928467\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4294331073760986\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4263899326324463\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4008007049560547\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4194719791412354\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3954379558563232\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3574492931365967\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.390113592147827\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.357358455657959\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4095656871795654\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3951356410980225\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4083609580993652\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3758392333984375\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4174532890319824\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.408440113067627\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4095053672790527\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.385317087173462\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.424586057662964\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.395162343978882\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.447150230407715\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.422905683517456\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.339326858520508\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.351816415786743\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4054698944091797\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3620104789733887\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4396955966949463\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.405268430709839\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.370863199234009\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.365692138671875\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3636889457702637\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4202733039855957\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.370973825454712\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4077200889587402\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4331955909729004\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.413550615310669\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.384758710861206\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.395191192626953\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.374864339828491\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.364076852798462\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3512110710144043\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4004530906677246\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.376671314239502\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.379415512084961\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3737080097198486\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3798892498016357\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.388887882232666\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.37504243850708\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3912274837493896\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.390073299407959\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3736448287963867\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4231367111206055\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.39829158782959\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.407987356185913\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4038736820220947\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.397498369216919\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4078657627105713\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.365067720413208\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4142005443573\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4022841453552246\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.358764171600342\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.427536725997925\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.436917543411255\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3932409286499023\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3697922229766846\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.370375871658325\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3822999000549316\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4101288318634033\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3976991176605225\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4258999824523926\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.349241256713867\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3990814685821533\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3828799724578857\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.378328800201416\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.398332118988037\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4219746589660645\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.382319211959839\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4013705253601074\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.387930393218994\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.353365182876587\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3955671787261963\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.381028652191162\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.351494550704956\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4179930686950684\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.426309823989868\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.394531011581421\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.430924892425537\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3543999195098877\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3838770389556885\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.393908739089966\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3833556175231934\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4183189868927\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.421875238418579\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4260239601135254\n",
      "Training accuracy 0.1015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 17/36 [1:05:58<1:13:00, 230.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.412904094904661\n",
      "Training loss: 2.3906354904174805\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3589539527893066\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.416661024093628\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.41001033782959\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.400930643081665\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.384945869445801\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3683066368103027\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3842201232910156\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.395439863204956\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.431039333343506\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.378749370574951\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.400550365447998\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.376708507537842\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.348686933517456\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3860912322998047\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4022696018218994\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.434539794921875\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.406373977661133\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.395742654800415\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3734469413757324\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.345658779144287\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.447216272354126\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.393169641494751\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3760082721710205\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3926749229431152\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3780975341796875\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3412625789642334\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.387091875076294\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3771069049835205\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4302093982696533\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4093124866485596\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.386380910873413\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.397678852081299\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.373060464859009\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.3949944972991943\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.390857219696045\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.41353440284729\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.386730194091797\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3520290851593018\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4210710525512695\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.385948896408081\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.395805597305298\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.41916561126709\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4008681774139404\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3673453330993652\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3903396129608154\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3402485847473145\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.428624391555786\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.394458055496216\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4075140953063965\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.417030096054077\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3822238445281982\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3297483921051025\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.372469186782837\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.443574905395508\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.341966390609741\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4196884632110596\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.394413709640503\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3947744369506836\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3827664852142334\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.382758140563965\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3749327659606934\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3906285762786865\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.380476713180542\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3429994583129883\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.460282802581787\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3994381427764893\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.420454740524292\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3824462890625\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3986408710479736\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3653364181518555\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.350349187850952\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.355177402496338\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3913354873657227\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.409104585647583\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4112932682037354\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.433955669403076\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.401876449584961\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3566339015960693\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4116013050079346\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3578574657440186\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3952338695526123\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.40548038482666\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.403191566467285\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3912854194641113\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.405587673187256\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.364128828048706\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.411506414413452\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3824198246002197\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.382905960083008\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3952605724334717\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.385828971862793\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.428152084350586\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.423236608505249\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.384186267852783\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4236233234405518\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.408299446105957\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.368194103240967\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.372422218322754\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3900985717773438\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3596036434173584\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.390970468521118\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3422091007232666\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3896358013153076\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4270801544189453\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3688268661499023\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4086902141571045\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3568406105041504\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.388639450073242\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3933162689208984\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3899073600769043\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4360485076904297\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.363900661468506\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.40018367767334\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3990976810455322\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3300094604492188\n",
      "Training accuracy 0.15625\n",
      "Training loss: 2.3940961360931396\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3562254905700684\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3735482692718506\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4369242191314697\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.383275270462036\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.374950408935547\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3448872566223145\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.33138370513916\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.369692802429199\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3660266399383545\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3671488761901855\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4072704315185547\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3892977237701416\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4073173999786377\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.440520763397217\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4139060974121094\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.38305401802063\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.456594944000244\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4073424339294434\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.410783052444458\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.403029203414917\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3848743438720703\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.392219066619873\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.410156488418579\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3921306133270264\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3659322261810303\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.38183331489563\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.379918098449707\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.373504638671875\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4178271293640137\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4343364238739014\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3891761302948\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.385336399078369\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.394123077392578\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3838119506835938\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.356856107711792\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.4319067001342773\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4310295581817627\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.398876905441284\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3934848308563232\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3903937339782715\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.407881259918213\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.370452404022217\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3789525032043457\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.350170373916626\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.4028022289276123\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4423928260803223\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.381518840789795\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.387794256210327\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.410623550415039\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4448416233062744\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3812143802642822\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3986334800720215\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.409966468811035\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.424987316131592\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.389338731765747\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.412008285522461\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4084794521331787\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3957138061523438\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4089174270629883\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3666298389434814\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3965110778808594\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3767454624176025\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.39650559425354\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3840737342834473\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4184014797210693\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.354943037033081\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3776702880859375\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.40169095993042\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3979945182800293\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.409601926803589\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.422975540161133\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.39877986907959\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4017138481140137\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4160330295562744\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.428849697113037\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.381743907928467\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.414123773574829\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.444776773452759\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.389906167984009\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.408418893814087\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.418203592300415\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.428864002227783\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3642635345458984\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.401069164276123\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3849127292633057\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4216041564941406\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.369901180267334\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.413067102432251\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.393512010574341\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3374600410461426\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3942604064941406\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.392036199569702\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4377315044403076\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3687047958374023\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3904759883880615\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.418362617492676\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.353921413421631\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3892030715942383\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.394864797592163\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.402390718460083\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.444187641143799\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3711063861846924\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3513901233673096\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3743796348571777\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.398298501968384\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.433575391769409\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.36126446723938\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3838045597076416\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.351086139678955\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.41171932220459\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.409409999847412\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.375930070877075\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.3997771739959717\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.382899284362793\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.419883966445923\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.377232313156128\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.373279571533203\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4225692749023438\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3753256797790527\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.36830735206604\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3779289722442627\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3605756759643555\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.364898681640625\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.40547251701355\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4188284873962402\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3502328395843506\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.37288498878479\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3770861625671387\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.378347396850586\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.403386354446411\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3698651790618896\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3837878704071045\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.432893753051758\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.374803304672241\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.379835605621338\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3674113750457764\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3895280361175537\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3792521953582764\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4409570693969727\n",
      "Training accuracy 0.08203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 18/36 [1:09:49<1:09:10, 230.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129040772095323\n",
      "Training loss: 2.384190559387207\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4095818996429443\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.373596668243408\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.362414598464966\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3880882263183594\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.411872386932373\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.406210422515869\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.395176887512207\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.438568592071533\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3990752696990967\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.361112117767334\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.4067513942718506\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.372406482696533\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3710758686065674\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4034764766693115\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.43947172164917\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3779845237731934\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4142844676971436\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.375328302383423\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3646016120910645\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.36478328704834\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3776590824127197\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.36222505569458\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.383176326751709\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.38362455368042\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4061965942382812\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3723533153533936\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.389645576477051\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3930773735046387\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3696281909942627\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4333837032318115\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4051218032836914\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4364349842071533\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.401151657104492\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.382399559020996\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.395407199859619\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3735859394073486\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.447267770767212\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4029736518859863\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3991570472717285\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4082138538360596\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3952224254608154\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.400940418243408\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.386523962020874\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.376295566558838\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.373117208480835\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3751204013824463\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.400111436843872\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.4256703853607178\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3907008171081543\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.376736879348755\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.402374029159546\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4342498779296875\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.333439350128174\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.395723819732666\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.413135051727295\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3999199867248535\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.389099597930908\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4131736755371094\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3954672813415527\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4787189960479736\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3845393657684326\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.388786554336548\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4076623916625977\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.353550434112549\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3858327865600586\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.380739212036133\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3769540786743164\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.393158435821533\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.423191785812378\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.38582444190979\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.41599702835083\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3458449840545654\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4199225902557373\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4105281829833984\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4095234870910645\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3895626068115234\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3849592208862305\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3782074451446533\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.418773651123047\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.394012212753296\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.386838912963867\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.354806900024414\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3661999702453613\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.387577533721924\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.410149097442627\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.376023292541504\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3563284873962402\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4061334133148193\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3905603885650635\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4179177284240723\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4652342796325684\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3889617919921875\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4091176986694336\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3503384590148926\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.403744697570801\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.342362642288208\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3505518436431885\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.39951753616333\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4349141120910645\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4083011150360107\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3524131774902344\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.401947259902954\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3840928077697754\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4127063751220703\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4106526374816895\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4157233238220215\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.374131679534912\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3854143619537354\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.373626947402954\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.382401943206787\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.379978656768799\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4224414825439453\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.368096113204956\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.396937847137451\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3673653602600098\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.395988941192627\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.386294364929199\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.445812702178955\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.3693535327911377\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3514585494995117\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4522852897644043\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.417025327682495\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4176836013793945\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.373281717300415\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3911619186401367\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3849449157714844\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3560738563537598\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.37139630317688\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4149932861328125\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4093451499938965\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3825817108154297\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3481385707855225\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.366795539855957\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4010777473449707\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4273555278778076\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3917760848999023\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.396263599395752\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.355032205581665\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.355660915374756\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.404552459716797\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.403754711151123\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3710825443267822\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.374133825302124\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.412505626678467\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.389124631881714\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.373807668685913\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4183530807495117\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3633005619049072\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.375415563583374\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3696212768554688\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3856489658355713\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.414846420288086\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.38965106010437\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4047117233276367\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.391963005065918\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4147722721099854\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.404672861099243\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.461703062057495\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3952157497406006\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3938472270965576\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.353341817855835\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4002645015716553\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3577678203582764\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4025840759277344\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3798060417175293\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3805367946624756\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3699114322662354\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.363917350769043\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3834731578826904\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.386077642440796\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3871543407440186\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4080214500427246\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.397273540496826\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4394614696502686\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4182708263397217\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3703532218933105\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3927791118621826\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.417606830596924\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.404545783996582\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.41070556640625\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.384047031402588\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4100136756896973\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.350835084915161\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.355165481567383\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.395468235015869\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.38021183013916\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3408432006835938\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3740711212158203\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.386808156967163\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3678765296936035\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.385006904602051\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3910839557647705\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.342573881149292\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4099252223968506\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.413727045059204\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3621420860290527\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.389585256576538\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.378180742263794\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3664307594299316\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3563432693481445\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3897438049316406\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3994827270507812\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3637123107910156\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.395630359649658\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4184587001800537\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4020497798919678\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3695762157440186\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.4032371044158936\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4048972129821777\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.378399610519409\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.407855749130249\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.427490472793579\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3569791316986084\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.37138032913208\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4278836250305176\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4092414379119873\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.383810043334961\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3937132358551025\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.355485677719116\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3732950687408447\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.378363609313965\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3734874725341797\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.369396209716797\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.410191297531128\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.367673873901367\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4188859462738037\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.378488302230835\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3748538494110107\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4731578826904297\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3712902069091797\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.402892827987671\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.403005838394165\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4035000801086426\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.407711982727051\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.420437812805176\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3786110877990723\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4240591526031494\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3904647827148438\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3660566806793213\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.41658878326416\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.408808708190918\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3830034732818604\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3582041263580322\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.423004388809204\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3794431686401367\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.390969753265381\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.357692241668701\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3532826900482178\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3766965866088867\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3957886695861816\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4401378631591797\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3912477493286133\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4344265460968018\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3826346397399902\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.398115634918213\n",
      "Training accuracy 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 19/36 [1:13:38<1:05:14, 230.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041070118546\n",
      "Training loss: 2.4029934406280518\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3431670665740967\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.417646884918213\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.405170440673828\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.427503824234009\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4109294414520264\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.391709327697754\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3788645267486572\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4078330993652344\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3938522338867188\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.400531053543091\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4394664764404297\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3826828002929688\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.405660390853882\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.366421699523926\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4096879959106445\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.37424635887146\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.333530902862549\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4106664657592773\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.377596378326416\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3832812309265137\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.427337884902954\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4114980697631836\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3801956176757812\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.402517795562744\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3765182495117188\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3873915672302246\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4042444229125977\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3543918132781982\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.390167713165283\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4005188941955566\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3908674716949463\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3768718242645264\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.386033535003662\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4203226566314697\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3834593296051025\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.390418529510498\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4321632385253906\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.403972625732422\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.36429500579834\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4110329151153564\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3750760555267334\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.393893003463745\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.395690679550171\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4578301906585693\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.394364595413208\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.354822874069214\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.385711193084717\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.317110300064087\n",
      "Training accuracy 0.1640625\n",
      "Training loss: 2.347273588180542\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.361818790435791\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.387942314147949\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.331233024597168\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3989360332489014\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.416543960571289\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4095897674560547\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.342632532119751\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.371141195297241\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3506555557250977\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3258554935455322\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.359124183654785\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3665730953216553\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3815877437591553\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3966054916381836\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3979194164276123\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.384477376937866\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3446662425994873\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4084596633911133\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4111950397491455\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.387213945388794\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4051225185394287\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.390557289123535\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.405156135559082\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.395686626434326\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3824081420898438\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.426741600036621\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3617970943450928\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3964784145355225\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.37764835357666\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.367668867111206\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.381068706512451\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.434479236602783\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.341641902923584\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3907055854797363\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3988406658172607\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.3718302249908447\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3789126873016357\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3863251209259033\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.442507743835449\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3816215991973877\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3439419269561768\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.399940013885498\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.372819185256958\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3638930320739746\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.394840955734253\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.390716314315796\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3969504833221436\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.39217472076416\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4136180877685547\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.35876202583313\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4262771606445312\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4042744636535645\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3831446170806885\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3610341548919678\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4435670375823975\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.386890411376953\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3561017513275146\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3971943855285645\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4150805473327637\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4014766216278076\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.374647617340088\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.42055606842041\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3648416996002197\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.374941825866699\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.385427236557007\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4158823490142822\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.384115219116211\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.412395477294922\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3511767387390137\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.391930103302002\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.405921459197998\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4214799404144287\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.430979013442993\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3766298294067383\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3787598609924316\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.391787052154541\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.392864227294922\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.385249137878418\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.420262098312378\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3909246921539307\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.390216588973999\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.396529197692871\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3973517417907715\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3516831398010254\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.408719778060913\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.382920503616333\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3857462406158447\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3923580646514893\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.389998435974121\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.418858051300049\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3599936962127686\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4089884757995605\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.429647922515869\n",
      "Training accuracy 0.05078125\n",
      "Training loss: 2.4187304973602295\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4211297035217285\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3627068996429443\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.411374807357788\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.432465076446533\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3656013011932373\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.375868797302246\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4119043350219727\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3710193634033203\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.420668601989746\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3993260860443115\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.411320447921753\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.399836301803589\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.411529541015625\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4170665740966797\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.350956916809082\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.377598285675049\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.358950614929199\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3913440704345703\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3956222534179688\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3777167797088623\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3958122730255127\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4043314456939697\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.404210090637207\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.395449638366699\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4169061183929443\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.384981632232666\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4299206733703613\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4448554515838623\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4241549968719482\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.392998456954956\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.400028705596924\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3708596229553223\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4081101417541504\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3995609283447266\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.390780448913574\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4008612632751465\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.397873878479004\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4367964267730713\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.415897846221924\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4232046604156494\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3442468643188477\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3925578594207764\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4020345211029053\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3826937675476074\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3782386779785156\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.384366035461426\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4101011753082275\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.398698329925537\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.385953903198242\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3798162937164307\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3824567794799805\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4342849254608154\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3681533336639404\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4239468574523926\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4376134872436523\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.365614652633667\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.364898204803467\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3623058795928955\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3809938430786133\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3701040744781494\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3980555534362793\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4336767196655273\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.4210941791534424\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.419264078140259\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4302706718444824\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.403059482574463\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.386019706726074\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.393005609512329\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3666036128997803\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4251866340637207\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3729588985443115\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.399674892425537\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3428080081939697\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.411102771759033\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3926730155944824\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.395935297012329\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3604953289031982\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3584508895874023\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.417656421661377\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4394145011901855\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3827130794525146\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.360752582550049\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3475568294525146\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3716681003570557\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.390986204147339\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3862364292144775\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.390181303024292\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.414486885070801\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4064457416534424\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4190545082092285\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.375464677810669\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4048962593078613\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3792057037353516\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.441877841949463\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3626065254211426\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3840019702911377\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.378432035446167\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.379617929458618\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3881781101226807\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.334756374359131\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.396932363510132\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3983333110809326\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.370990037918091\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.438483715057373\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3955867290496826\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.419589042663574\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.368389368057251\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.365917205810547\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.383286476135254\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.38931941986084\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4227733612060547\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4074747562408447\n",
      "Training accuracy 0.09765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 20/36 [1:17:30<1:01:28, 230.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.412904106080532\n",
      "Training loss: 2.3727078437805176\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.385611057281494\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3778014183044434\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4011013507843018\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3803751468658447\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3608486652374268\n",
      "Training accuracy 0.15625\n",
      "Training loss: 2.3720791339874268\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3949062824249268\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.435831308364868\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.435736894607544\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4242050647735596\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4141921997070312\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.342853307723999\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.369436264038086\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3952364921569824\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4294097423553467\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3520798683166504\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.4056451320648193\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3993608951568604\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3670833110809326\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4369187355041504\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3993911743164062\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3461503982543945\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3960933685302734\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3855698108673096\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.385279893875122\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3698160648345947\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.40216064453125\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.42669415473938\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.38877272605896\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.424334764480591\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4220783710479736\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3861095905303955\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.37727427482605\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3925533294677734\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4038240909576416\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3901443481445312\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4310142993927\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3817012310028076\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3983230590820312\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3910250663757324\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.398061752319336\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.452296257019043\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3703198432922363\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.375075340270996\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3807754516601562\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3836467266082764\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.378497838973999\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.405571460723877\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.383674383163452\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4025168418884277\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.390058994293213\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4172005653381348\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.34454345703125\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.368950605392456\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.375746726989746\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.422030210494995\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.403841018676758\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3941965103149414\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.410304546356201\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.361976146697998\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.413552761077881\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4187283515930176\n",
      "Training accuracy 0.05078125\n",
      "Training loss: 2.390021324157715\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3661277294158936\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3921427726745605\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.356700897216797\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.451587677001953\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3496248722076416\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3686892986297607\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3743937015533447\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3488731384277344\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3648734092712402\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.388956308364868\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4251837730407715\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4092864990234375\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3769617080688477\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4087753295898438\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.409176826477051\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.331204414367676\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3851239681243896\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3942155838012695\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.399240255355835\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3512160778045654\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.335744857788086\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.404982566833496\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3437089920043945\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4237990379333496\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.399362325668335\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4137179851531982\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4060442447662354\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.376685380935669\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.405294895172119\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3954129219055176\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.447944402694702\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4247426986694336\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4090945720672607\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.332322597503662\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.397301197052002\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3519515991210938\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4323980808258057\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.366577386856079\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.405266761779785\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3725268840789795\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.408407688140869\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3528192043304443\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3943891525268555\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.33717942237854\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.406250238418579\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3578360080718994\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3993401527404785\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.415755033493042\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.40059757232666\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.359518527984619\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.366091728210449\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.386016368865967\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.381622076034546\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.323479175567627\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.430854082107544\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.424677848815918\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4275684356689453\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.4346213340759277\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.384819746017456\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4158060550689697\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3745079040527344\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3915750980377197\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4125254154205322\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4022433757781982\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3722100257873535\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3929853439331055\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4066309928894043\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.376128673553467\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4233248233795166\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.394474983215332\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.397120952606201\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3725550174713135\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.421262502670288\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4141335487365723\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.423264741897583\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.433638095855713\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3889882564544678\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4122071266174316\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.393305778503418\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3890798091888428\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.401423454284668\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.450087785720825\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4067251682281494\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4138941764831543\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3929083347320557\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4040493965148926\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4152116775512695\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.371093273162842\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.340949773788452\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.4008822441101074\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.428084135055542\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3468456268310547\n",
      "Training accuracy 0.1640625\n",
      "Training loss: 2.3433423042297363\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4166080951690674\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3964953422546387\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4086623191833496\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3871214389801025\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4375429153442383\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3793702125549316\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.385948657989502\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4209511280059814\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4505481719970703\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3859972953796387\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3985228538513184\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3978567123413086\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4038145542144775\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3616223335266113\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3994174003601074\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3782150745391846\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3966660499572754\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3523664474487305\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3945870399475098\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4160635471343994\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4065816402435303\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.385942220687866\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3626575469970703\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3709206581115723\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.333174228668213\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3810343742370605\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.399521827697754\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.319352865219116\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.382439613342285\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3761253356933594\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3874974250793457\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3879153728485107\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.389249324798584\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3739540576934814\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3848724365234375\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4364874362945557\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3642311096191406\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3722140789031982\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.441831350326538\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.41766619682312\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4317405223846436\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.38667631149292\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3497984409332275\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.348893642425537\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3239481449127197\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.402831554412842\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.351318836212158\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.390040874481201\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.415682077407837\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4230458736419678\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.410696506500244\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3864777088165283\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4044811725616455\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3492252826690674\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.447310447692871\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.391120672225952\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.395623207092285\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3598639965057373\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.386449098587036\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.380537986755371\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3845131397247314\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3469719886779785\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3827922344207764\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3862781524658203\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4383959770202637\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.40974760055542\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.396897792816162\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4036223888397217\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.39017915725708\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4033148288726807\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.36822772026062\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.411687135696411\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.440371513366699\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3328306674957275\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.340834379196167\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.401771306991577\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.390043258666992\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3510541915893555\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3828976154327393\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.383662462234497\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3704349994659424\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3530871868133545\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.370516538619995\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.370091199874878\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4167959690093994\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.427029609680176\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4013454914093018\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3876214027404785\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3907454013824463\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3822219371795654\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3875138759613037\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.34653377532959\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4053587913513184\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.372013807296753\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.373086452484131\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4224157333374023\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4134321212768555\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3654372692108154\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3941683769226074\n",
      "Training accuracy 0.09375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 21/36 [1:21:20<57:39, 230.62s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129040967673063\n",
      "Training loss: 2.4143686294555664\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3689465522766113\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3731441497802734\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.408092498779297\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.368138313293457\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4461708068847656\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.358036756515503\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4089999198913574\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3967032432556152\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4271655082702637\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4108376502990723\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.415889024734497\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.372103452682495\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3899059295654297\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3839187622070312\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.411339282989502\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3760175704956055\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3503341674804688\n",
      "Training accuracy 0.16015625\n",
      "Training loss: 2.43208909034729\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.411975145339966\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.38303279876709\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.428330659866333\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4279775619506836\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4248015880584717\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3835794925689697\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.379021644592285\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.355635643005371\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3820414543151855\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3631088733673096\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.375664710998535\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.377483606338501\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3735055923461914\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4043290615081787\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3852336406707764\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3531320095062256\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3827500343322754\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3734214305877686\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.355086088180542\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.423499822616577\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.425178289413452\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3761043548583984\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4390804767608643\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.391739845275879\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.410928964614868\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.366424322128296\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.389542818069458\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.400439500808716\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.399648666381836\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.368533134460449\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4137864112854004\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3690688610076904\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.406360387802124\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.402707576751709\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.413011074066162\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3890397548675537\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3803598880767822\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3969972133636475\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3952372074127197\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4312195777893066\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.408658504486084\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4130606651306152\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.373708486557007\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.347946882247925\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3828887939453125\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3894426822662354\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.381343126296997\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.367316246032715\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3500349521636963\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3613955974578857\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4501218795776367\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3891255855560303\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3766090869903564\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.347992420196533\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.3762872219085693\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.376840591430664\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.404315233230591\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3974452018737793\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4001452922821045\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.398838996887207\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4054970741271973\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.415942668914795\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3388209342956543\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3869266510009766\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.3314619064331055\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3951940536499023\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.391188621520996\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.377816915512085\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.377042531967163\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4226737022399902\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4008119106292725\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.379390001296997\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3747007846832275\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4064435958862305\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.399278163909912\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.420363664627075\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3788623809814453\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3864951133728027\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.394104480743408\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3656511306762695\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.390540599822998\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3730692863464355\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3658487796783447\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3876454830169678\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4284844398498535\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3882901668548584\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3973541259765625\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.396296739578247\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4120090007781982\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3585896492004395\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3728716373443604\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4045441150665283\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.41646146774292\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4203217029571533\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3709354400634766\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.407181739807129\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.393831729888916\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.375413656234741\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3823819160461426\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.40797758102417\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.39664626121521\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.435884475708008\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3938910961151123\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4183080196380615\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.367048501968384\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.41974139213562\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.385518789291382\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.407003164291382\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3930628299713135\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.381028890609741\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3922996520996094\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3896307945251465\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3657729625701904\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.430172920227051\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3983073234558105\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.417614459991455\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.409273862838745\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.358955144882202\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3460328578948975\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3749492168426514\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4394116401672363\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.386436700820923\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3567609786987305\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4125349521636963\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.391505718231201\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.344881772994995\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3572354316711426\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.4073262214660645\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4165823459625244\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.398242712020874\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.421660900115967\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3920702934265137\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.363304615020752\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.429868698120117\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3946502208709717\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.37941312789917\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.455415725708008\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.399259090423584\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.403451681137085\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.373791217803955\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.422161817550659\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4074599742889404\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.396688938140869\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3761887550354004\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4309353828430176\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3804783821105957\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4055962562561035\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4086904525756836\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3889496326446533\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.447141408920288\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.385166645050049\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3856265544891357\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3678247928619385\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.369371175765991\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4034855365753174\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3358161449432373\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.331380605697632\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3882126808166504\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.428346633911133\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4234375953674316\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.421905994415283\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3883233070373535\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.439544677734375\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3745150566101074\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4076273441314697\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3625450134277344\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3998141288757324\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.396663188934326\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4256701469421387\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4193854331970215\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.429234504699707\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3626086711883545\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3698928356170654\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3771488666534424\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.4095330238342285\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3736252784729004\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3929026126861572\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3561606407165527\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.373734474182129\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4041709899902344\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.408918619155884\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.366170644760132\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4025464057922363\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.416170597076416\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3828511238098145\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.342698097229004\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.3717002868652344\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.353140115737915\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3604679107666016\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.382913827896118\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.369654655456543\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.397631883621216\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4183433055877686\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3973095417022705\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.411067485809326\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3770601749420166\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3565142154693604\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4340262413024902\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.405052661895752\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3895251750946045\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.427849769592285\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.369023561477661\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.420522928237915\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.345578193664551\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.388766050338745\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4196600914001465\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3471715450286865\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.397359848022461\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4000139236450195\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4056124687194824\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3949124813079834\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4008467197418213\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3956282138824463\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.4267797470092773\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.396818161010742\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.39758038520813\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3663601875305176\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3973779678344727\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.365314483642578\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4270286560058594\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3978710174560547\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3943021297454834\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3899571895599365\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.344008445739746\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3616156578063965\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.369173526763916\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3903727531433105\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4073190689086914\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.359682083129883\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.369508743286133\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3874683380126953\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3558907508850098\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.408384323120117\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.385716199874878\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3983213901519775\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.439814805984497\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4563722610473633\n",
      "Training accuracy 0.05859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 22/36 [1:25:13<53:56, 231.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041209816933\n",
      "Training loss: 2.4111344814300537\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.413787364959717\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.373202085494995\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.368494987487793\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3956637382507324\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4138333797454834\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.419637680053711\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.415839433670044\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4102890491485596\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3729538917541504\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3718390464782715\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.47499680519104\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4156973361968994\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.430328130722046\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.368263006210327\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.3983612060546875\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3574411869049072\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4252877235412598\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.389404773712158\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4232959747314453\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.316596746444702\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3674678802490234\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3900110721588135\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.390434980392456\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3998265266418457\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.332475185394287\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3700404167175293\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.404214382171631\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.377635955810547\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.380767822265625\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3590052127838135\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4260213375091553\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.413902759552002\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3727686405181885\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3779282569885254\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.389484167098999\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.378617763519287\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.381553888320923\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3932735919952393\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.405453681945801\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3900578022003174\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4110827445983887\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.409489870071411\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4011332988739014\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3968701362609863\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.407160758972168\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.398069381713867\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.395970582962036\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.425906181335449\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.431157350540161\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.414782762527466\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3715555667877197\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.377521514892578\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3935182094573975\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3898329734802246\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.417451858520508\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.407395362854004\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3850295543670654\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.416841983795166\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.435279607772827\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.372544288635254\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.429581642150879\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.362612724304199\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.406755208969116\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4350411891937256\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3908302783966064\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3575146198272705\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4251339435577393\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.403090000152588\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4014668464660645\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.369065999984741\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3651251792907715\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.382869243621826\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3726513385772705\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3807456493377686\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.353673219680786\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.332648277282715\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.431562900543213\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.405566692352295\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3599865436553955\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3508718013763428\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4305367469787598\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4640560150146484\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.431950330734253\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3462767601013184\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3911967277526855\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3965275287628174\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.375265598297119\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3666939735412598\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.358290910720825\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3834168910980225\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3584461212158203\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.394710063934326\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3866844177246094\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4155187606811523\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4035942554473877\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4307687282562256\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3772318363189697\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.348184585571289\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3926520347595215\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.42182993888855\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.417876720428467\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3819916248321533\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.419790267944336\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3894758224487305\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4103946685791016\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3865199089050293\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4045097827911377\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3854682445526123\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.368232250213623\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4036965370178223\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.372283935546875\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.351016044616699\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.394444704055786\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4230432510375977\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.422065019607544\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.392434597015381\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3668344020843506\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4181618690490723\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4243669509887695\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.382209539413452\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4168052673339844\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.395874261856079\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4187684059143066\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3770556449890137\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.384000301361084\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4333112239837646\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.371936321258545\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.369312047958374\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4175381660461426\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3506689071655273\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3750290870666504\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4355387687683105\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.426825761795044\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3540830612182617\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3946895599365234\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.415303945541382\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4284253120422363\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.433833599090576\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.366352081298828\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3977890014648438\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3801281452178955\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.396019220352173\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3608198165893555\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4451122283935547\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4006025791168213\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3739871978759766\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.399540662765503\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3698642253875732\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4296631813049316\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3648083209991455\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.377845287322998\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.356360912322998\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3494372367858887\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.404287338256836\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.392725944519043\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.385998249053955\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.375675678253174\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3805224895477295\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.37283992767334\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.449171304702759\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3979244232177734\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.383326768875122\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.333317518234253\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3561954498291016\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3652920722961426\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.35160493850708\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.38877534866333\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.373887538909912\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.389341115951538\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4163753986358643\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3998594284057617\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3975164890289307\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3708744049072266\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.412222146987915\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3644728660583496\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3932039737701416\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3925347328186035\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4177701473236084\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.381270170211792\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4070329666137695\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3964016437530518\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.409364700317383\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4432029724121094\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4066951274871826\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.418962240219116\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.401214599609375\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.417958974838257\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.383248805999756\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.41929030418396\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.396196126937866\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.406836986541748\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4248509407043457\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3561456203460693\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3782389163970947\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.373469352722168\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3954830169677734\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.348698616027832\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.370086669921875\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4096457958221436\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.408125400543213\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3831377029418945\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3806676864624023\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3971357345581055\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3930811882019043\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.394989252090454\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.379772663116455\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3479318618774414\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3562498092651367\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.363149404525757\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3953990936279297\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.369032144546509\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4685442447662354\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.395781993865967\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.405884027481079\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3963944911956787\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3727290630340576\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.391503095626831\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3995401859283447\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.373255729675293\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.353504180908203\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4423069953918457\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.387105941772461\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.347684621810913\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.389997720718384\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3863160610198975\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.372499465942383\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3911502361297607\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.39064884185791\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4057390689849854\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.363384962081909\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.405836820602417\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.390547037124634\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.392557382583618\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3349666595458984\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.407696008682251\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.378164291381836\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3665874004364014\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3979969024658203\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.411707639694214\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.369875907897949\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.351384401321411\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3789596557617188\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.38346529006958\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.404533863067627\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.407667398452759\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3692946434020996\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4081223011016846\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3860230445861816\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4284753799438477\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4481358528137207\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.378025770187378\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.36592173576355\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4072728157043457\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.368134021759033\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3306376934051514\n",
      "Training accuracy 0.12109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 23/36 [1:29:02<49:58, 230.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129040902480483\n",
      "Training loss: 2.380373477935791\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4070844650268555\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3670432567596436\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.399022102355957\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4119369983673096\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3501217365264893\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.431757688522339\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.379013776779175\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3830015659332275\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.428257942199707\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3749632835388184\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.390536308288574\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3957314491271973\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.361889362335205\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3982930183410645\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4262237548828125\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4002437591552734\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3960368633270264\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4010417461395264\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4312572479248047\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3689374923706055\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4311037063598633\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3907852172851562\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3476688861846924\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3892674446105957\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.428436279296875\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4112319946289062\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3581533432006836\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3593881130218506\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.43349027633667\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4094367027282715\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.388183355331421\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3521924018859863\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4569544792175293\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3645894527435303\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.393876791000366\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.412169933319092\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.352198839187622\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3920321464538574\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.405435800552368\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4183995723724365\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.390822410583496\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4338114261627197\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.369567394256592\n",
      "Training accuracy 0.15625\n",
      "Training loss: 2.431485652923584\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4207472801208496\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.317934036254883\n",
      "Training accuracy 0.15625\n",
      "Training loss: 2.3904948234558105\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.382382392883301\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3970561027526855\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.387012481689453\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.391155481338501\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.393146276473999\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4004437923431396\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3629112243652344\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.367647647857666\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3573713302612305\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3868825435638428\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.41355299949646\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.382862091064453\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.381242513656616\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4250190258026123\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3747775554656982\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.44687819480896\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.393996238708496\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.383315086364746\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.380253314971924\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4082295894622803\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.392789125442505\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3909413814544678\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4336938858032227\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4002091884613037\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.43265438079834\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.450193405151367\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3833889961242676\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.405921697616577\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.38018536567688\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3970656394958496\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.447303533554077\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.382240056991577\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4059693813323975\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3863253593444824\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4237029552459717\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.371583938598633\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3894734382629395\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4051103591918945\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.403205633163452\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.375497579574585\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.421588659286499\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3464627265930176\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.379326581954956\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.413020133972168\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.38563871383667\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.390005588531494\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.377189874649048\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.374450206756592\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.424614429473877\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3922433853149414\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3693742752075195\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3907909393310547\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.406510353088379\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.401707887649536\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.353471517562866\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.389134407043457\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.411773920059204\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3672971725463867\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.348243474960327\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4027483463287354\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.371673822402954\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3765923976898193\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.406536340713501\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4167122840881348\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3618721961975098\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.419365167617798\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.402492046356201\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4091708660125732\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4322309494018555\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4053173065185547\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3957161903381348\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3769354820251465\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3948097229003906\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.400695562362671\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3963863849639893\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.376321315765381\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4053220748901367\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.35648250579834\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4393579959869385\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4266669750213623\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3800711631774902\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.391023874282837\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.408432722091675\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.363276958465576\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.325528621673584\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4198505878448486\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.46246075630188\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3942224979400635\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3544692993164062\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3900694847106934\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3638172149658203\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.406473159790039\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3977365493774414\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.372795820236206\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4115567207336426\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4116690158843994\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.397961378097534\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3749985694885254\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.388307571411133\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4416863918304443\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.42063570022583\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3777289390563965\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3596365451812744\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3679118156433105\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3825554847717285\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3418800830841064\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3835036754608154\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.380175828933716\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3791632652282715\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.382952928543091\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.383967161178589\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.368025302886963\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.434783935546875\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3909192085266113\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4030261039733887\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.443509578704834\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.38883113861084\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.404006004333496\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3945395946502686\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.343949556350708\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.4304332733154297\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3998525142669678\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3910179138183594\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.369955539703369\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.39517879486084\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.409834861755371\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4173011779785156\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.433767318725586\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3629016876220703\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3706631660461426\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3823184967041016\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4598119258880615\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.418578624725342\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3504638671875\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3709828853607178\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3735108375549316\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3333332538604736\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.417667865753174\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.376753568649292\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.371701717376709\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3800530433654785\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.384808301925659\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4203040599823\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3670735359191895\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.40250301361084\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3915863037109375\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.374497413635254\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.429567575454712\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.347679615020752\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4277865886688232\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.344169855117798\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.441992998123169\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.378074884414673\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.370779037475586\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3701839447021484\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.342735528945923\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3939106464385986\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.418590545654297\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3773365020751953\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3703277111053467\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3473775386810303\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3986406326293945\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3933403491973877\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.390448570251465\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3929388523101807\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3912477493286133\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.342721700668335\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3929603099823\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.382490634918213\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.385143518447876\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3935132026672363\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.346014976501465\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3593289852142334\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.365058422088623\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3821828365325928\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.356311559677124\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3938584327697754\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4290971755981445\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.356823444366455\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.410658597946167\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3989980220794678\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3844430446624756\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3774633407592773\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.362091541290283\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.415330648422241\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4417898654937744\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.446136236190796\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.401109457015991\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.442282199859619\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.415640354156494\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3863370418548584\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3838155269622803\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.397700309753418\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3510844707489014\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.413144826889038\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.369736909866333\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.391288995742798\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.417509078979492\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.401329517364502\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.366368532180786\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.408576250076294\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.397333860397339\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3528635501861572\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4308621883392334\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3913328647613525\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.392838478088379\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.366812229156494\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.354041576385498\n",
      "Training accuracy 0.13671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 24/36 [1:32:51<46:00, 230.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129040874540806\n",
      "Training loss: 2.390667200088501\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.407322406768799\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3597350120544434\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3949615955352783\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.389415979385376\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3912746906280518\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.435940742492676\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3596763610839844\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4064457416534424\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.385150194168091\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.437556743621826\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.373560667037964\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.33512806892395\n",
      "Training accuracy 0.1640625\n",
      "Training loss: 2.3869941234588623\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3943278789520264\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3870010375976562\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.393670082092285\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3828258514404297\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.407017230987549\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4437153339385986\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3891663551330566\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.380411386489868\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.393291473388672\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4449236392974854\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.414604663848877\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4643161296844482\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.383635997772217\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4534566402435303\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.329864025115967\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.408632278442383\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.372305393218994\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.387470245361328\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.397365093231201\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.392914056777954\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3620030879974365\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.390746831893921\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3854422569274902\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.389932870864868\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3620522022247314\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4067118167877197\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.377937078475952\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3816721439361572\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4242923259735107\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.399374485015869\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4081270694732666\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3168976306915283\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3933770656585693\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3823041915893555\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.399785041809082\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.387882947921753\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3801534175872803\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3757011890411377\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4061875343322754\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.371187686920166\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4002552032470703\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.376417875289917\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3833746910095215\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.442014455795288\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3568079471588135\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3916523456573486\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.348555564880371\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3951914310455322\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.412472724914551\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.410816192626953\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3737170696258545\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3321003913879395\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3503785133361816\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4718573093414307\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.406642436981201\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4006378650665283\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.412428379058838\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.392733335494995\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3760666847229004\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3728067874908447\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4002718925476074\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3297383785247803\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4007041454315186\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4151268005371094\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4050347805023193\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4126901626586914\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.384305238723755\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.401536464691162\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.436776876449585\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.403815746307373\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.368767738342285\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3892853260040283\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3444557189941406\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3991928100585938\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3864145278930664\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.39037823677063\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.375499963760376\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.419037342071533\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4069807529449463\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4210052490234375\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3793327808380127\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3931639194488525\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.392709255218506\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3273983001708984\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3489112854003906\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.396230459213257\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3640902042388916\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3596887588500977\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.425738573074341\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.352810859680176\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.353693723678589\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.388416290283203\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3872857093811035\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4146182537078857\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3617396354675293\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4302966594696045\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.380141258239746\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.414747953414917\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3824965953826904\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4500067234039307\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3870842456817627\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.379857063293457\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3789453506469727\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.375033140182495\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.381495952606201\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3842973709106445\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3960866928100586\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3858649730682373\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3395254611968994\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3693501949310303\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4187862873077393\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4327170848846436\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3831260204315186\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.372589349746704\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.432581901550293\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.379873514175415\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4266374111175537\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3653576374053955\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.388672113418579\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.366016149520874\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4252326488494873\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.406461238861084\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.387523651123047\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3617610931396484\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.360417604446411\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4234633445739746\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.361389636993408\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3778140544891357\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.395460605621338\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3880255222320557\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3855831623077393\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3775320053100586\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.413911819458008\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3869755268096924\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.403890609741211\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3666832447052\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.440497636795044\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.422942638397217\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3960461616516113\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3678903579711914\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.374512195587158\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4251978397369385\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.423570394515991\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3950369358062744\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.39902663230896\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4437460899353027\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.375368118286133\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3976964950561523\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3967995643615723\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3547143936157227\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4164605140686035\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4248299598693848\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3679730892181396\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3627138137817383\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.376432418823242\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3656680583953857\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4084110260009766\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.356686592102051\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4158406257629395\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3898868560791016\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4465794563293457\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3628146648406982\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4292471408843994\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.412808418273926\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4073731899261475\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3941774368286133\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.398684501647949\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.381349802017212\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.379730224609375\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4433670043945312\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3828811645507812\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.392285108566284\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3451650142669678\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3639848232269287\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4206020832061768\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3752071857452393\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4221794605255127\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3951284885406494\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.39455509185791\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3333866596221924\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3890554904937744\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.385918140411377\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.38077974319458\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3703927993774414\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3315932750701904\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.374703884124756\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.411698579788208\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3589723110198975\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3788185119628906\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3898708820343018\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.355851650238037\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4027576446533203\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3950605392456055\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3755385875701904\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3972980976104736\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3711259365081787\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4055562019348145\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3672099113464355\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4111592769622803\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3623080253601074\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4168765544891357\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.400899648666382\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4027068614959717\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.40378475189209\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3678832054138184\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.353375196456909\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.432370901107788\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3822977542877197\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.407175302505493\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3943543434143066\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3772685527801514\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3627946376800537\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4261720180511475\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.402336359024048\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.4015491008758545\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3742053508758545\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3865714073181152\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.437370538711548\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.355332851409912\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3930094242095947\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3902370929718018\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4260356426239014\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.390052556991577\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3557071685791016\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.386399745941162\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.360771656036377\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4388628005981445\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.45417857170105\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3950562477111816\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4268784523010254\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4073076248168945\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4251790046691895\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3406119346618652\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.394688129425049\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4251866340637207\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3966169357299805\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4031896591186523\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4224798679351807\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.382930040359497\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.391756772994995\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.419506788253784\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.395796537399292\n",
      "Training accuracy 0.08203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 25/36 [1:36:43<42:15, 230.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041135311127\n",
      "Training loss: 2.374110460281372\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3763296604156494\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3726658821105957\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3707141876220703\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3718783855438232\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4025280475616455\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3819544315338135\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3764734268188477\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.368384838104248\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.34165358543396\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3886735439300537\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.362081289291382\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.455493688583374\n",
      "Training accuracy 0.046875\n",
      "Training loss: 2.4281702041625977\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.370002031326294\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3722944259643555\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4237871170043945\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4404687881469727\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4012818336486816\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3985276222229004\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.395859479904175\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3910720348358154\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.391587972640991\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3984835147857666\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.44718337059021\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4083306789398193\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.406418561935425\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.390383243560791\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3881704807281494\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3495728969573975\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4461874961853027\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4054040908813477\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.393519163131714\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.433718681335449\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3913347721099854\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.387700080871582\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4083964824676514\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.416347026824951\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3908543586730957\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.410132884979248\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.36857008934021\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3833630084991455\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3342127799987793\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4220447540283203\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4270918369293213\n",
      "Training accuracy 0.04296875\n",
      "Training loss: 2.3975045680999756\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.400930404663086\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4269399642944336\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4284374713897705\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.43062162399292\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4159927368164062\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.405301570892334\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3987414836883545\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.412362813949585\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3485918045043945\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.415710687637329\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3989858627319336\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4033796787261963\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.412363052368164\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.415217876434326\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3843133449554443\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4024975299835205\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3548851013183594\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3632264137268066\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3698058128356934\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.3485960960388184\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.346590518951416\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.437843084335327\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.3930485248565674\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3937454223632812\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3819026947021484\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.408632755279541\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3955047130584717\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.371835708618164\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3931918144226074\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3354179859161377\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.336172580718994\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.3522441387176514\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.410861015319824\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4363627433776855\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.379849672317505\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.410621404647827\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.345781087875366\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3817930221557617\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.385653495788574\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4121172428131104\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.43338680267334\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4222259521484375\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3958749771118164\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3803088665008545\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.410966634750366\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4006755352020264\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3773953914642334\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.4322707653045654\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3655004501342773\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.402116537094116\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.434694766998291\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3967654705047607\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.441056251525879\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.423492908477783\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4287166595458984\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3406736850738525\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3941328525543213\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.341244697570801\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3621277809143066\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.393707752227783\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3689701557159424\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3890249729156494\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3852243423461914\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.392158031463623\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3783578872680664\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.366062879562378\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3837521076202393\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3748843669891357\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4596431255340576\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.425382137298584\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4170937538146973\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4032883644104004\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.383157730102539\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4298934936523438\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.385146379470825\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4255125522613525\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.436232089996338\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3832316398620605\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4020328521728516\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3999335765838623\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3964109420776367\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3712072372436523\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.342930555343628\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3394625186920166\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3985607624053955\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.373140573501587\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4183719158172607\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.382563591003418\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.373558521270752\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.367649555206299\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.371148109436035\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.364358901977539\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.423903226852417\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3643953800201416\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4418699741363525\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.365523099899292\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3931069374084473\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.364443302154541\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4019198417663574\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.419689655303955\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.382331371307373\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3528943061828613\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.406128406524658\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.407865524291992\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4220988750457764\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.379180431365967\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3865325450897217\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3747735023498535\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4473512172698975\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4105300903320312\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.393594980239868\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.401625394821167\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3548965454101562\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.37727689743042\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4041364192962646\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.390717029571533\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4175760746002197\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3910207748413086\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3829636573791504\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.405151605606079\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3054909706115723\n",
      "Training accuracy 0.1640625\n",
      "Training loss: 2.3683817386627197\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3951475620269775\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.407761812210083\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3661117553710938\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3386597633361816\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3774256706237793\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.367579936981201\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.38885498046875\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3911097049713135\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.384106397628784\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3815317153930664\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.362321138381958\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4253623485565186\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.38704514503479\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3976142406463623\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4332878589630127\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4368631839752197\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3687682151794434\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.420679807662964\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.339008331298828\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4103853702545166\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.411653995513916\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3995466232299805\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.338923454284668\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.373566150665283\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.380481719970703\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.417268991470337\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3728866577148438\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.374223232269287\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.434047222137451\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3796777725219727\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3545496463775635\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.373243570327759\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4174041748046875\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.374075412750244\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.38301157951355\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.441821336746216\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.370784044265747\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3652687072753906\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4268321990966797\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.373173236846924\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3953583240509033\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3873074054718018\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3861422538757324\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3318593502044678\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.361466884613037\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.417496919631958\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.411916971206665\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3525476455688477\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4337899684906006\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3791253566741943\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.376418113708496\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3685810565948486\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.342172384262085\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4268078804016113\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3533871173858643\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3694465160369873\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.402215003967285\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.376023292541504\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3620829582214355\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.347148895263672\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3868637084960938\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.393672466278076\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4108362197875977\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4412178993225098\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4089555740356445\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3897173404693604\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.393946409225464\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4142906665802\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3518078327178955\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3894784450531006\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3817474842071533\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4031031131744385\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3414146900177\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.379335403442383\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3673646450042725\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3884143829345703\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3994975090026855\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.35752272605896\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4256112575531006\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3808109760284424\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3954856395721436\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4074277877807617\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.407367706298828\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3104488849639893\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.413742780685425\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4004886150360107\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.432769775390625\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.37550687789917\n",
      "Training accuracy 0.09765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 26/36 [1:40:33<38:23, 230.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129040958359838\n",
      "Training loss: 2.3767316341400146\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3963654041290283\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.351099729537964\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3877615928649902\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4232466220855713\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4266765117645264\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.367340326309204\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3962392807006836\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4319589138031006\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3760452270507812\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4399850368499756\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3910505771636963\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.405195951461792\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4149253368377686\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3986141681671143\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3577072620391846\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3939321041107178\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4084365367889404\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.442812442779541\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3576111793518066\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3663527965545654\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.345536947250366\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.375459909439087\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.44027042388916\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.408052921295166\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3855485916137695\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3475825786590576\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.348414182662964\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4247183799743652\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3920342922210693\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.366583824157715\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3900442123413086\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3838071823120117\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3779044151306152\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.413600444793701\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3943986892700195\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.399482011795044\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3779563903808594\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3749265670776367\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4140989780426025\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4098408222198486\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4033491611480713\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3830504417419434\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3721859455108643\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.373730421066284\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.446246862411499\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.374119281768799\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4354751110076904\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.364964485168457\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.382305383682251\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4064955711364746\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.378264904022217\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3905105590820312\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.370934009552002\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3804125785827637\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.364617347717285\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4345808029174805\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3799045085906982\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.38704252243042\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.405665874481201\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3289597034454346\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.380409002304077\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.390570640563965\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.350555658340454\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.432506561279297\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.384641170501709\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3793883323669434\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4227216243743896\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.388092279434204\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.383371114730835\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3937935829162598\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3888754844665527\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.379368782043457\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4664158821105957\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.454702138900757\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.353455066680908\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3899295330047607\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4440197944641113\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.388362169265747\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4210774898529053\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3724377155303955\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.393845319747925\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4434609413146973\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4200990200042725\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4148170948028564\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3955390453338623\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.411691427230835\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.388897180557251\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.42989182472229\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3923048973083496\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.361039161682129\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3722269535064697\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.356968402862549\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3444864749908447\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.380985975265503\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3724420070648193\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3679635524749756\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.410090446472168\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4007160663604736\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4248111248016357\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3987793922424316\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3831868171691895\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3518307209014893\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3576788902282715\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.407367467880249\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.384779453277588\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.368587017059326\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4272727966308594\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3958628177642822\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3708674907684326\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.359539270401001\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.465831756591797\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.395278215408325\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.40153431892395\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.393256187438965\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4038777351379395\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.431560516357422\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3669018745422363\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3736181259155273\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3778815269470215\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.361659049987793\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.381054639816284\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.404287099838257\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3867692947387695\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3937301635742188\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.414027214050293\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.406611919403076\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4144325256347656\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.358537435531616\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.361929416656494\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.372089147567749\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4262266159057617\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3685193061828613\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4165291786193848\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3694918155670166\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.434148073196411\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.41851806640625\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.416465997695923\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.390904664993286\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4036200046539307\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.382875442504883\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.402048349380493\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.414342164993286\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3772335052490234\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3652570247650146\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.408731460571289\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4126815795898438\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.372061014175415\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3693575859069824\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3881025314331055\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3776280879974365\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3231050968170166\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.401884078979492\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.388601064682007\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.396552801132202\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3860270977020264\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.397799015045166\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.416881561279297\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.383737087249756\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4119386672973633\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4105260372161865\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3934600353240967\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3919034004211426\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3491287231445312\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4139583110809326\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4111108779907227\n",
      "Training accuracy 0.046875\n",
      "Training loss: 2.3268985748291016\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4009745121002197\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.409971237182617\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.372511386871338\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3292553424835205\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.417186975479126\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4319369792938232\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3662967681884766\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3566510677337646\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3859798908233643\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3912177085876465\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.412163734436035\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3880887031555176\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3685543537139893\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3935813903808594\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.409306049346924\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.428422451019287\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.38478422164917\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3912129402160645\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.384960412979126\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.414505958557129\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4229581356048584\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3504927158355713\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.391887903213501\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4227943420410156\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.425673723220825\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3764917850494385\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.390239953994751\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.393585205078125\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3778135776519775\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.381507635116577\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4005074501037598\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3744821548461914\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4317562580108643\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.394973039627075\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.409829616546631\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.417782783508301\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.413297414779663\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3972339630126953\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3983852863311768\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.370849609375\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.378091335296631\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.392014503479004\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.413939952850342\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.355372190475464\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3841285705566406\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3879945278167725\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3698132038116455\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3507883548736572\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3913369178771973\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3881022930145264\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.359360933303833\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3939027786254883\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.372677803039551\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3939483165740967\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3909285068511963\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4333293437957764\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.371182918548584\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.397150754928589\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.386568307876587\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.397902488708496\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3788459300994873\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3617630004882812\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3924007415771484\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.400829553604126\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3900485038757324\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.422952890396118\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4460582733154297\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3482842445373535\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4073338508605957\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.367565155029297\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3980588912963867\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.381582736968994\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.41538143157959\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4260411262512207\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.381943941116333\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4318580627441406\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3911354541778564\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4011833667755127\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4216272830963135\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.354583263397217\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3745672702789307\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.427283525466919\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4187965393066406\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4047558307647705\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3750078678131104\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3305492401123047\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.377830982208252\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3307669162750244\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.394104480743408\n",
      "Training accuracy 0.08984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 27/36 [1:44:22<34:31, 230.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.412904099561274\n",
      "Training loss: 2.4091269969940186\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.335641622543335\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.394653558731079\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4447219371795654\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3964102268218994\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.412968635559082\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.372748613357544\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3762354850769043\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3965063095092773\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3664889335632324\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.413158416748047\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.332582950592041\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3719027042388916\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.426689624786377\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.385025978088379\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3512208461761475\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.431014060974121\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3776893615722656\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.397022008895874\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.368079900741577\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.366607666015625\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.38147234916687\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3633522987365723\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4085965156555176\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3992919921875\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3864665031433105\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.379939317703247\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.393218517303467\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.418264389038086\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3643500804901123\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.401582956314087\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4034156799316406\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.371189832687378\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.367007255554199\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3949005603790283\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.413978099822998\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3745670318603516\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.398777723312378\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.387852191925049\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3484866619110107\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4046945571899414\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3862338066101074\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.414947986602783\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.338413953781128\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.436054229736328\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.368274211883545\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3972127437591553\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.39186954498291\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.390489339828491\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3710267543792725\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.353907346725464\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3996217250823975\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3973228931427\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.400730609893799\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3901546001434326\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.316331624984741\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4171483516693115\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3859732151031494\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3648087978363037\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3943393230438232\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3645808696746826\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.4404590129852295\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.395920991897583\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.427717685699463\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3925585746765137\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.398577928543091\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.390376091003418\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3958613872528076\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3818421363830566\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.395893096923828\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.375091314315796\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4240334033966064\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3607017993927\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3109757900238037\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.441007137298584\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4123146533966064\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.358778476715088\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.391186475753784\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.392946720123291\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4338715076446533\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.392256021499634\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4040040969848633\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.39316725730896\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.356252670288086\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4126858711242676\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3551156520843506\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4005696773529053\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4019856452941895\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3747284412384033\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.400838851928711\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3654603958129883\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3666305541992188\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4184067249298096\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.376762866973877\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3466591835021973\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3637771606445312\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.388371229171753\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.349921703338623\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3979830741882324\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3206682205200195\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4128880500793457\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3684349060058594\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3941128253936768\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.384495496749878\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4066081047058105\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3919930458068848\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3828442096710205\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.393588066101074\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3964502811431885\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.36653995513916\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4067037105560303\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.374626874923706\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.420513153076172\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.376573085784912\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.375683069229126\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.448718786239624\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3991432189941406\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4079525470733643\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.402625322341919\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3992624282836914\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3596537113189697\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4142649173736572\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4080278873443604\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3762600421905518\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.397597551345825\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3903472423553467\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.382260799407959\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3437745571136475\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.35534405708313\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3736727237701416\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.455253839492798\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3795382976531982\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3880984783172607\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4396018981933594\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3802640438079834\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4248857498168945\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3754055500030518\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4056763648986816\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.402364730834961\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.383465051651001\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3961570262908936\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3781850337982178\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.404226779937744\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.410705089569092\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.388176918029785\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4272427558898926\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3572428226470947\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.382054567337036\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3893580436706543\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3798084259033203\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4518468379974365\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3804831504821777\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4068331718444824\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.405686378479004\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3992810249328613\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.356635570526123\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3841185569763184\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4256298542022705\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.360708475112915\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3922019004821777\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3681676387786865\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3597466945648193\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4165091514587402\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.368591785430908\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4155375957489014\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3894541263580322\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3773646354675293\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4298436641693115\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.41469407081604\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.417980194091797\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3786964416503906\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4099273681640625\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.383490800857544\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.420074701309204\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.394397497177124\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3684630393981934\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.384836196899414\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3542351722717285\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.402726650238037\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3886170387268066\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.367431879043579\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.450624942779541\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.417743682861328\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4396750926971436\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3433170318603516\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.418308734893799\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4054558277130127\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.434993028640747\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.375208616256714\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.363586664199829\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.360913038253784\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.411527633666992\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.392944812774658\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.404205799102783\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3889074325561523\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3904945850372314\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4044601917266846\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4197680950164795\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3740427494049072\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.379946231842041\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.343839406967163\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.403074026107788\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.392813205718994\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3158628940582275\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.429555892944336\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.404043197631836\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4243974685668945\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3681836128234863\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4110984802246094\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.388853073120117\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3761417865753174\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4209377765655518\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4366862773895264\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3957104682922363\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4010562896728516\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.426553964614868\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3781516551971436\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4054036140441895\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.38472580909729\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3756537437438965\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3852412700653076\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.387573719024658\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4753968715667725\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.438650369644165\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4025816917419434\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.425069808959961\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3727080821990967\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.333258867263794\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3661396503448486\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4087860584259033\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3887500762939453\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.339385986328125\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4324021339416504\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.402564525604248\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3786544799804688\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.38132643699646\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4039154052734375\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.385403633117676\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.401275634765625\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3642215728759766\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.452965497970581\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4146077632904053\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.381152391433716\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.392500162124634\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4276864528656006\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.397535562515259\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3609986305236816\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.411834716796875\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.392158269882202\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3777832984924316\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.423229694366455\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3799004554748535\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.373183488845825\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.380655527114868\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3861992359161377\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.388185739517212\n",
      "Training accuracy 0.078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 28/36 [1:48:12<30:40, 230.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041396081448\n",
      "Training loss: 2.3746337890625\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.376568555831909\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3761720657348633\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.412809371948242\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.365328788757324\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3568453788757324\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.388591766357422\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3871569633483887\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3315069675445557\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4289002418518066\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.407712936401367\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3995308876037598\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3601396083831787\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.41211199760437\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.414191961288452\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4212169647216797\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3876383304595947\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3575222492218018\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3975396156311035\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3365464210510254\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.408569812774658\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3284754753112793\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.39265775680542\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4106979370117188\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4361681938171387\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3770203590393066\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3661348819732666\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4256272315979004\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.353868007659912\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.366875648498535\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.371041774749756\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4109745025634766\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4211323261260986\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4237287044525146\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.375354766845703\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3650927543640137\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4275479316711426\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4008588790893555\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.375551700592041\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.40254545211792\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.426860809326172\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4027318954467773\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.379939556121826\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.413771390914917\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.371645450592041\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3880465030670166\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.437363624572754\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.4161536693573\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4240164756774902\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.396632194519043\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4077587127685547\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.446056842803955\n",
      "Training accuracy 0.05078125\n",
      "Training loss: 2.3962955474853516\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3533847332000732\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.40006685256958\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4150006771087646\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.381209135055542\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3758492469787598\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.445936918258667\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.392115354537964\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.333336591720581\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3809759616851807\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.38653564453125\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.360084056854248\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.414797067642212\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3823628425598145\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3624019622802734\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3778960704803467\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.323575496673584\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3694632053375244\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4324378967285156\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.373833179473877\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.380601167678833\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3957438468933105\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3632516860961914\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.409522533416748\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3711867332458496\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.349097967147827\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3929603099823\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3640341758728027\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3626785278320312\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4312522411346436\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4182748794555664\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.375572919845581\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.379148483276367\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.387643337249756\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3767924308776855\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3538453578948975\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.441706657409668\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.36738657951355\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.437875747680664\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.392300605773926\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4273760318756104\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.380167245864868\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.387403964996338\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3868765830993652\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3978731632232666\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3858842849731445\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.412729501724243\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.402390241622925\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.416834831237793\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3853039741516113\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.371908664703369\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4316532611846924\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.402461051940918\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4040367603302\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3419189453125\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3387646675109863\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.411259412765503\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.395169258117676\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4358019828796387\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.370840549468994\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.361081123352051\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.362877130508423\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3425261974334717\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4252769947052\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.404130458831787\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3557746410369873\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.3747398853302\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.417656421661377\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3626821041107178\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4024734497070312\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4151527881622314\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.3818717002868652\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.376659393310547\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.370924472808838\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.366507053375244\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.3497157096862793\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.391754627227783\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.393176317214966\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.396139621734619\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.368685007095337\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3720574378967285\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.39872407913208\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4112350940704346\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.351937770843506\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3659896850585938\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3986880779266357\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.429760456085205\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3993077278137207\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4031732082366943\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4371302127838135\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3602588176727295\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.357313871383667\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.335047721862793\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4142119884490967\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3562231063842773\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.372331380844116\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.414783000946045\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.347935676574707\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.428755044937134\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4104385375976562\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3716578483581543\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.340926170349121\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3458831310272217\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4272568225860596\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3354735374450684\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.390986204147339\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3820884227752686\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3987460136413574\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.381466865539551\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4201419353485107\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4094626903533936\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.331174612045288\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3969905376434326\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.400784969329834\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3958520889282227\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4366841316223145\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.361720085144043\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.410024404525757\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.415276288986206\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.402066230773926\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.374177932739258\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4033045768737793\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.407752513885498\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3998947143554688\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3525848388671875\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4276552200317383\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3759162425994873\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.384620189666748\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4212756156921387\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3676280975341797\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.37668776512146\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3387362957000732\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.343146562576294\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.4017651081085205\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3921663761138916\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.38547420501709\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.428781509399414\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.439887285232544\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3934595584869385\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4139280319213867\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.397361993789673\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.410268783569336\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3993029594421387\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3897781372070312\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3454275131225586\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.369636297225952\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.449455738067627\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.3919432163238525\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3973491191864014\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.426649570465088\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4287543296813965\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4245786666870117\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3718860149383545\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.305915117263794\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4220597743988037\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.406261920928955\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.401360511779785\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4134154319763184\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3912458419799805\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.411508321762085\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3948025703430176\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3749303817749023\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.416870594024658\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4194953441619873\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4183058738708496\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.402076482772827\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.422804594039917\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3911027908325195\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4432787895202637\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4018630981445312\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4413139820098877\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3911757469177246\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.383413791656494\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3702805042266846\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3590073585510254\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3475844860076904\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.370490550994873\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.370009660720825\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4243927001953125\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.363873243331909\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3867671489715576\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.383380651473999\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3992373943328857\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4340105056762695\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4284236431121826\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.455421209335327\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3773374557495117\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.40792179107666\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.417600393295288\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4563825130462646\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4356517791748047\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.372770309448242\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.397512197494507\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.384165048599243\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.430880069732666\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.373335838317871\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4361159801483154\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3824303150177\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4345221519470215\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3846027851104736\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3886682987213135\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3975889682769775\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.426523208618164\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.448591947555542\n",
      "Training accuracy 0.0703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 29/36 [1:52:03<26:51, 230.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041228443384\n",
      "Training loss: 2.412304401397705\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4039573669433594\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4087796211242676\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4152820110321045\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3847949504852295\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.333324670791626\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.432539701461792\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.384777545928955\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.411775588989258\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4045159816741943\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4182679653167725\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.361370325088501\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.370349645614624\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.433558702468872\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3951194286346436\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.369020700454712\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3927106857299805\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.359130859375\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3539719581604004\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.419239044189453\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3563597202301025\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.3914949893951416\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3981685638427734\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.468794822692871\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3832201957702637\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.379873275756836\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4092307090759277\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.402294397354126\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4184439182281494\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4223062992095947\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.426513671875\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4339232444763184\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3647360801696777\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4003372192382812\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4037258625030518\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.408280611038208\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3899338245391846\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.355626106262207\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4095256328582764\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3629016876220703\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.33971905708313\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.4040629863739014\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3851118087768555\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3845252990722656\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4294955730438232\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.372581720352173\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3925533294677734\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.396451234817505\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3965418338775635\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4068875312805176\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3952560424804688\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.345730781555176\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3842055797576904\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.411311388015747\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3539421558380127\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.386443614959717\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3948562145233154\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3789379596710205\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.392145872116089\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3481671810150146\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3895068168640137\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.429013729095459\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.41086483001709\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3658382892608643\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.401097297668457\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.434612274169922\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3961498737335205\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3400449752807617\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.392320394515991\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.405125617980957\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.382838726043701\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4076895713806152\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3922460079193115\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.390697956085205\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.464362382888794\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4056577682495117\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.389491319656372\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.340068817138672\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.369603157043457\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3933677673339844\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.354539155960083\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.439279794692993\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.343716621398926\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4421191215515137\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.383937358856201\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4359514713287354\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3815901279449463\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4414021968841553\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3822686672210693\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3606462478637695\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.349907875061035\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.384460926055908\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.416260242462158\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.374492883682251\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.356847047805786\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3672869205474854\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.391116142272949\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.39780592918396\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3472087383270264\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.398045301437378\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3516151905059814\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.410975933074951\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4326703548431396\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.401920795440674\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3450794219970703\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.416930675506592\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4113948345184326\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.389756202697754\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4043824672698975\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.393328905105591\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3750979900360107\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.370121955871582\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.399879217147827\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.403090238571167\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3547401428222656\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.37591290473938\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4104063510894775\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3331637382507324\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.352733612060547\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.389157772064209\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.365952253341675\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3872251510620117\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.406869649887085\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4255197048187256\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3874151706695557\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3826189041137695\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3507113456726074\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3693978786468506\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3742799758911133\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.36000657081604\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.414851665496826\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.37451171875\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.405893564224243\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.416322708129883\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4097728729248047\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.401782512664795\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4165139198303223\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3314030170440674\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.4258108139038086\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.372328281402588\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.367774724960327\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3502745628356934\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3731281757354736\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.377187728881836\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.345423698425293\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.402231216430664\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4038403034210205\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3938379287719727\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4439806938171387\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.358574628829956\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.417501211166382\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.38198184967041\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3969967365264893\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4034173488616943\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.408801794052124\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3996715545654297\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.401463747024536\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.425072431564331\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4291276931762695\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3832271099090576\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3752899169921875\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.403203010559082\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4396653175354004\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4190099239349365\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3665883541107178\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3841161727905273\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4099278450012207\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3670730590820312\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3779733180999756\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4030001163482666\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4239137172698975\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3850314617156982\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3816094398498535\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3694069385528564\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3969526290893555\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3945751190185547\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.397895574569702\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3885421752929688\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.393264055252075\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3804678916931152\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3504412174224854\n",
      "Training accuracy 0.15625\n",
      "Training loss: 2.4593093395233154\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3689792156219482\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3605728149414062\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4321675300598145\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4146389961242676\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.383697271347046\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3592846393585205\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3947179317474365\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3951518535614014\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.36407208442688\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.405320405960083\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3907277584075928\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.35113263130188\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4113447666168213\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.416898488998413\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3976471424102783\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3800923824310303\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3989439010620117\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.382154941558838\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4039955139160156\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.40814208984375\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.383622646331787\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3677818775177\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3659327030181885\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.420624256134033\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.398380994796753\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.361569404602051\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4433627128601074\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.391519546508789\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3610541820526123\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.40921688079834\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3950021266937256\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.369753122329712\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.372044563293457\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.387160301208496\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.405101776123047\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3682782649993896\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.393068552017212\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3975584506988525\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3952436447143555\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.393566131591797\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3930585384368896\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.390042543411255\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.391691207885742\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3842151165008545\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.391331672668457\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.418064594268799\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4099247455596924\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3446598052978516\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3890366554260254\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.41294527053833\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.351485013961792\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3683407306671143\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3992624282836914\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.400484085083008\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3746137619018555\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3932974338531494\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3616788387298584\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.358057975769043\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.377568244934082\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3652024269104004\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.422245740890503\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3380565643310547\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4341413974761963\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.417302370071411\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.409301519393921\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4002997875213623\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4566938877105713\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.37123703956604\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4100325107574463\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3842339515686035\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.396449327468872\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.446331262588501\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.419188976287842\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3930506706237793\n",
      "Training accuracy 0.07421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 30/36 [1:55:51<22:58, 229.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129040902480483\n",
      "Training loss: 2.367835283279419\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.38405704498291\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3924715518951416\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.314725875854492\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.4272265434265137\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.375002384185791\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4183614253997803\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.381290912628174\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4191951751708984\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.388674736022949\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3683032989501953\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.366461753845215\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4405698776245117\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.410384178161621\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.399871826171875\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3853859901428223\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4048893451690674\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.388291597366333\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3820247650146484\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3817830085754395\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3954126834869385\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3887877464294434\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3701658248901367\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4113731384277344\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.392094850540161\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.420013904571533\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4027903079986572\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4099574089050293\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4054675102233887\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3488285541534424\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3778955936431885\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.372363567352295\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.382511615753174\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3923306465148926\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.390833854675293\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.380664348602295\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.396404981613159\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.383605480194092\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3935546875\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.367635488510132\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.401001453399658\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.42337965965271\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3899049758911133\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.387310028076172\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.376343250274658\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.387556552886963\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.366826295852661\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.381974697113037\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.413987159729004\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3941802978515625\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.41835618019104\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.394707679748535\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.403611898422241\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.367886543273926\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.414334774017334\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3746297359466553\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3960459232330322\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.366481065750122\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3767364025115967\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3967878818511963\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3683314323425293\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3552350997924805\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4452478885650635\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3619189262390137\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.361349582672119\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3711867332458496\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3572795391082764\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3615987300872803\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3832759857177734\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3834118843078613\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.347599983215332\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3598685264587402\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.39290189743042\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4014434814453125\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.366248846054077\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4303436279296875\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4031918048858643\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4137351512908936\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3939461708068848\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3844432830810547\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.362779140472412\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.377574920654297\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4428939819335938\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.353313446044922\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.371441125869751\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3863234519958496\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3509433269500732\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.383427619934082\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3679919242858887\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3826870918273926\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.412400007247925\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3589653968811035\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4190046787261963\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.416614532470703\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3761098384857178\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4140374660491943\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4226794242858887\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3443691730499268\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.368291139602661\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4015657901763916\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.409324884414673\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3720967769622803\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.383195400238037\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3782780170440674\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.38289213180542\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3542704582214355\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4581222534179688\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.378481388092041\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.355815887451172\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.425985336303711\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.337834596633911\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4048821926116943\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4312851428985596\n",
      "Training accuracy 0.05078125\n",
      "Training loss: 2.377424716949463\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4361321926116943\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.383697509765625\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3444724082946777\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.36711049079895\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.402721643447876\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3638064861297607\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3876051902770996\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.400524377822876\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.356635093688965\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3459842205047607\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3770651817321777\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.359935998916626\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.412146806716919\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4250924587249756\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.385876178741455\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3950705528259277\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.396066904067993\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4152979850769043\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4275217056274414\n",
      "Training accuracy 0.05078125\n",
      "Training loss: 2.3463187217712402\n",
      "Training accuracy 0.1640625\n",
      "Training loss: 2.4123122692108154\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.383702039718628\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3554742336273193\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4048306941986084\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4165241718292236\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3746931552886963\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.38962721824646\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.349632740020752\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.442678213119507\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.383726119995117\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3854613304138184\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3648290634155273\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3683290481567383\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.386042356491089\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.366262435913086\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3793227672576904\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3996217250823975\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.444549560546875\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3767383098602295\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.422349691390991\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3735768795013428\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4353322982788086\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.346778154373169\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.34867262840271\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3988239765167236\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3945446014404297\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.388463258743286\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.398218870162964\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3457820415496826\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.375424861907959\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.372295618057251\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.416210412979126\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.44175124168396\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4051830768585205\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4340059757232666\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.43119478225708\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3823626041412354\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.399229049682617\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3844873905181885\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4051578044891357\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.405482053756714\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4023184776306152\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.388925313949585\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3900701999664307\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3828842639923096\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3616104125976562\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4282870292663574\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4162776470184326\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4047765731811523\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.42313289642334\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4034535884857178\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.390064239501953\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.443375587463379\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3493313789367676\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4157602787017822\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3862571716308594\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3985886573791504\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3695530891418457\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3789095878601074\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3902790546417236\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4217162132263184\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.402416706085205\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.415055513381958\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3666365146636963\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3833675384521484\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.3880481719970703\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.393932580947876\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4232285022735596\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.379514217376709\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3750884532928467\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.386500597000122\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3770320415496826\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.409205436706543\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.423104763031006\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3794665336608887\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.340531349182129\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3833045959472656\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4112398624420166\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.441788673400879\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3957347869873047\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4202046394348145\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.399211883544922\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.410612106323242\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.365086317062378\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4018020629882812\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4311914443969727\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.398024559020996\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.416778802871704\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.351120948791504\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.389817953109741\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.407865285873413\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.412720203399658\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.368601083755493\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.378582239151001\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4004204273223877\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3734471797943115\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4262049198150635\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4182486534118652\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3772683143615723\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.38008451461792\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.386118173599243\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4138333797454834\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.39640736579895\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3958699703216553\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4297096729278564\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3634226322174072\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.436038017272949\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3873984813690186\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4398818016052246\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3751604557037354\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.385091543197632\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.388242721557617\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.350177049636841\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.343168020248413\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.420598268508911\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.423619270324707\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3906850814819336\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4334630966186523\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4201674461364746\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3805084228515625\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.427826166152954\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4389779567718506\n",
      "Training accuracy 0.0859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 31/36 [1:59:40<19:07, 229.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129040921106935\n",
      "Training loss: 2.3614819049835205\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.375077724456787\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3887548446655273\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.400569438934326\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3391964435577393\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.430957078933716\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3641676902770996\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.397442102432251\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3834874629974365\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3734195232391357\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3691253662109375\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3882057666778564\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.371518850326538\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.392528533935547\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.403336524963379\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.383293628692627\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3408989906311035\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3598742485046387\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.365201950073242\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4020140171051025\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.368225336074829\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4017961025238037\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.440697431564331\n",
      "Training accuracy 0.05078125\n",
      "Training loss: 2.4281609058380127\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3793787956237793\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3862640857696533\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.414971113204956\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.401503562927246\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4569969177246094\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4072604179382324\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.380040168762207\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.392759084701538\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3823928833007812\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.38295578956604\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.396918535232544\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.419482946395874\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.402141809463501\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4254894256591797\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4212682247161865\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3446762561798096\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3867108821868896\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3716835975646973\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4025444984436035\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3780527114868164\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3470890522003174\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3948819637298584\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3772661685943604\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.382014274597168\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.366201877593994\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4150259494781494\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3961827754974365\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3764638900756836\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3830060958862305\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.323942184448242\n",
      "Training accuracy 0.15625\n",
      "Training loss: 2.3635072708129883\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.404005289077759\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3851089477539062\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.393519878387451\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3595635890960693\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4139420986175537\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4013524055480957\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.403571844100952\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.408470630645752\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.395061731338501\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3449621200561523\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3707640171051025\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4146010875701904\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3704442977905273\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.439110517501831\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4054830074310303\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3606584072113037\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4105093479156494\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.360599994659424\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3585195541381836\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4144701957702637\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4076013565063477\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4441895484924316\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4098479747772217\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.388726234436035\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.375962018966675\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.386801242828369\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3437416553497314\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3889973163604736\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3857321739196777\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3936305046081543\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.378728151321411\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3616456985473633\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3502540588378906\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.410956621170044\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3814074993133545\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.398890733718872\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3372013568878174\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.413896083831787\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.335221529006958\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3812077045440674\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.413524866104126\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3737757205963135\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.361715316772461\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.439530849456787\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.402468204498291\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4174001216888428\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.37642502784729\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.360428810119629\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3934378623962402\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3987672328948975\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.355769395828247\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.395519256591797\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4069530963897705\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3537683486938477\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4011566638946533\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.360139846801758\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3780462741851807\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4172277450561523\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3794991970062256\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.435772657394409\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3849005699157715\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3647491931915283\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.441737174987793\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.4056289196014404\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.448315382003784\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4281959533691406\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.374840259552002\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4208178520202637\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4179463386535645\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3999617099761963\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.386367082595825\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3744163513183594\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3745572566986084\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.428232431411743\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.353511095046997\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.446812391281128\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.396960496902466\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3789045810699463\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3744683265686035\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3729724884033203\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.4235756397247314\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.439068078994751\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3871939182281494\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3958866596221924\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3974783420562744\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3646724224090576\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4148240089416504\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3568005561828613\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3554272651672363\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3249783515930176\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4113829135894775\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.418884515762329\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.391782522201538\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4023597240448\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.420926332473755\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4011049270629883\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4155330657958984\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.424828290939331\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.400167465209961\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4536800384521484\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.382446527481079\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.419409990310669\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3906869888305664\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3839962482452393\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4541919231414795\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.351301908493042\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3512017726898193\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3721981048583984\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3785409927368164\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.371065855026245\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.372990131378174\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4056203365325928\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4009385108947754\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3505747318267822\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4386274814605713\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3802855014801025\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4158504009246826\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.39931321144104\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.38744854927063\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.422732353210449\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4127845764160156\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4025278091430664\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3548760414123535\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3995859622955322\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.395742177963257\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4378035068511963\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.412564516067505\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4262592792510986\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3866193294525146\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.369600772857666\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4281527996063232\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3918399810791016\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4161288738250732\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.430098056793213\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.383626699447632\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3569910526275635\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.354226589202881\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.386481285095215\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3830811977386475\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.331817865371704\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3420605659484863\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3793091773986816\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3919084072113037\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.444528341293335\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.399960994720459\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.383882761001587\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3884329795837402\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3820412158966064\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.450775146484375\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.408857583999634\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3556454181671143\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4008901119232178\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.38789963722229\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3897573947906494\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.372746467590332\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4329993724823\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4481492042541504\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.4096176624298096\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.422393798828125\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.362429141998291\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4405720233917236\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.389089584350586\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4250917434692383\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.356778860092163\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.439026355743408\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.381502866744995\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.430206298828125\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3605706691741943\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3819353580474854\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.420640230178833\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3761346340179443\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3738040924072266\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3347530364990234\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.37636137008667\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3795018196105957\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4007365703582764\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.374535322189331\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.39054274559021\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.394106388092041\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.393721342086792\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3726227283477783\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3635804653167725\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4023244380950928\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3653788566589355\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3547613620758057\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3884201049804688\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4098362922668457\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.411803960800171\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.394944429397583\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.40747332572937\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.407467842102051\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.355015277862549\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3802688121795654\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.40840744972229\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4523327350616455\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.4011545181274414\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4459314346313477\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.4168899059295654\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.394153594970703\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3717005252838135\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3581180572509766\n",
      "Training accuracy 0.1328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 32/36 [2:03:32<15:20, 230.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129040921106935\n",
      "Training loss: 2.382091522216797\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.439649820327759\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3952269554138184\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4202489852905273\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4397311210632324\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.431279420852661\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4705004692077637\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.396843671798706\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3826510906219482\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.378553867340088\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.364720106124878\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.373426914215088\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.343125104904175\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.353677272796631\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.406377077102661\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.394426107406616\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3737294673919678\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3985445499420166\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3611998558044434\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3546059131622314\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.360297203063965\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.345510959625244\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4199700355529785\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3896753787994385\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3965911865234375\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4455645084381104\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4149672985076904\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3848659992218018\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3755767345428467\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.381103277206421\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3518857955932617\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4032483100891113\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3848330974578857\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.37888503074646\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.346775770187378\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4298784732818604\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3425815105438232\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4179484844207764\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3982255458831787\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3746719360351562\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.413830518722534\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.384086847305298\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.432737350463867\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.391057252883911\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.380786418914795\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3667104244232178\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3852100372314453\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3727376461029053\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3767614364624023\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3826894760131836\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.396717071533203\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.383989095687866\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.437473773956299\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3902950286865234\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.42268705368042\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.349748134613037\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3925557136535645\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.404679298400879\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3795647621154785\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.414067029953003\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4333736896514893\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.404170036315918\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3737738132476807\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3977715969085693\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.36352276802063\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.418884038925171\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.388927936553955\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.421584367752075\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.383435010910034\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3545265197753906\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4019641876220703\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3669204711914062\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3488264083862305\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4306864738464355\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.405712366104126\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.425868511199951\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.427328109741211\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3324177265167236\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.4431755542755127\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3734934329986572\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4196617603302\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4411816596984863\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3836467266082764\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3966476917266846\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.419750690460205\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4026310443878174\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3696985244750977\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.374337673187256\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.415225028991699\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4262499809265137\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.394742012023926\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.397340774536133\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3792362213134766\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3844728469848633\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.399240493774414\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3935229778289795\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.38519024848938\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3507823944091797\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3866333961486816\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.384643316268921\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.301410675048828\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3975391387939453\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3812899589538574\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3861985206604004\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4355340003967285\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3757054805755615\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3863742351531982\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.408109664916992\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.39243221282959\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3685033321380615\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.389531373977661\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.418034076690674\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3804922103881836\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3773910999298096\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4104037284851074\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3811135292053223\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.345843553543091\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3976829051971436\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3674023151397705\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3658530712127686\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3851118087768555\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4329774379730225\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4016079902648926\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.406825304031372\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.34995436668396\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3581743240356445\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3431601524353027\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4037628173828125\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.423748254776001\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.418705940246582\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.416011095046997\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3753137588500977\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.423469066619873\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.422940254211426\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3599021434783936\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.384397268295288\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4087767601013184\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3531274795532227\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3936963081359863\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4161462783813477\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3870763778686523\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3788399696350098\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4411239624023438\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3667147159576416\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.422790288925171\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4179062843322754\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4380476474761963\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3757107257843018\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4078755378723145\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3776028156280518\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4135892391204834\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.40777325630188\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.381498336791992\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.385598659515381\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3582725524902344\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.389303684234619\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3683478832244873\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.412580728530884\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.411735773086548\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.371032238006592\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4237117767333984\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3429527282714844\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.405548334121704\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4167468547821045\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.404956579208374\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4105031490325928\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.395714044570923\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3578684329986572\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.408984899520874\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.390676975250244\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3828556537628174\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.395857810974121\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3876335620880127\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3654251098632812\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.400501251220703\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3986992835998535\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.385063886642456\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3830108642578125\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.386068105697632\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3999173641204834\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.40958833694458\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.370849609375\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4053051471710205\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.366445302963257\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4221255779266357\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.373715877532959\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3707096576690674\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3705573081970215\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4072203636169434\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3680667877197266\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3839495182037354\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4027435779571533\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4033944606781006\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4345924854278564\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3897926807403564\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4054641723632812\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3472633361816406\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.388695001602173\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3633062839508057\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3799214363098145\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3493545055389404\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.451143264770508\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4179346561431885\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.415048837661743\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3840129375457764\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3932836055755615\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.370168924331665\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4094033241271973\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.366178512573242\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4033994674682617\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.420656442642212\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.409538745880127\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3490025997161865\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.4249746799468994\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4133641719818115\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3333003520965576\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3923656940460205\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3943517208099365\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.391522169113159\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3804800510406494\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3950560092926025\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3270413875579834\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4222609996795654\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.392385482788086\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3786556720733643\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.4168753623962402\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4095494747161865\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.401414632797241\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.386173725128174\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.398341417312622\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3749098777770996\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4365291595458984\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3998632431030273\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.405386447906494\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3307440280914307\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3706207275390625\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.397244691848755\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.385361909866333\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.388561725616455\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.408142328262329\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4042606353759766\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3951525688171387\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3740622997283936\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.361325263977051\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3758766651153564\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.387247085571289\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.33914852142334\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3812096118927\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4088430404663086\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.40051007270813\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.374044418334961\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3535220623016357\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.377652645111084\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.407883882522583\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4122164249420166\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.398735284805298\n",
      "Training accuracy 0.08203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 33/36 [2:07:23<11:31, 230.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.412904086522758\n",
      "Training loss: 2.384958505630493\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.420762300491333\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3757212162017822\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3862810134887695\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3434200286865234\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3983659744262695\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3441693782806396\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3702468872070312\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.45652174949646\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.414080858230591\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.384340763092041\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.395922899246216\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.392415761947632\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.389289379119873\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3817484378814697\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.394735336303711\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3423922061920166\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3702142238616943\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.404726982116699\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.406376600265503\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.354637384414673\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4060962200164795\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.382481336593628\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4164695739746094\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3866446018218994\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.417945146560669\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.348944664001465\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.3992977142333984\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.399937629699707\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4139604568481445\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.395554304122925\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4057693481445312\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3668625354766846\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.365248680114746\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3783328533172607\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4088428020477295\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3771800994873047\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3907687664031982\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3660831451416016\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3965206146240234\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3737294673919678\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4173102378845215\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3990983963012695\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3771257400512695\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.424210548400879\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3782851696014404\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.418203830718994\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.365013599395752\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3931679725646973\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.3574891090393066\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3882853984832764\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.397986888885498\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.407231569290161\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3965609073638916\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.414848566055298\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3569602966308594\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3760037422180176\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3736684322357178\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.407240867614746\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3929550647735596\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3267040252685547\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3450169563293457\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.366264581680298\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4277937412261963\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.335695266723633\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4198784828186035\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.36346435546875\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.36553692817688\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.4109654426574707\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.413503885269165\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3794262409210205\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3827202320098877\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3939151763916016\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4190235137939453\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3819897174835205\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.433181047439575\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4166815280914307\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.4038448333740234\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4282310009002686\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3912062644958496\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4363300800323486\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.431685447692871\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.372065544128418\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.426088809967041\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3753905296325684\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4072091579437256\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3532211780548096\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.416839599609375\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.400864839553833\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4189822673797607\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4010496139526367\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3688161373138428\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3901302814483643\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4015884399414062\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3776063919067383\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.3996968269348145\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.419675827026367\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3906733989715576\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3618218898773193\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3921849727630615\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.390660047531128\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.391331195831299\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3813891410827637\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.406181812286377\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.394333600997925\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.383716583251953\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4136345386505127\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.422675609588623\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.346890449523926\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3783516883850098\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4202146530151367\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.435816764831543\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.409769058227539\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4063854217529297\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.36568546295166\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.348759174346924\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.38686203956604\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.417194128036499\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3630967140197754\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.4187862873077393\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3552186489105225\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.413144588470459\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3306121826171875\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.390503406524658\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3910300731658936\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.41435170173645\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4034860134124756\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.391305923461914\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.430104970932007\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.390803575515747\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.382056951522827\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3753464221954346\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3799898624420166\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.435262680053711\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.362814426422119\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.404726266860962\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.391071081161499\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.380648612976074\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4224016666412354\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3822453022003174\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.365464448928833\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3778281211853027\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3861496448516846\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.396899700164795\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.458991289138794\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3498566150665283\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3743276596069336\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4022018909454346\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.423008680343628\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4151268005371094\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4088053703308105\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4232993125915527\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3662362098693848\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3776016235351562\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.418754816055298\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.397986650466919\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3876469135284424\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.399421215057373\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.365278720855713\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.381434202194214\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3542590141296387\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3977646827697754\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.412221670150757\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3837337493896484\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4214351177215576\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3722169399261475\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.402040958404541\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.342047929763794\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.400501251220703\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4225471019744873\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.397796392440796\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.359999179840088\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3534748554229736\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3456006050109863\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.3692855834960938\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3649537563323975\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4622721672058105\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.387124538421631\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.401735305786133\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.403416395187378\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.406097173690796\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4174184799194336\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3873696327209473\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4441916942596436\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.393881320953369\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.372161626815796\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3836281299591064\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4017415046691895\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4224088191986084\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.37851881980896\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.388052463531494\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.344083309173584\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3717644214630127\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.396361827850342\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3946292400360107\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.383199691772461\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4184937477111816\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.456328868865967\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3910622596740723\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3761911392211914\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.369546413421631\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3396971225738525\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.373570680618286\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.401881456375122\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3866686820983887\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4113306999206543\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.385450839996338\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3914992809295654\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3665390014648438\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.385155200958252\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.358672618865967\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3943088054656982\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.426659345626831\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4173879623413086\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3590352535247803\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.418795585632324\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.442500114440918\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.417070150375366\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4013450145721436\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3709194660186768\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.40232253074646\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3580310344696045\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4224114418029785\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3716847896575928\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3792572021484375\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3915011882781982\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.334996223449707\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.411576986312866\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3956117630004883\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4000566005706787\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4127049446105957\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.42059326171875\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3801047801971436\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.414560556411743\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.404764413833618\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4467618465423584\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3410987854003906\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3852274417877197\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3972930908203125\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4165656566619873\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.383770704269409\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3958988189697266\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.394639492034912\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3445587158203125\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.408967971801758\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3933374881744385\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.393528699874878\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4311885833740234\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.430985927581787\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.331071138381958\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3774826526641846\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3769595623016357\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.388012647628784\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3941447734832764\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3767242431640625\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.376213312149048\n",
      "Training accuracy 0.09765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 34/36 [2:11:14<07:41, 230.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041116684675\n",
      "Training loss: 2.395073175430298\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.354625701904297\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.376952648162842\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.388679265975952\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4379642009735107\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.377223491668701\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.406076192855835\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.384443998336792\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4047725200653076\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4014906883239746\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.387967109680176\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.354780673980713\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.414186477661133\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3956551551818848\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4053869247436523\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.416731357574463\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3651697635650635\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.367952346801758\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3510873317718506\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.422410726547241\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4253528118133545\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3756396770477295\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3988590240478516\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4138667583465576\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.411336898803711\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.374134063720703\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.309821128845215\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.413863182067871\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3634467124938965\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.409898281097412\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.402669906616211\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4431793689727783\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.411062717437744\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.392022132873535\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.359752893447876\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3877413272857666\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3470802307128906\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4121193885803223\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.407755136489868\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3883984088897705\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3908848762512207\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.404759407043457\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3630523681640625\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3878538608551025\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4324400424957275\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4285874366760254\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.380999803543091\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3978538513183594\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4242706298828125\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4194211959838867\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3554413318634033\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3903722763061523\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.356947898864746\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3550329208374023\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3401806354522705\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4449715614318848\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.415984630584717\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3911373615264893\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.404405355453491\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.390098810195923\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4396016597747803\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4073853492736816\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3961730003356934\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3518123626708984\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4350624084472656\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3758671283721924\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.433884620666504\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.370868682861328\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3478970527648926\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3929331302642822\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3891913890838623\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4191932678222656\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4050512313842773\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3650267124176025\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4239070415496826\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.328988552093506\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3926029205322266\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3901102542877197\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.376861572265625\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4048776626586914\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3986122608184814\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.38189697265625\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.38863205909729\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.347475528717041\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.403172731399536\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.362144947052002\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3783531188964844\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3877875804901123\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.426377296447754\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4470484256744385\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.357454538345337\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.399203300476074\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4023351669311523\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4000766277313232\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.371598720550537\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4068048000335693\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.361182451248169\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.364320755004883\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4002525806427\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4377923011779785\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.436706304550171\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4100828170776367\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4138033390045166\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3664567470550537\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3688886165618896\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3617327213287354\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.386655807495117\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3483896255493164\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.389239549636841\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3804571628570557\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4015064239501953\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.366472005844116\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.425764560699463\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.352541923522949\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.394402503967285\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4121241569519043\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4004030227661133\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4275646209716797\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4198641777038574\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3538427352905273\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.366567373275757\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3729143142700195\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.395785331726074\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.426074504852295\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3765764236450195\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3694980144500732\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3542041778564453\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.378093719482422\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4186925888061523\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.419741153717041\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3797011375427246\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.388057231903076\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4229068756103516\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.381894588470459\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3858373165130615\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.332057237625122\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3980579376220703\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.359436511993408\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.382749557495117\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3703718185424805\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4069292545318604\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.375877857208252\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3561604022979736\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4277594089508057\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3547205924987793\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3559505939483643\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3796257972717285\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.38511323928833\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4009034633636475\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.388859748840332\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3889167308807373\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.390166759490967\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.403120994567871\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.416245698928833\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.40189266204834\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3618805408477783\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4200334548950195\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.408799171447754\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4630205631256104\n",
      "Training accuracy 0.05078125\n",
      "Training loss: 2.3729941844940186\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3759398460388184\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.354890823364258\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.362698793411255\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.413207769393921\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.394320249557495\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4027135372161865\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3749303817749023\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3945391178131104\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3351495265960693\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.4354636669158936\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3830883502960205\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.373661518096924\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.378527879714966\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.378138303756714\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.373441219329834\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.395707130432129\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3717212677001953\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4240081310272217\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.398381471633911\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3818626403808594\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.361420154571533\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.354412317276001\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.426088571548462\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3560242652893066\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.419673204421997\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3577351570129395\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3782718181610107\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3893959522247314\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4064178466796875\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4137799739837646\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3860020637512207\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3766496181488037\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.38594388961792\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3344109058380127\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.383244276046753\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4557151794433594\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.369600296020508\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.414597988128662\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4564921855926514\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.355226516723633\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.4105138778686523\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.374589204788208\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.383974075317383\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3935985565185547\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4378693103790283\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.441115379333496\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3869621753692627\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.397611618041992\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3745408058166504\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.37157940864563\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.409304618835449\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.379640817642212\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3793556690216064\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3572587966918945\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3974456787109375\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.355186939239502\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.410830497741699\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.393916130065918\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3980226516723633\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.381713390350342\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3820650577545166\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4181034564971924\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.4155960083007812\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4272844791412354\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4097585678100586\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.392920732498169\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4004855155944824\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3931331634521484\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3656489849090576\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3649165630340576\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4076240062713623\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.378565788269043\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4272971153259277\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3581736087799072\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.396299123764038\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4240548610687256\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4377636909484863\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4433727264404297\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4151411056518555\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.386592388153076\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3252835273742676\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.3980252742767334\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.417595624923706\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3732569217681885\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.344291925430298\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.406898260116577\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3895974159240723\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3929507732391357\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4117002487182617\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3731002807617188\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.426811695098877\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3619327545166016\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3943777084350586\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.36419415473938\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4294979572296143\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4002490043640137\n",
      "Training accuracy 0.12109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 35/36 [2:15:07<03:51, 231.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041032865644\n",
      "Training loss: 2.3774983882904053\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3896162509918213\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.379739999771118\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.351518154144287\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3888211250305176\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.402535915374756\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3455698490142822\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.356877088546753\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.4012386798858643\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4121809005737305\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3892312049865723\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.369748830795288\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.404658555984497\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3287644386291504\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3419079780578613\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.430842876434326\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3934895992279053\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.423792839050293\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3679118156433105\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4054508209228516\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3933355808258057\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3948047161102295\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3660221099853516\n",
      "Training accuracy 0.1484375\n",
      "Training loss: 2.358705997467041\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.4246768951416016\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.411811590194702\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.4055376052856445\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3795876502990723\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3976244926452637\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4209563732147217\n",
      "Training accuracy 0.05859375\n",
      "Training loss: 2.3904306888580322\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.444448232650757\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4084231853485107\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3657524585723877\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3993570804595947\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3733232021331787\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3604490756988525\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.406564712524414\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.364300489425659\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.382810354232788\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4035825729370117\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4008421897888184\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3907084465026855\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4150047302246094\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.390843629837036\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3749642372131348\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3458566665649414\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3759615421295166\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.409193992614746\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.35396409034729\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3977155685424805\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3850018978118896\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.408352851867676\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4171669483184814\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3953964710235596\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.39656925201416\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.456982374191284\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3986525535583496\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4053738117218018\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3821358680725098\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.442270517349243\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3754827976226807\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4175775051116943\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.371765375137329\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.402707099914551\n",
      "Training accuracy 0.14453125\n",
      "Training loss: 2.3430821895599365\n",
      "Training accuracy 0.140625\n",
      "Training loss: 2.3581066131591797\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3607120513916016\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3988640308380127\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3758316040039062\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4122512340545654\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3904082775115967\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3937366008758545\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3611257076263428\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.405759334564209\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.357426881790161\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.4450395107269287\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3735039234161377\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3916103839874268\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.375079870223999\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.385526657104492\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.407452344894409\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3713419437408447\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.465538740158081\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.355884313583374\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3869099617004395\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4015655517578125\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.400563955307007\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4022905826568604\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4528677463531494\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.360574960708618\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3790206909179688\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.408226728439331\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4070048332214355\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.395569324493408\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4089958667755127\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4413373470306396\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4306483268737793\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3590004444122314\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.388002872467041\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3819143772125244\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4172632694244385\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.3567709922790527\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3611338138580322\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4017937183380127\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4057199954986572\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.4416816234588623\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.394102096557617\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.386244058609009\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4255259037017822\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.399831533432007\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.401989459991455\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3930978775024414\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.375089406967163\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3924546241760254\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4025051593780518\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.356459617614746\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4049293994903564\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.371412754058838\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.362764596939087\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.371971368789673\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4092226028442383\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3712024688720703\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.368004560470581\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3986623287200928\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3922371864318848\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.37773060798645\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3832356929779053\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.362797498703003\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.420783758163452\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4012551307678223\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4191110134124756\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.392608404159546\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4074575901031494\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.380937337875366\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3943305015563965\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3912291526794434\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3980934619903564\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3183372020721436\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.404104232788086\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4069581031799316\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.4444870948791504\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4218826293945312\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.410594940185547\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4187986850738525\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3964879512786865\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3448235988616943\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4027912616729736\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3893442153930664\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3773224353790283\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.345423698425293\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3774516582489014\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4038681983947754\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.35890531539917\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4043867588043213\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3813579082489014\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.4181079864501953\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.389817476272583\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.404677629470825\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.435863971710205\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.3991377353668213\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.418851852416992\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3798842430114746\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4131481647491455\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.386981964111328\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.406439781188965\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3711540699005127\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3574671745300293\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4153881072998047\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3647022247314453\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.352977991104126\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3869521617889404\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3930702209472656\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.393559694290161\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.424293279647827\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.378225803375244\n",
      "Training accuracy 0.1328125\n",
      "Training loss: 2.3602683544158936\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3699400424957275\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4278273582458496\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3717832565307617\n",
      "Training accuracy 0.13671875\n",
      "Training loss: 2.413290500640869\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.3549299240112305\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3826937675476074\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3748459815979004\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3895134925842285\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4120688438415527\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.418242931365967\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.371601104736328\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3921074867248535\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3758814334869385\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.398554563522339\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.4101781845092773\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.385348320007324\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3747847080230713\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3783881664276123\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.44323992729187\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.406226634979248\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3844404220581055\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.386868476867676\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.463622808456421\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4003992080688477\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.379819631576538\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.346404552459717\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.364622116088867\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.387874126434326\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.40663743019104\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.3654332160949707\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3815510272979736\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.3819375038146973\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.3702094554901123\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3905696868896484\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.384290933609009\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3863487243652344\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3913419246673584\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.462315559387207\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.390298843383789\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.4157421588897705\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.3985860347747803\n",
      "Training accuracy 0.0859375\n",
      "Training loss: 2.395277500152588\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.389543056488037\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.384223461151123\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.4134361743927\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.364243745803833\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.4308583736419678\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.352928638458252\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.4407107830047607\n",
      "Training accuracy 0.10546875\n",
      "Training loss: 2.4147772789001465\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.436274290084839\n",
      "Training accuracy 0.0625\n",
      "Training loss: 2.4022624492645264\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.3760106563568115\n",
      "Training accuracy 0.12109375\n",
      "Training loss: 2.3617961406707764\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.3984742164611816\n",
      "Training accuracy 0.1015625\n",
      "Training loss: 2.4422523975372314\n",
      "Training accuracy 0.0703125\n",
      "Training loss: 2.396069049835205\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3839404582977295\n",
      "Training accuracy 0.06640625\n",
      "Training loss: 2.336174249649048\n",
      "Training accuracy 0.15234375\n",
      "Training loss: 2.3702552318573\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3954854011535645\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.3440396785736084\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.4014368057250977\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.3726232051849365\n",
      "Training accuracy 0.125\n",
      "Training loss: 2.3843283653259277\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.3843202590942383\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.412787914276123\n",
      "Training accuracy 0.12890625\n",
      "Training loss: 2.3748598098754883\n",
      "Training accuracy 0.109375\n",
      "Training loss: 2.389603853225708\n",
      "Training accuracy 0.11328125\n",
      "Training loss: 2.393669843673706\n",
      "Training accuracy 0.07421875\n",
      "Training loss: 2.4342966079711914\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.3467607498168945\n",
      "Training accuracy 0.09765625\n",
      "Training loss: 2.380082130432129\n",
      "Training accuracy 0.08984375\n",
      "Training loss: 2.4440598487854004\n",
      "Training accuracy 0.0546875\n",
      "Training loss: 2.39477276802063\n",
      "Training accuracy 0.08203125\n",
      "Training loss: 2.418121099472046\n",
      "Training accuracy 0.078125\n",
      "Training loss: 2.3499655723571777\n",
      "Training accuracy 0.1171875\n",
      "Training loss: 2.390115737915039\n",
      "Training accuracy 0.09375\n",
      "Training loss: 2.3664653301239014\n",
      "Training accuracy 0.09765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [2:18:59<00:00, 231.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4129, acc: 0.0966\n",
      "eval loss 2.4129041004925966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "test_losses = []\n",
    "config[\"in_context_learner\"][\"dim\"] = 128\n",
    "config[\"in_context_learner\"][\"inner_dim\"] = config[\"in_context_learner\"][\"dim\"] * 4\n",
    "model = InContextLearner(**config[\"in_context_learner\"]).to(config[\"device\"])\n",
    "for epoch in tqdm.tqdm(range(start_epoch,config[\"epochs\"])):\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(config[\"device\"]), y.to(config[\"device\"])\n",
    "        y_hat = model(x)\n",
    "        if config[\"whole_seq_prediction\"]:\n",
    "            loss = F.cross_entropy(y_hat.view(-1, 10), y.view(-1))\n",
    "        else:\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "        loss.backward()\n",
    "        model_optim.step()\n",
    "        model_optim.zero_grad()\n",
    "\n",
    "        # update the plot\n",
    "        # if i % 10 == 9:\n",
    "        #     if config[\"whole_seq_prediction\"]:\n",
    "        #         acc_over_seq = (y_hat.argmax(dim=-1) == y).float().mean(dim=0) # (seq_len,)\n",
    "        #         acc_max_improvement_within_seq = \\\n",
    "        #             ((y_hat[:,1:,:].argmax(dim=-1) == y[:,1:]).float().max(dim=-1).values \\\n",
    "        #             - (y_hat[:,0,:].argmax(dim=-1) == y[:,0]).float()).mean().item()\n",
    "            #     live_plot.update({\"train_acc_over_seq\": acc_over_seq.tolist()}, reset=True)\n",
    "            #     live_plot.update({\"train_acc_max_improvement_within_seq\": acc_max_improvement_within_seq})\n",
    "            # live_plot.update({\"train_loss\": loss.item(), \"train_acc\": (y_hat.argmax(dim=-1) == y).float().mean().item()})\n",
    "            # live_plot.draw()\n",
    "        train_losses.append(loss.item())\n",
    "        train_accuracy.append((y_hat.argmax(dim=-1) == y).float().mean().item())\n",
    "        print(f\"Training loss: {loss.item()}\")\n",
    "        print(f\"Training accuracy {(y_hat.argmax(dim=-1) == y).float().mean().item()}\")\n",
    "\n",
    "    ### evaluate\n",
    "    eval_loss, eval_acc, acc_over_seq, acc_max_improvement_within_seq = eval(model, test_loader)\n",
    "    #live_plot.update({\"eval_loss\": eval_loss, \"eval_acc\": eval_acc})\n",
    "    if config[\"whole_seq_prediction\"]:\n",
    "        print(acc_max_improvement_within_seq)\n",
    "        print(acc_over_seq)\n",
    "    #     live_plot.update({\"eval_acc_max_improvement_within_seq\": acc_max_improvement_within_seq})\n",
    "    #     live_plot.update({\"eval_acc_over_seq\": acc_over_seq}, reset=True)\n",
    "    # live_plot.draw()\n",
    "    print(f\"eval loss {eval_loss}\")\n",
    "    test_accuracy.append(eval_acc)\n",
    "    test_losses.append(eval_loss)\n",
    "    ### save model\n",
    "    if epoch % config[\"ckpt_freq\"] == 0:\n",
    "        save_checkpoint(epoch=epoch, model=model, optimizer=model_optim, loss_train=loss.detach(), loss_eval=eval_loss, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(epoch=epoch, model=model, optimizer=model_optim, loss_train=loss.detach(), loss_eval=eval_loss, config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5692ede66a2eeda96ca4e496ad881a063b66ee8e9ec6003b28974c60439bc6fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
